QUERY: When calculating the normal approximation for the binomial do we use half intervals e.g. 111.5 instead of 112 if we're looking for the area to the right in the curve; and can we use a different normal distribution applet than the ones recommended. If so, what are the answer tolerances. Can one be several tenths of a percentage point off. This ambiguity is killing me. Literally, I'm dying.
*************************
((2, '/Users/jag/Downloads/Stanford medstats/HRP262 Unit 2 Module 1.srt', "times actually follow something like an exponential distribution. Function. If you can pin that down then that's obviously you can fit those data to a pretty nice, smooth mathematical model. So I'm just going to be using t rather than x to start with here since eventually we're just going to be talking about probability distributions for the, survival times. So this is an exponential function I've written out here. You do not have to do any intervals in this Course, but just totally for fun, I'm going to integrate this curve from zero to infinity. It starts at zero, there's no negatives here on an exponential. From 0 to infinity, if I integrate this curve, I can show you indeed that the area under the curve is one. If you totally hate integrals and you don't want to see this, feel free to forward the video. But just for fun, here's my function. [SOUND] And it's an easy function to integrate actually because the exponential the derivative of the exponential is just the exponential. ", ['times', 'something', 'distribution', 'Function', 'data', 'model', 't', 'x', 'probability', 'distributions', 'survival', 'times', 'function', 'intervals', 'Course', 'fun', 'curve', 'infinity', 'zero', 'negatives', 'infinity', 'curve', 'area', 'curve', 'integrals', 'video', 'fun', 'function', '[', 'function']), 0.21650635094610968, 0.18398032358475422)
((15, '/Users/jag/Downloads/Stanford medstats/Unit 4 module 5 part 1.srt', "The column is the .06 part. And, I get an area, the area to the left, remember the charts are setup to always give you the area to the left. So this is giving me the area to the left. The area to the left, there's 2.46 which we know is way out in the tail. That area, not surprisingly is very big, 99.31%. So what's the chance, what's the area to the right then? How do you get the area to the right? You're going to have to subtract 1 minus .9931 will give me the area to the right. Which turns out to be .69%, a pretty small, percentage there. Because as you don't tend to have babies that heavy. Let's answer the 2nd practice question, what's the chance of obtaining a birth weight of 120 or lighter? Again, let's draw out the picture, so again, we're on a normal curve where the mean is 109. The standard deviation is 13. I want to know, what's the chance of obtaining a birth rate of 120 or ", ['column', 'part', 'area', 'area', 'left', 'remember', 'charts', 'area', 'left', 'area', 'left', 'area', 'left', 'way', 'tail', 'area', '%', 'chance', 'area', 'right', 'area', 'right', 'minus', 'area', 'right', '%', 'percentage', 'babies', 'Let', 'answer', 'practice', 'question', 'chance', 'weight', 'draw', 'picture', 'curve', 'deviation', 'chance', 'rate']), 0.3192211990345307, 0.17654713275738437)
((6, '/Users/jag/Downloads/Stanford medstats/Unit 4 module 5 part 1.srt', "Well 575 wasn't on one of those boundaries that I just showed you, it doesn't fall at either 1 standard deviation or 2 standard deviations or 3 standard deviations away from the mean. So our 68-95-99.7 rule doesn't help us here. So you might think oh, no we're going to have to go to calculus and do an integral to figure this out, right. I mean you would get the answer if you, if you know the math and I'm like, here's my normal curve. Here's my mean of 500. It's got a standard deviation of 50, and I literally could integrate from negative infinity all the way up to 575. I could do this integral under the curve, right? Now, again, I'm not going to, you're not going to have to do this, I promise. But that would be mathematically the w-, one way. To, to solve that, you could actually do that integral and find out that, that area under the curve, and that would be the probability of getting 575 or less. But here's the integral again, just for fun. Solve this integral? No thanks. It looks like a pain to solve, and we don't want to, ", ['Well', 'boundaries', 'deviation', 'deviations', 'deviations', 'rule', 'answer', 'math', 'curve', 'deviation', 'infinity', 'way', 'curve', 'w-', 'way', 'find', 'area', 'curve', 'probability', 'again', 'fun', 'Solve', 'thanks', 'pain']), 0.20833333333333334, 0.17077551202783792)
((0, '/Users/jag/Downloads/Stanford medstats/Unit 1 Module 3 part 2.srt', "Now we're talking about the different shapes of the distribution as a left-skewed or right-skewed, so let me just summarize this. So left-skewed means that you have a left tail. So you have a few extreme values that are low to the left. Right-skewed means you have a right tail, a few extreme values that are high to the right. And of course symmetric means just what it sounds like, you have a nice symmetric distribution. There's a special type of symmetric distribution called a bell curve or a normal distribution that we're going to talk about a lot in this course. So I'm just going to introduce it now to just get you thinking about it. So a bell curve is just what it sounds like, it follows the shape of a bell. It turns out to be very important in statistics for a number of reasons. So I'll just say a little bit more about it now. So the reason it's important is, for one thing, it has sort of a predictable behavior that makes it very useful. So it turns out that if you're on a perfect normal curve that 68% of your observations will fall somewhere between one standard deviation below the mean to one standard deviation above the mean. ", ['shapes', 'distribution', 'means', 'tail', 'extreme', 'values', 'left', 'means', 'right', 'tail', 'extreme', 'values', 'right', 'course', 'means', 'distribution', 'type', 'distribution', 'bell', 'curve', 'distribution', 'lot', 'course', 'bell', 'curve', 'shape', 'bell', 'statistics', 'number', 'reasons', 'bit', 'reason', 'thing', 'sort', 'behavior', 'perfect', 'curve', '%', 'observations', 'deviation', 'deviation']), 0.24696958497680327, 0.16953237671615357)
((0, '/Users/jag/Downloads/Stanford medstats/Unit 4 Module 6 part 1.srt', "In this next module, I'm going to to talk about the normal approximation to the binomial. So whenever you have a binomial distribution, you might have noticed in the earlier modules that a lot of the binomial distributions when you do out all the probabilities and you plot them, they look an awful lot like a normal distribution. So it turns out that as long as you're on a binomial, where n times p is greater than 5, that is, the expected value is greater than 5, it usually looks an awful lot like a normal. And we can approximate that binomial, those binomial probabilities, with a normal distribution. Let me just remind you of some of the examples that we saw. So for example, we saw this one in the upper left hand right corner was the wrinkle study, n here was 10 patients, P was 0.5. That, get, exactly gives us an np of five. And p times p is equal to five, hey, that looks pretty much like a normal curve to me. Right? That's very close to a normal curve of course with jagged edges, but still very close, and, ", ['module', 'approximation', 'distribution', 'modules', 'lot', 'distributions', 'probabilities', 'lot', 'distribution', 'n', 'times', 'value', 'lot', 'probabilities', 'distribution', 'Let', 'examples', 'example', 'left', 'hand', 'wrinkle', 'study', 'n', 'patients', 'P', 'np', 'times', 'looks', 'curve', 'curve', 'course', 'edges']), 0.20801257358446093, 0.16612436986530638)
((6, '/Users/jag/Downloads/Stanford medstats/Unit 5 Homework with answers.srt', "That it will simply be smoother because as we have more and more observations the center limit harem will start kicking in, and we will have a more and more normal looking distribution. When we have ten times more values, this distribution will look ten times smoother. The correct answer to 'D' is two. The sample size of the hypothetical trial is larger, which is the reason that the 'p' value is lower. This question gives us the percentage of US adults Who had five or more drinks in one day at least once in a given year. And we have trends for, let's see, we have trends for seven years. We have the mean value as well as the 95% confidence intervals given. The question wants us to find for which years we could reset the null hypothesis that more than 20% of US adults had five or more drinks in one day. At a 0.25 significance level. The answer is 2000 and 2003 only. ", ['observations', 'center', 'limit', 'harem', 'distribution', 'times', 'values', 'distribution', 'times', 'correct', 'answer', 'sample', 'size', 'trial', 'reason', 'value', 'question', 'percentage', 'US', 'adults', 'drinks', 'day', 'year', 'trends', 'see', 'trends', 'years', 'value', '%', 'confidence', 'intervals', 'question', 'years', 'null', 'hypothesis', '%', 'US', 'adults', 'drinks', 'day', 'significance', 'level', 'answer']), 0.18325416653445784, 0.16451014730937927)
((1, '/Users/jag/Downloads/Stanford medstats/Unit 4 module 1 part 2.srt', "And let me just write up what that function is. So, the exponential distribution is e raised to negative x. We call that the exponential distribution in probability. And that function is just a mathematical function, where the area under the curve is one. And we can kind of just draw a picture of that function to see that. You can't have negative values. So it starts at one and it kind of goes down towards the x-axis like that. Now, if I just told you that the area under that curve is 1, you'd probably say that sounds reasonable, right? If you were to calculate the area under the curve here, it starts at 1 and it goes down towards the x value, it seems reasonable that that area might be 1. So if you want to just believe me, that's, that's fine. I can also show you the interval for those who are interested. Again, this is optional, but we could take the integral over all possible values of this function. So you can have any value from the x, can be anything from 0 to infinity. If we take the integral of e raised to negative x. And we, we integrated over the, from 0 to infinity. We get a value of 1. Again, that, that integral is just there for those of you who want it, but ", ['function', 'distribution', 'e', 'x', 'distribution', 'probability', 'function', 'function', 'area', 'curve', 'picture', 'function', 'values', 'goes', 'towards', 'x-axis', 'area', 'curve', 'sounds', 'area', 'curve', 'towards', 'x', 'value', 'area', 'fine', 'interval', 'values', 'function', 'value', 'x', 'anything', 'infinity', 'e', 'x', 'infinity', 'value']), 0.22169908758696596, 0.1521854330279222)
((3, '/Users/jag/Downloads/Stanford medstats/Kaplan Meier Curves 1  How to Draw Them.srt', "So the curve is going to come down a certain amount. And one patient, one patient died from surgical complications, from surgery. And one died, from complications of the pancreatic cancer. It does kill very quickly. Okay. And as a result, the curve will drop down. Now, how much will the curve drop down? Well, at, at this point that we're talking about, there are ten patients at risk and two died. So, two out of ten patients died. And the percent surviving is eight out of ten. So, the percent surviving is 80%. All right, good. So, we've got 80% and now we follow that out. The next interval will be from one month to four months. ", ['curve', 'amount', 'patient', 'patient', 'complications', 'surgery', 'complications', 'cancer', 'Okay', 'result', 'curve', 'drop', 'Well', 'point', 'patients', 'risk', 'patients', 'percent', 'percent', '%', 'right', '%', 'interval', 'month', 'months']), 0.1643989873053573, 0.15178741411342264)
QUERY: When calculating the normal approximation for the binomial do we use half intervals e.g. 111.5 instead of 112 if we're looking for the area to the right in the curve; and can we use a different normal distribution applet than the ones recommended. If so, what are the answer tolerances. Can one be several tenths of a percentage point off. This ambiguity is killing me. Literally, I'm dying.
*************************
((2, '/Users/jag/Downloads/Stanford medstats/HRP262 Unit 2 Module 1.srt', "times actually follow something like an exponential distribution. Function. If you can pin that down then that's obviously you can fit those data to a pretty nice, smooth mathematical model. So I'm just going to be using t rather than x to start with here since eventually we're just going to be talking about probability distributions for the, survival times. So this is an exponential function I've written out here. You do not have to do any intervals in this Course, but just totally for fun, I'm going to integrate this curve from zero to infinity. It starts at zero, there's no negatives here on an exponential. From 0 to infinity, if I integrate this curve, I can show you indeed that the area under the curve is one. If you totally hate integrals and you don't want to see this, feel free to forward the video. But just for fun, here's my function. [SOUND] And it's an easy function to integrate actually because the exponential the derivative of the exponential is just the exponential. ", ['times', 'something', 'distribution', 'Function', 'data', 'model', 't', 'x', 'probability', 'distributions', 'survival', 'times', 'function', 'intervals', 'Course', 'fun', 'curve', 'infinity', 'zero', 'negatives', 'infinity', 'curve', 'area', 'curve', 'integrals', 'video', 'fun', 'function', '[', 'function']), 0.1767766952966369, 0.15021930516492848)
((15, '/Users/jag/Downloads/Stanford medstats/Unit 4 module 5 part 1.srt', "The column is the .06 part. And, I get an area, the area to the left, remember the charts are setup to always give you the area to the left. So this is giving me the area to the left. The area to the left, there's 2.46 which we know is way out in the tail. That area, not surprisingly is very big, 99.31%. So what's the chance, what's the area to the right then? How do you get the area to the right? You're going to have to subtract 1 minus .9931 will give me the area to the right. Which turns out to be .69%, a pretty small, percentage there. Because as you don't tend to have babies that heavy. Let's answer the 2nd practice question, what's the chance of obtaining a birth weight of 120 or lighter? Again, let's draw out the picture, so again, we're on a normal curve where the mean is 109. The standard deviation is 13. I want to know, what's the chance of obtaining a birth rate of 120 or ", ['column', 'part', 'area', 'area', 'left', 'remember', 'charts', 'area', 'left', 'area', 'left', 'area', 'left', 'way', 'tail', 'area', '%', 'chance', 'area', 'right', 'area', 'right', 'minus', 'area', 'right', '%', 'percentage', 'babies', 'Let', 'answer', 'practice', 'question', 'chance', 'weight', 'draw', 'picture', 'curve', 'deviation', 'chance', 'rate']), 0.26064301757134345, 0.1441501302689977)
((6, '/Users/jag/Downloads/Stanford medstats/Unit 4 module 5 part 1.srt', "Well 575 wasn't on one of those boundaries that I just showed you, it doesn't fall at either 1 standard deviation or 2 standard deviations or 3 standard deviations away from the mean. So our 68-95-99.7 rule doesn't help us here. So you might think oh, no we're going to have to go to calculus and do an integral to figure this out, right. I mean you would get the answer if you, if you know the math and I'm like, here's my normal curve. Here's my mean of 500. It's got a standard deviation of 50, and I literally could integrate from negative infinity all the way up to 575. I could do this integral under the curve, right? Now, again, I'm not going to, you're not going to have to do this, I promise. But that would be mathematically the w-, one way. To, to solve that, you could actually do that integral and find out that, that area under the curve, and that would be the probability of getting 575 or less. But here's the integral again, just for fun. Solve this integral? No thanks. It looks like a pain to solve, and we don't want to, ", ['Well', 'boundaries', 'deviation', 'deviations', 'deviations', 'rule', 'answer', 'math', 'curve', 'deviation', 'infinity', 'way', 'curve', 'w-', 'way', 'find', 'area', 'curve', 'probability', 'again', 'fun', 'Solve', 'thanks', 'pain']), 0.17010345435994295, 0.13943762167691143)
((0, '/Users/jag/Downloads/Stanford medstats/Unit 1 Module 3 part 2.srt', "Now we're talking about the different shapes of the distribution as a left-skewed or right-skewed, so let me just summarize this. So left-skewed means that you have a left tail. So you have a few extreme values that are low to the left. Right-skewed means you have a right tail, a few extreme values that are high to the right. And of course symmetric means just what it sounds like, you have a nice symmetric distribution. There's a special type of symmetric distribution called a bell curve or a normal distribution that we're going to talk about a lot in this course. So I'm just going to introduce it now to just get you thinking about it. So a bell curve is just what it sounds like, it follows the shape of a bell. It turns out to be very important in statistics for a number of reasons. So I'll just say a little bit more about it now. So the reason it's important is, for one thing, it has sort of a predictable behavior that makes it very useful. So it turns out that if you're on a perfect normal curve that 68% of your observations will fall somewhere between one standard deviation below the mean to one standard deviation above the mean. ", ['shapes', 'distribution', 'means', 'tail', 'extreme', 'values', 'left', 'means', 'right', 'tail', 'extreme', 'values', 'right', 'course', 'means', 'distribution', 'type', 'distribution', 'bell', 'curve', 'distribution', 'lot', 'course', 'bell', 'curve', 'shape', 'bell', 'statistics', 'number', 'reasons', 'bit', 'reason', 'thing', 'sort', 'behavior', 'perfect', 'curve', '%', 'observations', 'deviation', 'deviation']), 0.20164982172669937, 0.13842260594529063)
((0, '/Users/jag/Downloads/Stanford medstats/Unit 4 Module 6 part 1.srt', "In this next module, I'm going to to talk about the normal approximation to the binomial. So whenever you have a binomial distribution, you might have noticed in the earlier modules that a lot of the binomial distributions when you do out all the probabilities and you plot them, they look an awful lot like a normal distribution. So it turns out that as long as you're on a binomial, where n times p is greater than 5, that is, the expected value is greater than 5, it usually looks an awful lot like a normal. And we can approximate that binomial, those binomial probabilities, with a normal distribution. Let me just remind you of some of the examples that we saw. So for example, we saw this one in the upper left hand right corner was the wrinkle study, n here was 10 patients, P was 0.5. That, get, exactly gives us an np of five. And p times p is equal to five, hey, that looks pretty much like a normal curve to me. Right? That's very close to a normal curve of course with jagged edges, but still very close, and, ", ['module', 'approximation', 'distribution', 'modules', 'lot', 'distributions', 'probabilities', 'lot', 'distribution', 'n', 'times', 'value', 'lot', 'probabilities', 'distribution', 'Let', 'examples', 'example', 'left', 'hand', 'wrinkle', 'study', 'n', 'patients', 'P', 'np', 'times', 'looks', 'curve', 'curve', 'course', 'edges']), 0.16984155512168939, 0.13563998000379562)
((6, '/Users/jag/Downloads/Stanford medstats/Unit 5 Homework with answers.srt', "That it will simply be smoother because as we have more and more observations the center limit harem will start kicking in, and we will have a more and more normal looking distribution. When we have ten times more values, this distribution will look ten times smoother. The correct answer to 'D' is two. The sample size of the hypothetical trial is larger, which is the reason that the 'p' value is lower. This question gives us the percentage of US adults Who had five or more drinks in one day at least once in a given year. And we have trends for, let's see, we have trends for seven years. We have the mean value as well as the 95% confidence intervals given. The question wants us to find for which years we could reset the null hypothesis that more than 20% of US adults had five or more drinks in one day. At a 0.25 significance level. The answer is 2000 and 2003 only. ", ['observations', 'center', 'limit', 'harem', 'distribution', 'times', 'values', 'distribution', 'times', 'correct', 'answer', 'sample', 'size', 'trial', 'reason', 'value', 'question', 'percentage', 'US', 'adults', 'drinks', 'day', 'year', 'trends', 'see', 'trends', 'years', 'value', '%', 'confidence', 'intervals', 'question', 'years', 'null', 'hypothesis', '%', 'US', 'adults', 'drinks', 'day', 'significance', 'level', 'answer']), 0.14962640041614492, 0.13432197280602473)
((1, '/Users/jag/Downloads/Stanford medstats/Unit 4 module 1 part 2.srt', "And let me just write up what that function is. So, the exponential distribution is e raised to negative x. We call that the exponential distribution in probability. And that function is just a mathematical function, where the area under the curve is one. And we can kind of just draw a picture of that function to see that. You can't have negative values. So it starts at one and it kind of goes down towards the x-axis like that. Now, if I just told you that the area under that curve is 1, you'd probably say that sounds reasonable, right? If you were to calculate the area under the curve here, it starts at 1 and it goes down towards the x value, it seems reasonable that that area might be 1. So if you want to just believe me, that's, that's fine. I can also show you the interval for those who are interested. Again, this is optional, but we could take the integral over all possible values of this function. So you can have any value from the x, can be anything from 0 to infinity. If we take the integral of e raised to negative x. And we, we integrated over the, from 0 to infinity. We get a value of 1. Again, that, that integral is just there for those of you who want it, but ", ['function', 'distribution', 'e', 'x', 'distribution', 'probability', 'function', 'function', 'area', 'curve', 'picture', 'function', 'values', 'goes', 'towards', 'x-axis', 'area', 'curve', 'sounds', 'area', 'curve', 'towards', 'x', 'value', 'area', 'fine', 'interval', 'values', 'function', 'value', 'x', 'anything', 'infinity', 'e', 'x', 'infinity', 'value']), 0.18101654700955422, 0.12425888573430394)
((3, '/Users/jag/Downloads/Stanford medstats/Kaplan Meier Curves 1  How to Draw Them.srt', "So the curve is going to come down a certain amount. And one patient, one patient died from surgical complications, from surgery. And one died, from complications of the pancreatic cancer. It does kill very quickly. Okay. And as a result, the curve will drop down. Now, how much will the curve drop down? Well, at, at this point that we're talking about, there are ten patients at risk and two died. So, two out of ten patients died. And the percent surviving is eight out of ten. So, the percent surviving is 80%. All right, good. So, we've got 80% and now we follow that out. The next interval will be from one month to four months. ", ['curve', 'amount', 'patient', 'patient', 'complications', 'surgery', 'complications', 'cancer', 'Okay', 'result', 'curve', 'drop', 'Well', 'point', 'patients', 'risk', 'patients', 'percent', 'percent', '%', 'right', '%', 'interval', 'month', 'months']), 0.1342312110428049, 0.12393390465147046)
QUERY: How about Placebo percentage for >=5 Kg?
*************************
((18, '/Users/jag/Downloads/Stanford medstats/Unit 1 Homework Solutions with questions.srt', "So, again, we're going to go and find the hypertension row here, which is in the co-morbidity section, and we see that the percentage of the patients in the placebo group with hypertension is 73.9%. The next part of this question is asking us to calculate how many binary variables are shown, okay. So, the trick here is to look at how these variables are reported. If there's a fraction, if there's a percentage reported for a certain variable, you can assume that it's binary, okay. And that's because when there are two levels, if you know the percentage here this one's going, just going to be 100 just minus the percentage given in the other category. That's why there are no like second level or third level values reported here. So, to get back to the question, all the variables with percentages reported, our fractions here, are age grid equals 75, female sex, white race diabetes, ", ['hypertension', 'row', 'co-morbidity', 'section', 'percentage', 'patients', 'placebo', 'group', 'hypertension', '%', 'part', 'question', 'variables', 'okay', 'trick', 'variables', 'fraction', 'percentage', 'okay', 'levels', 'percentage', 'percentage', 'category', 'level', 'level', 'values', 'question', 'variables', 'percentages', 'fractions', 'age', 'equals', 'sex', 'race', 'diabetes']), 0.2956885083818292, 0.17639995504987763)
((4, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 9 overview.srt', "So you know that, that category is going to have the fewest number of people in it. What's a little tricky, is that when SAS actually puts the outmake as the output, it just calls them intercept 1, 2, 3, 4 or 5, but that 1, 2, 3, 4 and 5 don't necessarily correspond to the numbering that was used for the actual variable. They're just labeling them as they come. So you actually have to look at the intercept values and figure out which logit value here represents the smallest group, and that's going to be the smallest logit. That would represent the smallest percentage of people. You know that a percentage that's, a logit that's negative, is going to mean less than 50%. So that's going to be the smallest group. We're going to be using that intercept, then the predictive variable here is a dummy coded variable. This person that we want to get the predicted probability for had an outdoor job. So we're going to using the beta of 0.8817 they did not have a mixed job. So we're going to just plug in now the intercept for this person and their beta coefficient. ", ['category', 'number', 'people', 'tricky', 'SAS', 'outmake', 'output', 'numbering', 'intercept', 'values', 'figure', 'logit', 'value', 'group', 'logit', 'percentage', 'people', 'percentage', 'logit', '%', 'group', 'intercept', 'dummy', 'person', 'probability', 'outdoor', 'job', 'beta', 'job', 'plug', 'intercept', 'person', 'beta', 'coefficient']), 0.15161960871578067, 0.1283571565073631)
((17, '/Users/jag/Downloads/Stanford medstats/Unit 1 Homework Solutions with questions.srt', "So, the first one is asking us what the median weight of the placebo group is. So, we're immediately our attention is drawn to this column, okay. And then we want to find the weight variable. We notice that it's given, the median is reported, okay. So, the median in this case is going to be 81, specifically, 81 kilograms. What is the percent of participants in the ranolazine, I'm not sure if I'm pronouncing that right, in the, in the drug group basically, okay. So, you notice it says female sex here, that's the percentage of the participants who are female, which means that about 66% of those participants are male. The next question asks us about the number the percentage of participants rather in the placebo group who have hypertension. ", ['weight', 'placebo', 'group', 'attention', 'column', 'okay', 'weight', 'okay', 'case', 'kilograms', 'percent', 'participants', 'ranolazine', 'right', 'drug', 'group', 'okay', 'sex', 'percentage', 'participants', '%', 'participants', 'question', 'number', 'percentage', 'participants', 'placebo', 'group', 'hypertension']), 0.15032920560056579, 0.12726473530920718)
((16, '/Users/jag/Downloads/Stanford medstats/Unit 9 Module 1 part 1.srt', "A little complicated, but it's for mathematical reasons that we, we choose that. And just to illustrate, I took those, the same groupings, ten groups and five groups, where I had calculated the percent, the probability of being book smart in each of those groups. And I transformed that into a log odds. So, I took the percentage, divided by 1 minus the percentage, and then took the natural log of that number. This gives me some numbers like 0.5, 1.0, 1.5, 2.0 the, the log odds has no inherent meaning to But that's okay, because when we eventually will translate it back into something meaningful. But now, this outcome variable is a number that could theoretically go anywhere from negative infinity to positive infinity. Makes a very fit, nicer fitting align. I'm graphing here the logit for each of these groups, against again, the mean homework time in each of those groups. And, you could fit a line to that, and that's what we're doing in ", ['reasons', 'groupings', 'groups', 'groups', 'percent', 'probability', 'book', 'smart', 'groups', 'log', 'percentage', 'minus', 'percentage', 'log', 'number', 'numbers', 'log', 'meaning', 'okay', 'something', 'outcome', 'variable', 'number', 'infinity', 'infinity', 'Makes', 'nicer', 'align', 'logit', 'groups', 'homework', 'time', 'groups', 'line']), 0.1421338109037403, 0.12032673059695415)
((16, '/Users/jag/Downloads/Stanford medstats/Unit 9 Module 1.srt', "A little complicated but, it's for mathematical reasons that we, we We choose that. And just to illustrate, I took those, the same groupings, ten groups and five groups, where I had calculate the percent, the probability of being book-smart in each of those groups, and I transformed that into a logit. so that's, I took the percentage, divided by 1 minus the percentage, and then I took the natural log Of that number. This gives me some numbers 0.5, 1.0, 1.5, 2.0. the, the logit has no inherent meaning to But that's okay because when we eventually will translate it back into something meaningful. But now this outcome variable is a number that can theoretically go anywhere from negative infinity to positive infinity. Makes a very fit, nicer fitting line. I'm graphing here the logit for each of these groups against again the mean homework time in each of those groups. And you could fit a line to that. And that's what we're doing in logistic regression. ", ['reasons', 'groupings', 'groups', 'groups', 'percent', 'probability', 'book-smart', 'groups', 'logit', 'percentage', 'minus', 'percentage', 'log', 'number', 'numbers', 'logit', 'meaning', 'okay', 'something', 'outcome', 'variable', 'number', 'infinity', 'infinity', 'Makes', 'nicer', 'line', 'logit', 'groups', 'mean', 'homework', 'time', 'groups', 'line', 'regression']), 0.13900960937138318, 0.1176818640889484)
((16, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit3 Mod1 pt1.srt', "And just to illustrate, I took those, the same groupings, ten groups and five groups, where I had calculated the percent, the probability of being book smart in each of those groups and I transformed that into a log odd. So that is I took the percentage divided by 1 minus the percentage and then took the natural log of that number. This gives me some numbers like 0.5, 1.0, 1.5, 2.0. The, the log odds has no inherent meaning to me, but that's okay 'because when we, eventually we'll translate it back into something meaningful. But now, this outcome variable is a number that can theoretically go anywhere from negative infinity to positive infinity, makes a very fit, nice for fitting a line. I'm graphing here the logit for each of these groups against again, the mean homework time in each of those groups, and you could fit a line to that. And that's what we're doing in logistic regression; that's the line that we're fitting. So we're going to estimate an alpha and beta, an intercept and a slope for that particular line. ", ['groupings', 'groups', 'groups', 'percent', 'probability', 'book', 'smart', 'groups', 'log', 'odd', 'percentage', 'minus', 'percentage', 'log', 'number', 'numbers', 'log', 'meaning', 'okay', 'something', 'outcome', 'variable', 'number', 'infinity', 'infinity', 'fitting', 'line', 'logit', 'groups', 'homework', 'time', 'groups', 'line', 'regression', 'line', 'alpha', 'beta', 'intercept', 'slope', 'line']), 0.12598815766974242, 0.10665824697123312)
((16, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit3 Mod1 pt1_2.srt', "And just to illustrate, I took those, the same groupings, ten groups and five groups where I had calculated the percent, the probability of being book smart in each of those groups and I transformed that into a log on. So that as I took the percentage, divided one minus the percentage and then took the natural log of that number. This gives me some numbers like 0.5, 1.0, 1.5, 2.0. The, the log odds has no inherent meaning to me. But that's okay, because when we eventually will translate it back into something meaningful. But now this outcome variable is a number that can theoretically go anywhere from negative infinity to positive infinity. Makes a very fit, nicer fore-fitting a line. I'm graphing here the logit for each of these groups against, again, the mean homework time in each of those groups. And you could fit a line to that. And that's what we're doing in logistic regression. That's the line that we're fitting. So we're going to estimate an alpha and a beta, an intercept and a slope for that particular line. ", ['groupings', 'groups', 'groups', 'percent', 'probability', 'book', 'smart', 'groups', 'log', 'percentage', 'minus', 'percentage', 'log', 'number', 'numbers', 'log', 'meaning', 'okay', 'something', 'outcome', 'variable', 'number', 'infinity', 'infinity', 'Makes', 'nicer', 'fore-fitting', 'line', 'logit', 'groups', 'homework', 'time', 'groups', 'line', 'regression', 'line', 'alpha', 'beta', 'intercept', 'slope', 'line']), 0.1252448582170299, 0.10602898928489715)
((12, '/Users/jag/Downloads/Stanford medstats/Unit 1 Module 3 part 1.srt', "the range of values. So in this case, you know, exercise range from 0 to 12, and you divide it up in to equal intervals. So in this case I've made, we call those bins. I've made bins of two hours per week. So I've divided up exercise into zero. To two, two to four, four to six, six to eight, eight to 10, 10 to 12, 12 to 14. And then what this graph shows you is what percent of my observations falls within each of those bins. So the Y axis here is a percentage. It's the percent of observations that occur within each of those bin. And you can see by looking at this graph, it's immediately obvious that most of my students, the biggest number of students fall between two and four hours of exercise per week. So it's just a really nice graphic, because you can immediately see kind of where everybody lies. Let me go into a little bit more detail about how to read this graph. So again the Y axis is a percentage. So you would scroll across for example for this lowest bin, the zero to two bin. If I scroll across to the Y axis, ", ['range', 'values', 'case', 'range', 'intervals', 'case', 'bins', 'bins', 'hours', 'week', 'exercise', 'zero', 'graph', 'percent', 'observations', 'falls', 'bins', 'Y', 'percentage', 'percent', 'observations', 'bin', 'looking', 'graph', 'students', 'number', 'students', 'hours', 'exercise', 'week', 'everybody', 'lies', 'Let', 'bit', 'graph', 'Y', 'percentage', 'example', 'bin', 'zero', 'bin', 'Y']), 0.12451456127293807, 0.10541073917896683)
QUERY: It seem that authors have calculated adjusted OR using logistic regression and they got different OR?
*************************
((3, '/Users/jag/Downloads/Stanford medstats/Unit 9 Module 2.srt', "It would still give odds ratios, and in this case, I'm not sure it would have made a huge difference to the results of this study. But it's correlated data, so that would have actually been the appropriate statistical test to use. What's the consequence here that the authors used logistic regression instead of conditional logistic regression. We have a situation where were are comparing within groups. Remember when we were talking about correlated data, I told you that if you ignore the correlation when you're doing a within group comparison. That your peer leaders are going to be too high. That means you might miss in effect by not analyzing the data correctly. So here what you should do is compare each case to their matched control. cases, those case control groups are more similar to each other than to other people in the study. If you account for that correlation, you actually would've gotten probably here a little bit better P values than what the authors got. So their analysis could be described as a little conservative. ", ['odds', 'ratios', 'case', 'difference', 'results', 'study', 'data', 'test', 'consequence', 'authors', 'regression', 'regression', 'situation', 'groups', 'Remember', 'data', 'correlation', 'within', 'group', 'comparison', 'peer', 'leaders', 'effect', 'data', 'case', 'control', 'cases', 'case', 'control', 'groups', 'people', 'study', 'correlation', 'bit', 'P', 'values', 'authors', 'analysis']), 0.254000254000381, 0.21502990696112753)
((0, '/Users/jag/Downloads/Stanford medstats/Unit 9 Module 1 part 1.srt', "[BLANK_AUDIO]. In this next module, we're going to dive right into the logistic regression model. We've talked about, when you have binary categorical outcomes, that, that is when you're looking at proportions. We've talked about some statistical tests when you don't want to adjust for anything that you can use. So, this is like the risk different, relative risk, the chi-square test, the McNemar's chi-square test. So, we've talked about those. But what about when you want to do a regression analysis when you have a binary or categorical outcome? In that case, you're going to be using logistic regression, which we're going to talk about in this module. I want to just give you a little overview of logistic regression. So, to remind you when we did linear regression, the model looked like the following. We were predicting the value of some continuous outcome variable, call it Y as a function of X. Here's the simple linear regression model. ", ['[', 'BLANK_AUDIO', ']', 'module', 'regression', 'model', 'outcomes', 'proportions', 'tests', 'anything', 'risk', 'different', 'risk', 'test', 'McNemar', 'test', 'regression', 'analysis', 'outcome', 'case', 'regression', 'module', 'overview', 'regression', 'regression', 'model', 'value', 'outcome', 'call', 'Y', 'function', 'X', 'linear', 'regression', 'model']), 0.3937807653241084, 0.21330313510250834)
((0, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit4 Mod4 pt2.srt', "What's interesting about this particular example, one of the reasons I like to actually use this example, is that the authors made a point of illustrating something called residual confounding. And it's very important as we start talking about statistical adjustment for confounding, it's really important to keep in the back of your mind That adjustment for confounding is not a panacea. You cannot adjust away all confounding and that is kind of well illustrated here. So, this first regression analysis was actually included in the paper just as an illustration of residual confounding. What they did, in that very first regression analysis is they model age crudely. They put into their regression as a predictor you're either younger than 35 or you're 35 and older. So they model age as a binary predictor. When they do that, we are indeed attenuating the odds ratio. It's going from 0.8 to 0.87, but we are not completely wiping out the effect ", ['example', 'reasons', 'example', 'authors', 'point', 'something', 'confounding', 'adjustment', 'confounding', 'back', 'mind', 'adjustment', 'confounding', 'panacea', 'regression', 'analysis', 'paper', 'illustration', 'confounding', 'regression', 'analysis', 'age', 'regression', 'predictor', 'age', 'predictor', 'odds', 'effect']), 0.2672612419124244, 0.20703474211863468)
((22, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit3 Mod1 pt1_2.srt', "a particular person times that slope. What's this logistic regression equation useful for? So, just like when we talked about linear regression, there's many couple different pote, potential functions here. So one thing is we could use the logistic regression equation model to try to do some kind of prediction. This is very useful in clinical applications, because often we want to classify people as having a condition, or not having a condition, that's a binary outcome. So, if we're doing that kind of prediction, we would likely be using logistic regression. So we can try to come up with a set of predictors that predicts who's going to have a disease or not have a disease. So that's one of the functions of logistic regression prediction. A second function of logistic regression which is probably used even more often, is just to try to say, is a particular predictor, like homework, related to the outcome. ", ['person', 'times', 'slope', 'regression', 'equation', 'useful', 'regression', 'couple', 'different', 'pote', 'functions', 'thing', 'regression', 'equation', 'model', 'kind', 'prediction', 'applications', 'people', 'condition', 'condition', 'outcome', 'kind', 'prediction', 'regression', 'set', 'predictors', 'disease', 'disease', 'functions', 'regression', 'prediction', 'function', 'regression', 'predictor', 'homework', 'outcome']), 0.3796283011826483, 0.20563702939946668)
((22, '/Users/jag/Downloads/Stanford medstats/Unit 9 Module 1 part 1.srt', "So, this is a continuous predictor. So, we would be multiplying the amount of homework for a particular person times that slope. What's this logistic regression equation useful for? So, just like when we talked about linear regression, there was many, a couple different potential functions here. So, one thing is we could use the logistic regression equation model to try to do some kind of prediction. This is very useful in clinical applications because often we want to classify people as having a condition or not having a condition. That's a binary outcome. So, if we're doing that kind of prediction, we would likely be using logistic regression. So, we can try to come up with set of predictors that predicts who's going to have a disease or not have a disease. So, that's one of the functions of logistic regression, prediction. A second function of logistic regression which is probably more, used even more often, is just to try to say is a particular predictor, ", ['predictor', 'amount', 'homework', 'person', 'times', 'slope', 'regression', 'equation', 'useful', 'regression', 'couple', 'different', 'potential', 'functions', 'thing', 'regression', 'equation', 'model', 'kind', 'prediction', 'applications', 'people', 'condition', 'condition', 'outcome', 'kind', 'prediction', 'regression', 'set', 'predictors', 'disease', 'disease', 'functions', 'regression', 'prediction', 'function', 'regression', 'predictor']), 0.37741470621203443, 0.20443796944362944)
((27, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 9 mod2.srt', "All you have to do is specify that you want to stratify on pair in this case. Or in this case, stratify on person, because they're, your eyes are correlated within a person. So you'd stratify on, say, the subject's ID number. And when I do that, I do the correct conditional logistic regression, here's what I get. I get, notice, no intercept, because it's conditional logistic regression. I get my beta coefficient, and I get my p-value. Notice, now that my p-value is statistically significant, the p-value from McNemar's was 0.029. So I don't get exactly the same p-value, but they're extremely close. Again, McNemar's is a little bit different mathematically than doing a likelihood estimation method. Applying conditional logistic regression really is pretty straightforward. The interpretation is the essentially the same as before. We're going to get odds ratios out. You'd want to do you can't really do a logit plot within strata, so sometimes we might just do an you know, we might do a logit plot where we don't account for the correlation, just being able to plot something if you've got a continuous predictor. ", ['pair', 'case', 'case', 'stratify', 'person', 'eyes', 'person', 'subject', 'ID', 'number', 'correct', 'regression', 'notice', 'intercept', 'regression', 'beta', 'coefficient', 'Notice', 'McNemar', 'McNemar', 'bit', 'different', 'likelihood', 'estimation', 'method', 'regression', 'interpretation', 'odds', 'ratios', 'logit', 'plot', 'strata', 'logit', 'plot', 'correlation', 'plot', 'something', 'predictor']), 0.2626128657194451, 0.20343386322015522)
((22, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit3 Mod1 pt1.srt', "So, just like when we talked about linear regression, there's many, a couple different pote-, potential functions here. So one thing is we could use the logistic regression equation model to try to do some kind of prediction. This is very useful in clinical applications, because often we want to classify people as having a condition or not having a condition, that's a binary outcome. So if we're doing that kind of prediction, we would likely be using logistic regression. So we can try to come up with a set of predictors that predicts who's going to have a disease or not have a disease. So that's one of the functions of logistic regression prediction. A second function of logistic regression which is probably used even more often, is just to try to say, is a particular predictor like homework, related to the outcome, in this current case, book smart or street smart? Are those variables related, is a treatment related to an outcome? And that's very useful in, in and of itself and ", ['regression', 'couple', 'different', 'pote-', 'functions', 'thing', 'regression', 'equation', 'model', 'kind', 'prediction', 'applications', 'people', 'condition', 'condition', 'outcome', 'kind', 'prediction', 'regression', 'set', 'predictors', 'disease', 'disease', 'functions', 'regression', 'prediction', 'function', 'regression', 'predictor', 'homework', 'outcome', 'case', 'book', 'smart', 'street', 'smart', 'Are', 'variables', 'treatment', 'outcome']), 0.3312945782245396, 0.199297868471253)
((2, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 9 overview.srt', "They wanted to say whether or not your occupation, if you work outside or indoors and outdoors, or only indoors, does that contribute to wrinkling? So here's some SAS output, and this is something I might do on the final exam is just to give you some out put from SAS, from SAS, and have you interpret it. So the question, this question is asking you what type of analysis did the authors want to generate this output? So you look at the output, you'll notice that they're five intercepts in this model. So that's a clue that this either going to be ordinal logistic regression or multinomial logistic regression. And I was very kind when I made this question because I didn't even put multinomial as a, as a choice here. So you can actually get this down to ordinal logistic regression pretty easily. However, what if I'd also given the choice of multinomial logistic regression? How would you know that this was ordinal and not multinomial? Well, you'd know that because there's only a single beta estimate, a single beta parameter, for each of the predictors in the model. ", ['occupation', 'indoors', 'outdoors', 'indoors', 'contribute', 'wrinkling', 'SAS', 'output', 'something', 'exam', 'SAS', 'SAS', 'question', 'question', 'type', 'analysis', 'authors', 'output', 'output', 'intercepts', 'model', 'clue', 'either', 'regression', 'regression', 'question', 'choice', 'down', 'regression', 'choice', 'regression', 'Well', 'beta', 'estimate', 'beta', 'parameter', 'predictors', 'model']), 0.2867696673382022, 0.1942173014895162)
QUERY: Does this mean logistic regression always gives adjusted ratios and the manually computed ratios are unadjusted?
*************************
((2, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 8 mod5.srt', "It's just a simple line of code. And so this is great because now you can take any situation where you've got binary outcome data and rather than running a logistic regression you could instead choose to simply run a Poisson regression with this modification. What does that buy you? Well, if you have a common outcome what that buys you is that you're going to get risk ratios. The Poisson gives you these incidence rate ratios which can be interpreted as risk ratios. Risk ratios are not misleading if you have a common outcome, odds ratios are misleading. I'll just refer you to the reference for this modification it's a 2004 paper. This author shows, through simulation, that this is a reasonable thing to do with binary outcome data. Now I haven't seen this applied too widely in the literature, but one can make an argument that whenever you have a common outcome, it's probably preferable to analyze the data this way, than to do them in logistic regression. ", ['line', 'code', 'situation', 'outcome', 'data', 'regression', 'choose', 'Poisson', 'regression', 'modification', 'buy', 'Well', 'outcome', 'buys', 'risk', 'ratios', 'Poisson', 'incidence', 'rate', 'ratios', 'risk', 'ratios', 'Risk', 'ratios', 'outcome', 'ratios', 'reference', 'modification', 'paper', 'author', 'simulation', 'thing', 'outcome', 'data', 'literature', 'argument', 'whenever', 'outcome', 'data', 'way', 'regression']), 0.49382916465843113, 0.3931449471439553)
((4, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 3 Module 1 Part 2.srt', "Obviously I can't name too many screenwriters who were, won Academy Awards whereas I can probably name a few actors and actresses. That's probably winning an Academy Award screenwriting doesn't give you the same kind of social status and visibility that an actor does for an actor or an actress. And maybe you know people who are really great screenwriters, they drink more and do more drugs, who knows, I, I'm just making things up but, in any case it was sort of interesting that it turned out that way. But that's the Kaplan-Meier curve. Then you want to adjust for confounders, get effect sizes so you can run a Cox regression and that's what the authors did next, they ran a Cox regression on the screenwriter data, that's what's shown here. What you're seeing, are the results of that regression. They're showing you, they're not showing you exactly the hazard ratios. They're showing you the relative increase in the death rate. So they're directly related to the hazard ratios, but they're doing a little bit of translation of the hazard ratios for you. They think it, perhaps, makes it easier to understand. But basically, in their basic analysis, when they didn't adjust for ", ['screenwriters', 'Academy', 'Awards', 'actors', 'actresses', 'Academy', 'Award', 'screenwriting', 'kind', 'status', 'visibility', 'actor', 'actor', 'actress', 'people', 'screenwriters', 'drugs', 'things', 'case', 'sort', 'way', 'curve', 'confounders', 'effect', 'sizes', 'Cox', 'regression', 'authors', 'Cox', 'regression', 'screenwriter', 'data', 'results', 'regression', 'hazard', 'ratios', 'increase', 'death', 'rate', 'hazard', 'ratios', 'bit', 'translation', 'hazard', 'ratios', 'analysis']), 0.4008918628686366, 0.38905658601153326)
((3, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit3 Mod1 pt1.srt', "And I referred to, in the second week of the course, we talked a lot about odds ratios, and I told you, hey, logistic regression spits out odds ratios, and this is where it comes from. The betas can be directly interpreted as odds ratios just by exponentiating, and the reason there's an exponential involved is because our outcome variable involves a natural log. So this is just the overview of where we're going this. Just keep in mind that the right hand side the equation's the same as with linear regression. So we're still fitting a line its just we're fitting the outcome variable, there's something trickier going on. Let me start here with a simple example using that example data set, data set on my Stanford students. One of the questions I asked my Stanford students is whether or not they considered themselves to be book-smart or street-smart. This is a binary variable. Book smart the, the variable is equal to one, that means that they consider themselves book smart. If the variable is equal to zero, the way I've coded it, ", ['week', 'course', 'lot', 'odds', 'ratios', 'regression', 'spits', 'odds', 'ratios', 'betas', 'odds', 'ratios', 'exponentiating', 'reason', 'outcome', 'involves', 'log', 'overview', 'mind', 'right', 'hand', 'side', 'equation', 'regression', 'line', 'outcome', 'something', 'Let', 'example', 'example', 'data', 'data', 'Stanford', 'students', 'questions', 'Stanford', 'students', 'Book', 'smart', 'book', 'smart', 'way']), 0.3666793988112845, 0.3817514614881096)
((1, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod2.srt', "Therefore, you cannot estimate risk ratios or rate ratios. It turns out that with a case-control study, the only valid measure of relative risk is the odds ratio. The odds ratio can therefore be applied in the case control study, and since it's the only relative risk that works, we use it there. A more important reason that odds ratios come up so much, however, is just simply the fact that even when we're not dealing with a case control study, a lot of times when you have covert studies. Or even randomized trials, as we saw in the last modules, or cross sectional studies, a lot of authors will choose to use logistic regression because it's the most commonly used regression method for a binary outcome. When you run a logistic regression it gives you odds ratios, not risk ratios. So you're just kind of stuck with odds ratios and you have to be very careful about what you do with those and how you report those. this class is largely about logistic regressions. Not surprisingly, we're going to see awful lot of odds ratios in this class. ", ['Therefore', 'risk', 'ratios', 'rate', 'ratios', 'study', 'valid', 'measure', 'risk', 'odds', 'odds', 'case', 'control', 'study', 'risk', 'reason', 'ratios', 'fact', 'case', 'control', 'study', 'lot', 'times', 'studies', 'trials', 'modules', 'cross', 'studies', 'lot', 'authors', 'regression', 'regression', 'method', 'outcome', 'regression', 'ratios', 'ratios', 'stuck', 'odds', 'ratios', 'report', 'class', 'regressions', 'lot', 'odds', 'ratios', 'class']), 0.5571524965427928, 0.37405920115449814)
((2, '/Users/jag/Downloads/Stanford medstats/Unit 2 Module 4 part 3.srt', "So they used logistic regression. And as I said, logistic regression gives you odds ratios. And if you read very carefully in the little footnote here, you'll notice it says. Odds ratios were calculated with logistic regression. And they even tell you all the confounders they adjusted for. This was obviously an observational study. There are lots of potential confounders that we want to adjust for. So they adjusted for things like age, sex, cholesterol, cognitive score lots of things. So that's why they're reporting odds ratio because they did this multivaried adjustment. Logistic regression, logistic regression gives you odds ratios. I'm now going to walk you through how to calculate odds ratio's. Using a, of course, the unadjusted percentages. We're going to use the unadjusted numbers we're going to get slightly different oz ratios then the authors got when they used this multivariated adjustment because, of course, when things are adjusted they're going to there going to shift slightly. But we're going to get, actually you'll see we get pretty similar numbers. The first step, before we can calculate an odds ratio, we have to calculate an odds for each group. ", ['regression', 'regression', 'ratios', 'footnote', 'Odds', 'regression', 'confounders', 'study', 'lots', 'confounders', 'things', 'age', 'sex', 'cholesterol', 'score', 'lots', 'things', 'odds', 'adjustment', 'regression', 'regression', 'ratios', 'odds', 'course', 'percentages', 'numbers', 'oz', 'ratios', 'authors', 'adjustment', 'course', 'things', 'numbers', 'step', 'odds', 'odds', 'group']), 0.43583586846268, 0.3663051623525364)
((0, '/Users/jag/Downloads/Stanford medstats/Unit 2 Module 4 part 2.srt', "All right, so turning now from the Vioxx story. I went over rate ratios and risk ratios. I now want to turn to hazard ratios which again are directly related to rate ratios. We're going to see these later in the course, so you don't have to worry about all the details now. The hazard ratio can be interpreted just like a rate ratio. I'm just going to walk you through it a little bit, so that you'll be familiar with it if you see it in the literature. Again we'll, we'll talk in more detail. Later in the course. So hazard ratios are calculated using, a type of regression technique called Cox regression. This is a complimentary regression. They are usually multivariant adjusted. And I'll just show you an example of a hazard ratio in the literature. This was a study comparing a drug, Ranolazine to a placebo. And they were looking at the outcome. It was a composite outcome. They were looking at death, myocardial infarction, heart attack, or recurrent ischemia, they wanted to know if they could reduce that composite outcome with this drug. Here is the hazard ratio I have circled it for you for the primary endpoint, turned ", ['right', 'Vioxx', 'story', 'rate', 'ratios', 'ratios', 'ratios', 'rate', 'ratios', 'course', 'details', 'hazard', 'ratio', 'rate', 'ratio', 'bit', 'literature', 'detail', 'course', 'hazard', 'ratios', 'type', 'regression', 'technique', 'Cox', 'regression', 'regression', 'example', 'hazard', 'ratio', 'literature', 'study', 'drug', 'Ranolazine', 'placebo', 'outcome', 'outcome', 'death', 'infarction', 'heart', 'attack', 'recurrent', 'ischemia', 'outcome', 'drug', 'hazard', 'ratio', 'endpoint']), 0.4562105641233377, 0.363196204182959)
((2, '/Users/jag/Downloads/Stanford medstats/Unit 2 Module 4 part 2.srt', "Because, at the investigator goes out and finds cases. So the proportion of cases in the study just reflects the study design. It does not say anything about the prevalence or risk of disease in the general population. Therefore we can't estimate the risk or prevalence of disease in the general population. Since we can't estimate disease frequency, we can't estimate risk ratios or weight ratios. It turns out that the odds ratio, for mathematical reasons, is valid though. The more important reason that you're going to see odds ratios all over the medical literature is because of logistic regression. So even when you're doing cohort studies and cross-sectional studies. Author's may choose to run a statistical technique called logistic regression. When you have a binary outcome and you want to adjust for confounders the typical statistical test that people use is logistic regression. It happens that logistic regression gives you out odds ratios. So that's what you get out of the logistic regression, that's what people usually report. And this means there's a lot of logistic regression is run in the literature, ", ['investigator', 'finds', 'cases', 'proportion', 'cases', 'study', 'study', 'design', 'anything', 'prevalence', 'disease', 'population', 'Therefore', 'risk', 'prevalence', 'disease', 'population', 'disease', 'frequency', 'risk', 'ratios', 'weight', 'ratios', 'odds', 'reasons', 'reason', 'odds', 'ratios', 'literature', 'regression', 'cohort', 'studies', 'studies', 'Author', 'technique', 'regression', 'outcome', 'confounders', 'test', 'people', 'regression', 'regression', 'odds', 'ratios', 'regression', 'people', 'report', 'lot', 'regression', 'literature']), 0.4830458915396479, 0.35666312261495925)
((3, '/Users/jag/Downloads/Stanford medstats/Unit 9 Module 1 part 1.srt', "if you exponentiate them, what it gives you out is Is the odds ratio. And I referred to, in the second week of the course, we talked a lot about odds ratios, and I told you, hey, logistic regression spits out odds ratios. And, and this is where it comes from. The betas can be directly interpreted as odds ratios just by exponetiating. And the reason there's an exponential involved is because our outcome variable involves a natural look. So, this is just the overview of where we're going. Just keep in mind that the right hand side the equations the same as with linear regression. So, we're still fitting a line. It's just we're fitting the outcome variable, there's something trickier going on. Let me start here with a simple example, using that example data set, data set on my Stanford students, that I've been using since the beginning of the course. One of the questions I asked my Standford students is, whether or not they consider their selves to be book smart or street smart. This is a binary variable. ", ['Is', 'odds', 'week', 'course', 'lot', 'odds', 'ratios', 'regression', 'spits', 'odds', 'ratios', 'betas', 'odds', 'ratios', 'exponetiating', 'reason', 'outcome', 'involves', 'look', 'overview', 'mind', 'right', 'hand', 'side', 'equations', 'regression', 'line', 'outcome', 'something', 'Let', 'example', 'example', 'data', 'data', 'Stanford', 'students', 'beginning', 'course', 'questions', 'Standford', 'students', 'selves', 'book', 'smart', 'street', 'smart']), 0.34236839400873026, 0.3564411723262253)
QUERY: In this lecture, I learned that the correlation coefficient zero doesn't necessarily means there is no  correlation. It just means there are no linear correlation.  I thought that can be applied to the covariance, too. If two variables are independent, the covariance must be 0. but the converse of this sentence is false, isn't it?  So i think, the quiz doesn't have any correct answer.
*************************
((7, '/Users/jag/Downloads/Stanford medstats/Unit 8 Module 1 part 1.srt', "So that's pretty high. As you can see from the graphc, there is a strong correlation here. The p value is less than 0.0001. The null hypothesis here is that r is zero. Because an r of zero would mean no correlation. So all that p-value tells us here is that we can be pretty confident that these two variables are really correlated. That there is some correlation here. I'm going to now walk you through where the correlation coefficient comes from. I haven't told you that yet in the course, now I'm going to tell you where it comes from. So to understand the correlation coefficient we have to actually go back and introduce a new concept called covariance. So let me start by talking about covariance and then you'll see how covariance leads into the correlation coefficient. So here I'm going to start with the formula for covariance. The formula is actually instructive here. Covariance is evaluating the relationship between 2 variables. But if you look carefully at this covariance formula you might recognize that it looks awfully similar to the formula for ", ['graphc', 'correlation', 'p', 'value', 'null', 'hypothesis', 'r', 'r', 'zero', 'correlation', 'tells', 'variables', 'correlation', 'correlation', 'coefficient', 'course', 'correlation', 'coefficient', 'concept', 'covariance', 'let', 'talking', 'covariance', 'covariance', 'leads', 'correlation', 'coefficient', 'formula', 'covariance', 'formula', 'Covariance', 'relationship', 'variables', 'covariance', 'formula', 'formula']), 0.6543303050815759, 0.5314052918575154)
((14, '/Users/jag/Downloads/Stanford medstats/Unit 8 Module 1 part 1.srt', "You'll end up with a unitless quantity. So it turns out if you divide the covariance by the standard deviation of x and the standard deviation of y, you divide out the units and you always end up with a value that will range between negative 1 and positive 1. And this is the Pearson's correlation coefficient, that measure that we've talked about earlier in the course. It's simply a standardized covariance. We could have just dealt directly with the covariance if there weren't this unit issue. But by standardizing you are making something, a correlation coefficient that has no units and something that always ranges between negative 1 and positive 1. It's comparable, you can compare it across many, many different situations, which is really nice. [SOUND] Again, the Pearson's correlation coefficient is just the standardized covariance of and I'm presenting the formula once again here. Again, you're probably not going to have to calculate this by hand, but if you ever wanted to calculate this by hand. This looks messy, but all this is, is the numerator is covariance, which I just showed you. ", ['unitless', 'quantity', 'covariance', 'deviation', 'x', 'deviation', 'y', 'units', 'value', 'Pearson', 'correlation', 'coefficient', 'measure', 'course', 'covariance', 'covariance', 'unit', 'issue', 'standardizing', 'something', 'correlation', 'coefficient', 'units', 'something', 'situations', '[', 'Again', 'Pearson', 'correlation', 'coefficient', 'covariance', 'formula', 'hand', 'hand', 'looks', 'numerator', 'covariance']), 0.49503867640745053, 0.49540452307301774)
((12, '/Users/jag/Downloads/Stanford medstats/Unit 8 Module 1 part 1.srt', "I just want to point out again, this formula's very similar to the formula for variance. Remember the formula for variance looks like this. It was xi minus x bar, the deviation for x, squared. So, you can think of a variance as just a covariance between a variable with itself. So, there's a direct similarity here. And this formula is not too had to understand. Just to summarize, again, if x and y are independent variables, then the covariance will be 0. If x and y are positively correlated, then the covariance will be above 0, will be positive. If x and y are inversely correlated, then the covariance will be negative. Now, notice that sounds awfully similar to what I told you earlier about the correlation coefficient. And indeed, there's a direct relationship between covariance and the correlation coefficient. For covariance, the problem with covariance as a measure is that it completely depends on the units. So think about what the units of covariance are going to be. If you're talking about the covariance between height and weights. ", ['point', 'formula', 'formula', 'variance', 'Remember', 'formula', 'variance', 'looks', 'bar', 'deviation', 'x', 'variance', 'covariance', 'similarity', 'formula', 'x', 'y', 'variables', 'covariance', 'x', 'y', 'covariance', 'x', 'y', 'covariance', 'notice', 'sounds', 'correlation', 'coefficient', 'relationship', 'covariance', 'correlation', 'coefficient', 'covariance', 'problem', 'covariance', 'measure', 'units', 'think', 'units', 'covariance', 'covariance', 'height', 'weights']), 0.4296009334548942, 0.3726242792622075)
((4, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 9 Module 1.srt', "To make it be exchangeable or auto regressive to explicitly estimate the correlations between different time points. Well we're going to do basically the same thing as strategy one in our mixed model. So if your in SAS and you're using PROC MIXED. If you want to fit a correlation matrix the errors, we're going to be including what's called a repeated statement. So you would be including a repeated statement. And that repeated statement is going to look very similar to the repeated statement that we saw in PROC GENMOD. You're going to be specifying type equals where you specify a particular covariance structure. We're actually going to be specifying the covariance structure rather than the correlation structure but same idea. So you might specify for example that you want a compound symmetry covariance structure. By explicitly estimating the within subject correlation, we are in that way correcting for it. And so that would involve the repeated statement in PROC MIXED. ", ['auto', 'correlations', 'time', 'points', 'Well', 'thing', 'strategy', 'model', 'SAS', 'PROC', 'MIXED', 'correlation', 'matrix', 'errors', 'statement', 'statement', 'statement', 'statement', 'PROC', 'GENMOD', 'type', 'equals', 'covariance', 'structure', 'covariance', 'structure', 'correlation', 'structure', 'idea', 'example', 'compound', 'symmetry', 'covariance', 'structure', 'within', 'subject', 'correlation', 'way', 'statement', 'PROC', 'MIXED']), 0.3110855084191276, 0.36789558598156197)
((13, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 9 Module 1.srt', 'the, asking for the marginal model, using the repeated statement. And in this, these results, I had chosen an exchangeable correlation matrix. So for PROC MIXED, I went ahead and chose compound symmetry. Compound symmetry means exchangeable correlation plus, that the same, the variance is, at different time points are all the same homogeneity of variances. So I went ahead and chose compound symmetry. I also, specifically requested a robust standard error just to get these as similar as possible. I just want to show you that you get pretty much the same results. So, the P-values are 0.4, 0.0491, rather than 0.0456, 0.007 rather than 0.0057, 0.01079 rather than 0.0108. I mean these are really, really close. We are in the third decimal place before you see any difference. So these models are very much essentially doing the same thing. Again, I slightly prefer PROC MIXED here because you have more flexibility for that correlation covariance structure. ', ['model', 'statement', 'results', 'correlation', 'matrix', 'PROC', 'MIXED', 'compound', 'symmetry', 'Compound', 'symmetry', 'correlation', 'variance', 'time', 'points', 'homogeneity', 'variances', 'compound', 'symmetry', 'error', 'results', 'P-values', 'place', 'difference', 'models', 'thing', 'PROC', 'MIXED', 'flexibility', 'correlation', 'covariance', 'structure']), 0.305085107923876, 0.36206289011574555)
((1, '/Users/jag/Downloads/Stanford medstats/Unit 8 Homework with Answers.srt', "Just kind of eyeballing this graphic here. Which Pearson's coefficient looks the most reasonable? So actually, it's D, negative 0.3. You can see that's there an awful lot of scatter around this line. This is really not a high correlation at all. So the correct answer is negative 0.3, it's negative certainly, because there our line is going down. But a negative 0.5 is actually too big here. So negative 0.3 is the correct answer. There is a little bit of correlation. So it's not 0.01 here. But it's, it's very weak. So it's point, negative 0.3. The correct answer to this is, is B. Actually, linear regression is probably not totally appropriate here. We have a fairly small sample size, and we're violating probably two of the assumptions of linear regression. So you can see that we have a lot of zeroes in terms of the alcohol exposure time in the movies. ", ['Pearson', 'coefficient', 'looks', 'D', 'lot', 'scatter', 'line', 'correlation', 'correct', 'answer', 'line', 'correct', 'answer', 'bit', 'correlation', 'point', 'correct', 'answer', 'B', 'regression', 'sample', 'assumptions', 'regression', 'lot', 'zeroes', 'terms', 'alcohol', 'exposure', 'time', 'movies']), 0.3676955262170047, 0.34758296148363627)
((5, '/Users/jag/Downloads/Stanford medstats/RExercise3.srt', "And then bring those two over, Pearson's is great. I don't need a, scatter plot this time, so I'm going to unselect that, and then I'm going to leave the options as we had them before. I'm going to hit Run. And I'm going to look at our output. So the correlation coefficient in this case is 0.1303. And it has a high p-value this time, meaning that it is not statistically significant. Notice that the p-value is greater than 0.05, it's 0.3936. Indicating that there's no statistically significant correlation between the two variables Mathlove and WritingLove. So the next question, question 3 asks us check the normality assumption of Pearson's correlation coefficient. So it's asking us to check the normality for the variables that we've used so far in this exercise, Mathlove, WritingLove, politics and Obama. ", ['Pearson', 'scatter', 'plot', 'time', 'options', 'Run', 'output', 'correlation', 'coefficient', 'case', 'time', 'Notice', 'correlation', 'variables', 'Mathlove', 'WritingLove', 'question', 'question', 'asks', 'normality', 'assumption', 'Pearson', 'correlation', 'coefficient', 'normality', 'variables', 'exercise', 'Mathlove', 'WritingLove', 'politics', 'Obama']), 0.35713746626659143, 0.3345075862329254)
((15, '/Users/jag/Downloads/Stanford medstats/Unit 8 Module 1 part 1.srt', "And of course you've, you''ve already calculated in this course before, the standard deviation. A standard deviation of a single variable, so the standard deviation of x and the standard deviation of y. So if you had to calculate this by hand if you understand this conceptually you should be able to calculate this by hand. Of course it's very tedious to do this by you would of course do this on a computer. But I think the formula here is, helps you to understand the concept of covariance and correlation. So correlation, the one thing to keep in mind that people forget is that the correlation is actually measuring the strength of the linear relationship between two variables. I'm going to show you in a minute that if the relationship is something other than linear, the correlation might, might not be picked up by the correlation coefficient. The correlation coefficient ranges again between negative 1 and positive 1. 0 means no correlation, means your variables are independent. Negative 1 means perfect inverse correlation, positive 1 means perfect positive correlation. So here's a picture that's showing you perfect positive correlation, ", ['course', 'course', 'deviation', 'deviation', 'deviation', 'x', 'deviation', 'y', 'hand', 'hand', 'course', 'course', 'computer', 'formula', 'helps', 'concept', 'covariance', 'correlation', 'correlation', 'thing', 'mind', 'people', 'correlation', 'strength', 'relationship', 'variables', 'minute', 'relationship', 'something', 'correlation', 'correlation', 'coefficient', 'correlation', 'coefficient', 'ranges', 'correlation', 'variables', 'means', 'inverse', 'correlation', 'means', 'correlation', 'picture', 'correlation']), 0.5554920598635309, 0.3314969275639038)
QUERY: Does odd ratio not statistically significance implies corresponding risk ratio (computed from odd ratio and prevalent rate) not statistically significance too ?
*************************
((3, '/Users/jag/Downloads/Stanford medstats/Unit 9 Module 5.srt', "It's very, very similar to a rate ratio except it's a ratio of two hazard rates. And a hazard rate is an instantaneous incidents rate, I'll talk about that in this module. Because we're doing regression often what comes out is in the adjusted hazard ratio, it's been adjusted for other things in the model. Again I mentioned this briefly, in the second week of the course just recall that I talked a little bit about hazard ratios In the context of the concept of relative risk. It's yet another measure of relative risk. And it has a similar interpretation as just a regular old basic rate ratio. It's the ratio of instantaneous incidence rates, however. So it's just slightly different from a rate ratio. It has to be calculated from regression. You're not going to be able to calculate it by hand. Since it comes from a regression, it's again, usually multivariable-adjusted. And I had presented this little example in the second week of the course, just to show you how to interpret a hazard ratio. They're actually straight forward to interpret. ", ['rate', 'ratio', 'ratio', 'rates', 'hazard', 'rate', 'incidents', 'rate', 'module', 'regression', 'hazard', 'ratio', 'things', 'model', 'briefly', 'week', 'course', 'bit', 'hazard', 'ratios', 'context', 'concept', 'risk', 'measure', 'risk', 'interpretation', 'rate', 'ratio', 'ratio', 'incidence', 'rates', 'rate', 'ratio', 'regression', 'hand', 'regression', 'example', 'week', 'course', 'hazard', 'ratio', 'forward']), 0.6811059575747279, 0.38780625098100724)
((0, '/Users/jag/Downloads/Stanford medstats/Unit 2 Module 4 part 1.srt', "In this next module, we're going to look at another measure of association called the relative risks. Instead of subtracting the risks in the different groups, what if we instead divided them to make a ratio, that's called a relative risk. There are several measures of relative risk and relative risks appear very frequently in the medical literature. If you take two incidents rates and divide them, that's called a rate ratio. You'll also see something in the literature called a hazard ratio. A hazard ratio is also a ratio of incidence rates. It's just a ratio of what we call instantaneous incidence rates or hazard rates. Hazard ratios actually come from a regression technique that we're going to talk about much later in the course. But for now, you can just interpret them exactly the same way you would interpret a rate ratio. If you divide two cumulative risks, or two prevalences, we call that a risk ratio. ", ['module', 'measure', 'association', 'risks', 'risks', 'groups', 'ratio', 'risk', 'measures', 'risk', 'risks', 'literature', 'incidents', 'rates', 'rate', 'ratio', 'something', 'literature', 'hazard', 'ratio', 'hazard', 'ratio', 'ratio', 'incidence', 'rates', 'ratio', 'incidence', 'rates', 'hazard', 'rates', 'Hazard', 'ratios', 'regression', 'technique', 'course', 'way', 'rate', 'ratio', 'risks', 'prevalences', 'risk', 'ratio']), 0.6797704848608184, 0.37409297253669643)
((9, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 8 mod4.srt', "because of an interaction term that means that this rate ratio depends on the year that we're talking about, technology has a different effect depending on the year. The easiest one to calculate it for is if I just look at 1969. So if I look at 1969 the interaction term will go to 0 because I've modeled this as year since 1969 so that will have a value, year will have a value of 0. This interaction term will go away. So at, in 1969, the ri, the risk ratio, the rate ratio for new technologies was e raised to the negative 0.27 which is a risk ratio of 0.76. So introducing a new technology in 1969 had little effect, if anything, it, there were less world records when that happened. However, scroll ahead 40 years later to 2009. [UNKNOWN]. What are we going to get in our model. So our risk ratio's going to be negative .27 but now we get the interaction term so I'm doing this very one, one new technology so ", ['interaction', 'term', 'rate', 'ratio', 'depends', 'year', 'technology', 'effect', 'year', 'interaction', 'term', 'year', 'value', 'year', 'value', 'interaction', 'term', 'ri', 'risk', 'ratio', 'rate', 'ratio', 'technologies', 'risk', 'ratio', 'technology', 'effect', 'anything', 'world', 'records', 'scroll', 'years', '[', 'UNKNOWN', ']', 'model', 'risk', 'ratio', 'interaction', 'term', 'technology']), 0.5172606001118717, 0.36472711194803714)
((3, '/Users/jag/Downloads/Stanford medstats/Unit 2 Module 4 part 1.srt', "we call that the null value. If you have a relative risk that's less than one, that indicates a protective effect. Whatever was in the numerator is protecting you from the outcome. If you have a relative risk that's greater than one, that indicates a harmful effect, an increased risk. Let's calculate the rate ratio first starting with the GI event data from the Vioxx study we've been talking about. Incident rates were 2.1 and 4.5 again. All we have to do here to make a rate ratio is to divide the two incidents rates. And notice because we're dividing two rates, the units here, the 100 person-years actually cancels out. And the rate ratio doesn't have any units, it's unitless. We come out with a rate ratio of 0.46. The interpretation is that Vioxx reduces the rate of GI events by 54%. I got that 54% simply by subtracting 1 minus 0.46. So that's a 40, 54% reduction in the rate of GI events. If I want to calculate the risk ratio for this same data, I'm just ", ['null', 'value', 'risk', 'effect', 'numerator', 'outcome', 'risk', 'effect', 'risk', 'Let', 'rate', 'ratio', 'GI', 'event', 'data', 'Vioxx', 'study', 'Incident', 'rates', 'rate', 'ratio', 'incidents', 'rates', 'notice', 'rates', 'units', 'person-years', 'rate', 'ratio', 'units', 'unitless', 'rate', 'ratio', 'interpretation', 'Vioxx', 'reduces', 'rate', 'GI', 'events', '%', '%', 'subtracting', 'minus', '%', 'reduction', 'rate', 'GI', 'events', 'risk', 'ratio', 'data']), 0.5839272948338148, 0.3617675303468835)
((5, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 8 mod5.srt', "eating disorders scored, just as before. But now I can take this beta coefficient and I can exponentiate it and what I get out are these relative risks, these risk ratios, rather than odds ratios. And if I had other things in the model, I would be getting out adjusted risk ratios. I am going to use the beta coefficient this computer output also gave me the 95% confidence interval for this beta coefficient, if I exponentiate these, I will get the 95% confidence interval for the odds ratio. I mean starting for the risk ratio. So, let me just do that now, so I'm going to exponentiate the 0.0653. When I exponentiate that, I get that the point estimate for my odds ratio is one, sorry for my risk ratio is 1.067. So this is an incidence rate ratio of course, in this case we're talking about cross sectional data. So it's really technically called a prevalence ratio, so you could interpret this as, for every one unit increase in the eating disorder score. ", ['disorders', 'beta', 'coefficient', 'risks', 'risk', 'ratios', 'odds', 'ratios', 'things', 'model', 'risk', 'ratios', 'beta', 'coefficient', 'computer', 'output', '%', 'confidence', 'interval', 'beta', 'coefficient', '%', 'confidence', 'interval', 'odds', 'risk', 'ratio', 'point', 'estimate', 'odds', 'risk', 'ratio', 'incidence', 'rate', 'ratio', 'course', 'case', 'cross', 'data', 'prevalence', 'ratio', 'unit', 'increase', 'eating', 'disorder', 'score']), 0.47149516679144476, 0.3554269315893449)
((3, '/Users/jag/Downloads/Stanford medstats/Unit 2 Homework with questions.srt', "The third part of this question is asking us about a rate ratio now remember whenever you see rate ratio you think time, okay, there's a component of time in this. So, in order to do. The, in order to calculate the rate ratio comparing Vioxx with Naproxen. Okay, what I'm going to do is take these numbers. Okay? So, we're going to do 20 over 2698 divided by 4 over 2699. Okay. And that's, what that's going to give us is a standardization. Across the number of person years of exposure we're standardizing against the exposure, the number of heart attacks in each group. You can think about it like that. And so the rate ratio here works out to be The last part of this question is asking us about the number needed to harm. And if you remember, the number needed to harm can simply be calculated as ", ['part', 'question', 'rate', 'ratio', 'remember', 'rate', 'ratio', 'time', 'okay', 'component', 'time', 'order', 'order', 'rate', 'ratio', 'Vioxx', 'Naproxen', 'Okay', 'numbers', 'Okay', 'Okay', 'standardization', 'number', 'person', 'years', 'exposure', 'exposure', 'number', 'heart', 'attacks', 'group', 'rate', 'ratio', 'part', 'question', 'number', 'harm', 'number', 'harm']), 0.4601578468344406, 0.33980069363725357)
((14, '/Users/jag/Downloads/Stanford medstats/Unit 9 Module 5.srt', "So they didn't go out and actually collect data ques, you know, questionnaire's on people, so they could only find limited information about these people on the internet. But they tried to adjust for basic confounders that they could find on the internet. So what you're looking at here is first of all, the very first row here, that's the hazard ratio from the basic analysis. So un-adjusted. So that corresponds kind of directly to the Kaplan-Meier curve. So the winners compared with the nominees, have a 37% higher rate of death, the higher mortality rate here, and it is statistically significant. Then they went on and tried to adjust for all of these different confounders that you see listed here. And you can see that as they adjust for confounders, the hazard ratio really doesn't budge much. And they finally at the end adjust for all the potential confounders at once. The hazard ratio there is 1.35. So basically there was no change in the hazard ratio, at least adjusting for these confounders. The ones that they had. ", ['data', 'ques', 'questionnaire', 'people', 'information', 'people', 'internet', 'confounders', 'internet', 'row', 'hazard', 'ratio', 'analysis', 'corresponds', 'curve', 'winners', 'nominees', '%', 'rate', 'death', 'mortality', 'rate', 'confounders', 'confounders', 'hazard', 'ratio', 'end', 'adjust', 'confounders', 'hazard', 'ratio', 'change', 'hazard', 'ratio', 'confounders', 'ones']), 0.41870402318820543, 0.3326290358768059)
((2, '/Users/jag/Downloads/Stanford medstats/Unit3 Module5 pt4.srt', "Finally, we include well risk after the forward slash and a table statement and everything else in Proc Freq is just the same as it was before. So, let's run this and look at the output. SAS will automatically calculate an odds ratio as well as a risk ratio. Depending on the type of study you're using, you might not be able to use a risk ratio. In this case, a risk ratio should work just fine. The second row shows the chance of getting cured in those who were treated versus those who were not treated. And the risk ratio is 2.1.  Next we'll take a look at another example from our data set on marriage. If we include this where statement, we can restrict the analysis to women only. In an earlier module we showed you how to run a by statement to run an analysis multiple times for different subsets of the data based on an additional variable. If you're using Proc Freq, there's another way how to do this. Take a look at this code, you'll see that we have three variables in the table statement. The last two variables, race and ", ['forward', 'slash', 'statement', 'everything', 'Proc', 'Freq', 'run', 'output', 'SAS', 'odds', 'risk', 'ratio', 'type', 'study', 'risk', 'ratio', 'case', 'risk', 'ratio', 'fine', 'row', 'chance', 'versus', 'risk', 'ratio', 'look', 'example', 'data', 'marriage', 'statement', 'analysis', 'women', 'module', 'statement', 'analysis', 'multiple', 'times', 'subsets', 'data', 'Proc', 'Freq', 'way', 'Take', 'look', 'code', 'variables', 'table', 'statement', 'variables', 'race']), 0.4482654517444207, 0.33101882861333504)
QUERY: I have a mundane question. In the videos, she usually states p-values as percentages. Do we use percentages or probability (decimals) for the answers? I hunted around the site and can't find a reference. If we use percentages, do we use the % sign? The instruction that we give our answer to 3 decimal places, thousandths, suggests to me, that we use decimals. Otherwise, it seems excessive compared to the answer to 11. I appreciate any information on the form of the answers. Percentages or decimals? If percentages, do we include the % sign? Thanks.
*************************
((16, '/Users/jag/Downloads/Stanford medstats/Unit 1 Homework Solutions with questions.srt', "The third question asks us what statistical measures are used to describe the central tendency and variability of the age variable. So, I already pointed this out, earlier when we were discussing this, but, it's reported with the median in the interquartile range as indicated here. The statistics used to describe age greater than or equal to 75 years is n and percentages, okay. So, this is the percentages of this end, essentially. So, 562 represents 17% of this overall cohort of 32, about 33 hundred people and 592 represents about 18% of this overall cohort of again, about 3,300 people. The next three questions ask us to, gather some values from this table. ", ['question', 'measures', 'tendency', 'variability', 'age', 'interquartile', 'range', 'statistics', 'age', 'years', 'percentages', 'okay', 'percentages', 'end', 'represents', '%', 'cohort', 'people', 'represents', '%', 'cohort', 'people', 'questions', 'values', 'table']), 0.27823802660725183, 0.41844626829889986)
((11, '/Users/jag/Downloads/Stanford medstats/Unit 3 Module 5 CME Alternate.srt', "So once you know you tested positive, you're on one of those branches, which of those is actually having cellulite? Only the top branch. So the probability that you're actually going to get the disease, given that you tested positive in this case, will be .546 divided by .259, plus .546, and that again, comes out to be 67.8%. So you can solve it this way if you're more comfortable still with the tree, and Bayes' Rule. And I'm going to show you even one more way to solve it. So alternatively we can use a little trick with 2 by 2 tables, and a lot of students find it easier to calculate it in this way, so if you're more comfortable with a 2 by 2 table, this is yet another way to get this right. So, to solve for positive predictive and negative predictive value, you could incorporate the true prevalence by just doing a little 2 by 2 table and assuming that you have a denominator of 100 people. Why a hundred, why am I picking a hundred here? Well, it's easy to deal with percentages. If you treat them as whole numbers, so if we start with 100 people, then all the percentages just become whole numbers. ", ['branches', 'cellulite', 'branch', 'probability', 'disease', 'case', '%', 'way', 'tree', 'Bayes', 'Rule', 'way', 'trick', 'tables', 'lot', 'students', 'way', 'way', 'right', 'value', 'prevalence', 'denominator', 'people', 'am', 'Well', 'percentages', 'numbers', 'people', 'percentages', 'numbers']), 0.20670265770077798, 0.3364195362022287)
((9, '/Users/jag/Downloads/Stanford medstats/MedStatsUnit3_Mod1_Part1.srt', "it's easy to read the chart. So, if you have 100 donors, somebody's figured out the prevalence of different blood types in the population. And out of 100 people, you'd expect 84 of them to be RH positive and 16 to be RH negative. And then within those, we get a breakdown of type O, versus Type A, versus Type B, versus Type A and B. And so, it's a very easy chart to read. Because rather than giving percentages or decimals, the person who put this chart together made the denominator a nice even hundred so it's easy to read this chart. So, for example, what's the probability that a random donor will be AB Positive? Well, as we see in the chart, 3 out of 100 donors are AB positive, so that answer will simply just be 3%. You can see this is quite easy to calculate when you set up the charts so nicely like this. What's the probability that a random donor will have a blood type of O? This one is just slightly harder to calculate. Because you have to recognize that there are O positives and ", ['chart', 'donors', 'somebody', 'prevalence', 'blood', 'types', 'population', 'people', 'RH', 'RH', 'breakdown', 'type', 'O', 'Type', 'Type', 'B', 'Type', 'B', 'chart', 'percentages', 'decimals', 'person', 'chart', 'denominator', 'chart', 'example', 'probability', 'random', 'donor', 'AB', 'Positive', 'Well', 'chart', 'donors', 'AB', '%', 'charts', 'probability', 'random', 'donor', 'blood', 'type', 'O', 'one', 'O', 'positives']), 0.14616085094950185, 0.29058837204354254)
((3, '/Users/jag/Downloads/Stanford medstats/Unit 4 Homework with questions.srt', "if we look up the probability distribution, [SOUND] we'll see that the p value corresponding to this Is 0.0668. Which is exactly the same value that we found in the previous question as well. This question is a straight-forward application of the binomial equation again. We have n equals 5. We are given that b is 0.513. And we want to find the probability Of x being 4. And this, recalling the expression that we had for the binomial distribution. And if we do the math here, we'll find that the answer is 16.9%. ", ['probability', 'distribution', 'SOUND', 'p', 'value', 'value', 'question', 'question', 'application', 'equation', 'equals', 'b', 'probability', 'x', 'expression', 'distribution', 'math', 'answer', '%']), 0.20043894080075436, 0.23368144957488884)
((5, '/Users/jag/Downloads/Stanford medstats/Unit 9 Homework with Answers.srt', "then dividing by 1 plus e raised to the predicted logit. If you calculate that all that out, you get a a percentage, a predicted probability of 54% so the correct answer here is e. In including continuous predictors in the model, the authors are assuming that these predictors have a linear relationship with our outcome here, which is the logit of death. So the correct answer here is b. The correct answer here is a. Our hazard ratio is 1.35 so that indicates a 35% increase in the rate of dementia after accounting for confounders. And it is statistically significant, because that confidence interval does not cross one, it goes from 1.14 to 1.60, so that is statistically significant. So, how do we interpret these hazard ratios for the underweight for women? So, what you'll notice is that they are all elevated by similar amounts as ", ['e', 'logit', 'percentage', 'probability', '%', 'correct', 'answer', 'e.', 'In', 'predictors', 'model', 'authors', 'predictors', 'relationship', 'outcome', 'logit', 'death', 'correct', 'answer', 'b', 'correct', 'answer', 'hazard', 'ratio', 'indicates', '%', 'increase', 'rate', 'dementia', 'confounders', 'confidence', 'interval', 'hazard', 'ratios', 'underweight', 'women', 'amounts']), 0.18968336763767799, 0.215599079053325)
((9, '/Users/jag/Downloads/Stanford medstats/Unit 3 Module 5 CME Alternate.srt', "So .84, times 0.65, that's equal to .546 we divide that by the denominator of .805, we get that the positive predictive value is 67.8%. All right, so the, the company calculated that, in their literature, now in their marketing material, they rounded that up, so they said, oh, your, the, the actual probability that you're going to get moderate to severe cellulite if you test positive is 60, it's about 68%. Well they said, they rounded that up to approximately 70%. They were trying to boost it, make it look, just look a little bit better. Okay. Now notice that that's an application of Bayes' Rule. For those of you who are more comfortable using the tree, which most students are when they first start this material, you could also do this on the tree. So let me just quickly show you how to do it on the tree. So, and then I'm going to show you how to do it using a 2 by 2 table. I'm going to give you many ways to get the right answer. And remember, with probability there's many ways to get the right answer. ", ['times', 'denominator', 'value', '%', 'right', 'company', 'literature', 'marketing', 'material', 'oh', 'probability', '%', 'Well', '%', 'bit', 'Okay', 'application', 'Bayes', 'Rule', 'tree', 'students', 'start', 'material', 'tree', 'let', 'tree', 'ways', 'right', 'answer', 'remember', 'probability', 'ways', 'right', 'answer']), 0.2016877936358443, 0.21370811778156054)
((18, '/Users/jag/Downloads/Stanford medstats/Unit 1 Homework Solutions with questions.srt', "So, again, we're going to go and find the hypertension row here, which is in the co-morbidity section, and we see that the percentage of the patients in the placebo group with hypertension is 73.9%. The next part of this question is asking us to calculate how many binary variables are shown, okay. So, the trick here is to look at how these variables are reported. If there's a fraction, if there's a percentage reported for a certain variable, you can assume that it's binary, okay. And that's because when there are two levels, if you know the percentage here this one's going, just going to be 100 just minus the percentage given in the other category. That's why there are no like second level or third level values reported here. So, to get back to the question, all the variables with percentages reported, our fractions here, are age grid equals 75, female sex, white race diabetes, ", ['hypertension', 'row', 'co-morbidity', 'section', 'percentage', 'patients', 'placebo', 'group', 'hypertension', '%', 'part', 'question', 'variables', 'okay', 'trick', 'variables', 'fraction', 'percentage', 'okay', 'levels', 'percentage', 'percentage', 'category', 'level', 'level', 'values', 'question', 'variables', 'percentages', 'fractions', 'age', 'equals', 'sex', 'race', 'diabetes']), 0.13335185571076982, 0.21085599723659162)
((6, '/Users/jag/Downloads/Stanford medstats/Unit 5 Homework with answers.srt', "That it will simply be smoother because as we have more and more observations the center limit harem will start kicking in, and we will have a more and more normal looking distribution. When we have ten times more values, this distribution will look ten times smoother. The correct answer to 'D' is two. The sample size of the hypothetical trial is larger, which is the reason that the 'p' value is lower. This question gives us the percentage of US adults Who had five or more drinks in one day at least once in a given year. And we have trends for, let's see, we have trends for seven years. We have the mean value as well as the 95% confidence intervals given. The question wants us to find for which years we could reset the null hypothesis that more than 20% of US adults had five or more drinks in one day. At a 0.25 significance level. The answer is 2000 and 2003 only. ", ['observations', 'center', 'limit', 'harem', 'distribution', 'times', 'values', 'distribution', 'times', 'correct', 'answer', 'sample', 'size', 'trial', 'reason', 'value', 'question', 'percentage', 'US', 'adults', 'drinks', 'day', 'year', 'trends', 'see', 'trends', 'years', 'value', '%', 'confidence', 'intervals', 'question', 'years', 'null', 'hypothesis', '%', 'US', 'adults', 'drinks', 'day', 'significance', 'level', 'answer']), 0.15905106915879735, 0.18690533277390745)
QUERY: Ok, I also see that, in some textbooks, the term standard error is used to denote what I know as the estimated standard error. An obvious estimate for the standard error of $overline{X}$ (but not of $T$) would indeed presumably be $frac{s}{sqrt{n}}$."
*************************
((2, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 9 mod3 Part2.srt', "Because we, the whole point of GEE is to correct for non-independence. But it turns out that GEE modeling relies on something called, you can calculate something called robust standard errors. And those standard errors, when you calculate those they're fairly. Robust against the misspecification of correlation. In fact, if you put independents in here but choose the robust standard errors, you're still going to get clues to the right answer. So of course, you, in this case, want to choose the exchangeable. But if you got it wrong and choose the independence, which is really wrong, you still do well as long as you report the robust standard error. I'll, I'll talk more about that in a bit. So first of all, I just applied regular, all this logistic regression to these data, ignoring the correlation. This is the incorrect analysis. So when I do that, what you'll notice is that I get a P-value of 0.07 that's too high, because the correct analysis here is, is comparing within subjects. And so ignoring the correlation makes are P value too high. Our odds ratio is about two. ", ['point', 'GEE', 'non-independence', 'GEE', 'modeling', 'something', 'something', 'robust', 'standard', 'errors', 'errors', 'Robust', 'misspecification', 'correlation', 'fact', 'independents', 'robust', 'standard', 'errors', 'clues', 'right', 'answer', 'course', 'case', 'choose', 'independence', 'report', 'robust', 'standard', 'error', 'bit', 'regression', 'data', 'correlation', 'incorrect', 'analysis', 'correct', 'analysis', 'subjects', 'correlation', 'P', 'value', 'odds']), 0.25642430590241294, 0.3656571058584304)
((11, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 9 mod3 Part2.srt', "gets correlations of about .59 so that's very close. It's, it's doing you know, what we'd expect here. Now I also asked to get the model based standard error just to show you, If I correctly specify exchangeable correlation matrix. Which is the right structure here, you know, I get the same beta, I get about the same P-value with this model-based standard error as I get with the robust standard errors. However, when I incorrectly specify the correlation structure, I say it's independent when clearly these are not independent. The robust standard errors still do perfectly. So notice that the P-value's still 0.11. That, that's the correct P value here. So the robust standard errors are empirical standard errors. I get the right answer even if I specify the totally wrong correlation structure. However, the model based standard errors take me back to just a regular logistic regression now, and so I get the wrong P value. So again, there's only certain situations where you might want to think about the model based errors, but in general you're going to chose the robust. And I just want to add one last example here just to show you again the, the effect, the impact that this can have upon standard errors and P values. ", ['gets', 'correlations', 'model', 'error', 'correlation', 'matrix', 'right', 'structure', 'beta', 'standard', 'error', 'errors', 'correlation', 'structure', 'robust', 'standard', 'errors', 'notice', 'correct', 'P', 'value', 'robust', 'standard', 'errors', 'errors', 'right', 'answer', 'correlation', 'structure', 'model', 'errors', 'regression', 'P', 'value', 'situations', 'model', 'errors', 'robust', 'example', 'effect', 'impact', 'errors', 'P', 'values']), 0.24206145913796354, 0.3241814429464843)
((8, '/Users/jag/Downloads/Stanford medstats/Unit 1 Module 5 part 2.srt', "So you may have the standard error of a mean. You may have the standard error of an odds ratio, you may have a standard error of a regression coefficient, those will all be different. Again, standard error is about the variability of a statistic. And we usually reserve the term standard deviation to talk about the variability of a trait, so that we don't get it confused with standard error. I don't expect you to completely understand that yet, but just tuck it away in the back of your head, and we're going to return to this concept a lot in, in future weeks. Okay, another way to measure the spread or variability of the data, is just to rank the data and look at percentiles. We did this when we looked at box plots. So, you could talk about things like 90th percentile of data. Where does the 90, 90th percentile lie? That will some information, some feeling about the data. So, the 90th percentile is the value for which 90% of observations are lower. You can talk about the 10th percentile of the data. That would be the value for which 10% of the observations are lower. We've already talked about the median, that's the 50th percentile, half are above and half are below the median. So sometimes we'll just talk about where those different percentiles lie, ", ['error', 'error', 'odds', 'error', 'regression', 'coefficient', 'error', 'variability', 'term', 'standard', 'deviation', 'variability', 'trait', 'error', 'back', 'head', 'concept', 'lot', 'weeks', 'Okay', 'way', 'spread', 'variability', 'data', 'data', 'percentiles', 'box', 'plots', 'things', 'data', 'percentile', 'lie', 'information', 'data', 'percentile', 'value', '%', 'observations', 'percentile', 'data', 'value', '%', 'observations', 'percentile', 'half', 'percentiles']), 0.3277811406876182, 0.31595568095126386)
((7, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 9 mod3 Part2.srt', "But I got the robust standard errors again, that's the default in SASS, so here is when I ask for the independent correlation structure. I get basically the same P-value as when I asked for the exchangeable. As long as I'm using the empirical or robust standard error, so nothing changes. However, if I specify independent correlation structure, but ask for the model-based standard errors, I'm basically going back to a regular old logistical regression. This is the same P-value I got out of an ordinary logistic regression. So, if I get the correlation structure wrong and ask for the model-bases standard errors, I could be very off. I'm essentially gone back to not correcting for the correlation here. but, the robust standard errors do well even if I get the correlation structure totally off. So, ju, that's just something to keep in mind. again, there's two types of standard errors, what we call robust or empirical and model-based. And in general, you're going to report the robust standard errors. That's generally what you report. It's the default in SASS. You have to specifically ask for the other type, type of standard error. The robust errors are great because they are robust against the incorrect choice of ", ['robust', 'standard', 'errors', 'default', 'SASS', 'correlation', 'structure', 'error', 'nothing', 'changes', 'correlation', 'structure', 'errors', 'regression', 'regression', 'correlation', 'structure', 'ask', 'model-bases', 'errors', 'correlation', 'robust', 'standard', 'errors', 'correlation', 'structure', 'ju', 'something', 'mind', 'types', 'errors', 'robust', 'standard', 'errors', 'default', 'SASS', 'type', 'type', 'error', 'robust', 'errors', 'incorrect', 'choice']), 0.22742941307367104, 0.30458543694340456)
((1, '/Users/jag/Downloads/Stanford medstats/Unit 5 Homework with answers.srt', 'the standard error is given by standard deviation divided by the square root. Of the number of observations in the sample. And if we substitute the values that we have, this comes out to 0.23. This tells us that a is the right answer. For this question, we know that the mean is .66. How do we notice this, is because we have 20 patients out of 30 who improve based on the exercise test, test. We also know that a standard error is given by the formula P times 1 minus p by a n in this case P is 0.66. 1 minus p is 0.33 and n is 30. So we have a standard error of 0.086. ', ['error', 'deviation', 'root', 'number', 'observations', 'sample', 'values', 'right', 'answer', 'question', 'patients', 'exercise', 'test', 'test', 'error', 'formula', 'P', 'minus', 'p', 'n', 'case', 'P', 'minus', 'p', 'n', 'error']), 0.3098898934004561, 0.301618818278803)
((17, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 8 Module 1.srt', "You could also choose unstructured because we only have three correlations to estimate. Although this data set only has 41 women in it, so we're limited on degrees of freedom. I went ahead and fit all of these different correlation structures just to compare them. See what happens. So I started with the exchangeable correlation matrix. I've already showed you these results before. So the beta coefficient was 0.0088. My P value is highly significant. My standard error is about 0.002. The exchangeable working correlation is 0.974. There it estimated for all of the correlations about 0.974. That seems like a good compromise, because they were all right around 0.96, 0.97, 0.98. I then want to have them fit the autoregressive, oh so the the, the QIC from the exchangeable is 126.88. I went ahead and fit the autoregressive. You'll notice that the QIC was just teeny, teeny bit better from the auto regressive. Although I would consider these models to be equally good. They're, that QIC isn't very different. But it is a little bit better from the autoregressive. So maybe the autoregressive is just slightly better here. ", ['correlations', 'data', 'set', 'women', 'degrees', 'freedom', 'correlation', 'structures', 'See', 'correlation', 'matrix', 'results', 'beta', 'coefficient', 'P', 'value', 'standard', 'error', 'working', 'correlation', 'correlations', 'compromise', 'oh', 'QIC', 'QIC', 'teeny', 'bit', 'better', 'auto', 'regressive', 'models', 'QIC', 'bit']), 0.15649215928719032, 0.2713107408348911)
((9, '/Users/jag/Downloads/Stanford medstats/Unit 5 Module 4 part 1.srt', "real data. They usually follow a T but by the time you're talking about a sample size of greater than 100 where, where it's interchangeable with the normal. So I'm just going to call it a normal distribution. In general, sample means the mean of the means is the true mean in the population. Well, that make sense. The standard error, somebody has worked out the formula for standard error, the standard error of a mean is the standard deviation of the trait, the variability of the trait like vitamin divided by the square root of n, the number of people you're averaging over. And let's remember, we saw that that in fact works out with exactly what we saw in the simulation because the standard deviation here was 33 if I divide by the square root of 100. I get exactly 3.3, exactly what came out in the simulation as well. So both the simulation and mathematical theory are matching up. So the standard error of a mean, again, is the standard deviation of the trait divided by the square root of n. So you can see some things about how standard error works just by ", ['data', 'T', 'time', 'sample', 'size', 'distribution', 'means', 'means', 'mean', 'population', 'Well', 'make', 'sense', 'error', 'somebody', 'formula', 'error', 'error', 'deviation', 'trait', 'variability', 'trait', 'vitamin', 'root', 'n', 'number', 'people', 'remember', 'fact', 'works', 'simulation', 'deviation', 'root', 'simulation', 'simulation', 'theory', 'error', 'deviation', 'trait', 'root', 'n.', 'So', 'things', 'error', 'works']), 0.3218450394066701, 0.25003926104937046)
((4, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 8 mod5.srt', "Again, common outcome. I get out my odds ratios for the EDIA score. This is the odds ratio for a one unit increase in your eating disorder score. It's highly statistical significant. The odds ratio comes out to be 1.12. You would interpret this as a 12% increase in your odds of having menstrual irregularity for every one point increase in your score on this eating disorder questionnaire. Remember, this isn't increase in odds though, because my outcome is common, so we would not want to misinterpret this as an increase in your risk, or the prevalence of menstrual irregularity. But, I can also run these same data with this binary outcome of menstrual irregularity in a Poisson regression with these robust standard errors. And that's what I've done here. And so here's my predictor variable, EDIA here's my beta coefficient, and here's my standard error, this is the robust standard error, so now these standard errors will be correct. You can see that I indeed get a highly statistically significant effect for ", ['outcome', 'odds', 'ratios', 'EDIA', 'score', 'odds', 'unit', 'increase', 'eating', 'disorder', 'score', 'odds', 'comes', '%', 'increase', 'odds', 'irregularity', 'point', 'increase', 'score', 'eating', 'disorder', 'questionnaire', 'Remember', 'odds', 'outcome', 'increase', 'risk', 'prevalence', 'irregularity', 'data', 'outcome', 'irregularity', 'Poisson', 'regression', 'errors', 'predictor', 'EDIA', 'beta', 'coefficient', 'error', 'robust', 'standard', 'error', 'errors', 'effect']), 0.15959855297967945, 0.23725540027319308)
QUERY: how can you best assess if data are independent or correlated. Seems intuitive but can be harder than it looks sometimes. It's not always as easy as faces or legs -- those are obviously correlated. Can you give more subtle examples of each? Thx
*************************
((1, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 6 Overview.srt', "This week I'll also just give you an introduction. To what does repeated measures data look like. So, I'll show you some examples from the literature. I'll talk about the different data structures for repeated measures. There's actually kind of two ways to structure the data and depending on what you're trying to do in the computer you nee, you may need it in one or the other form. And I'll also give you some examples of how to plot your repeated-measures data. Plotting is going to be very important here for you get to get a real understanding of what's going on in the data. Just to ground us on where are we on our statistical chart of tasks, so again I will talk a little bit about some binary repeated measures. Outcome data in the second half of this course, but I'm mostly going to fit, focus on continuous outcomes. So, if we have continuous outcomes, these are things like again, blood pressure, weight, pain scale, cognitive function on some kind of continuous scale. If we have independent groups, or independent observations, we're going to be doing something like a basic ANOVA or a basic linear regression. ", ['week', 'introduction', 'measures', 'data', 'examples', 'literature', 'data', 'structures', 'measures', 'ways', 'data', 'computer', 'form', 'examples', 'repeated-measures', 'data', 'understanding', 'data', 'chart', 'tasks', 'bit', 'measures', 'Outcome', 'data', 'course', 'focus', 'outcomes', 'outcomes', 'things', 'blood', 'pressure', 'weight', 'pain', 'scale', 'function', 'kind', 'scale', 'groups', 'observations', 'something', 'ANOVA', 'linear', 'regression']), 0.2504897164340598, 0.14042787436732815)
((10, '/Users/jag/Downloads/Stanford medstats/Unit 1 Homework Solutions with questions.srt', "And so the mean is going to be equal to .47. The second part of this question asks us to calculate the standard deviation of this data set. And so again, the first step is to remember what the formula for the standard deviation is. So I'll write that out one more time. It's the summation of the expression shown here, okay. And so n is the number of values in our data set so that's going to be 100. The mean we just calculate it to be 0.47, okay. So, let me clean this up a little bit and rewrite that expression here. All right, and the in contrast to previous examples, this is actually going to be a shorter expression to write out, because there are only two distinct values in our data set. ", ['part', 'question', 'deviation', 'data', 'set', 'step', 'formula', 'deviation', 'time', 'summation', 'expression', 'shown', 'number', 'values', 'data', 'okay', 'up', 'bit', 'expression', 'right', 'contrast', 'examples', 'shorter', 'expression', 'values', 'data']), 0.1781741612749496, 0.13802316141242313)
((2, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 9 mod1.srt', "so well we'll get into that, we'll delve into that in the spring. For today we'll stick with cross-sectional examples of correlation. Now I told you before that there are two possible consequences of ignoring the correlations in, in your data. So if you're doing what are called within-person or within-cluster comparisons, and you ignore the correlations, you're going to end up with p-values that are too big. You're going to be overly conservative. However, if you are doing between-person or between-cluster comparisons and you ignore the correlations, your p-values are going to end up too small. They're going to be too optimistic. So I've, I've told you this before, but I thought it might be helpful to actually draw out a picture of this just to make sure we're all clear on what I mean here. So I'm just going to draw a picture of some possible data. [NOISE] So the dots here represent observations, and I'm going to color the different clusters with different colors. So here's some datas that are somehow clustered, ", ['spring', 'today', 'examples', 'correlation', 'consequences', 'correlations', 'data', 'comparisons', 'correlations', 'p-values', 'comparisons', 'correlations', 'p-values', 'picture', 'picture', 'data', '[', 'dots', 'observations', 'clusters', 'colors', 'datas']), 0.14433756729740643, 0.12957410413863052)
((1, '/Users/jag/Downloads/Stanford medstats/Unit3 Module4 pt1.srt', "To accomplish this, I once again write Proc Sort, specify our people2 data set and name a new data set called people_sort_ages. In the next line, I write by descending age. This tells SAS to sort my original data by descending age values. Let me run this. You can now see that the data are ordered by age. From the highest to the lowest. So far, the examples we have used, have been pretty straightforward. But what if we want to sort on more than one variable at a time. Say we wanted to sort by gender and then by weight within the gender levels. Let's take a look at the code for this. For our by statement, we first wrote gender, and then specified weight. So within males and females, the data should be sorted by weight. Here we'll run it, within males and females, we now have the data sorted by weight. ", ['Proc', 'Sort', 'specify', 'people2', 'data', 'name', 'data', 'people_sort_ages', 'line', 'age', 'SAS', 'data', 'age', 'values', 'Let', 'data', 'age', 'examples', 'time', 'Say', 'gender', 'weight', 'gender', 'levels', 'Let', 'take', 'look', 'code', 'statement', 'gender', 'weight', 'males', 'females', 'data', 'weight', 'males', 'females', 'data', 'weight']), 0.2030905986149808, 0.1100101000585068)
((3, '/Users/jag/Downloads/Stanford medstats/Unit 9 Module 3.srt', "it's automatically a line. If you have a categorical predictor, you're going to be dummy coding it, so you're going to have a series of zero ones. Those are all going to make lines, so this is only a worry when you have a continuous predictor in logistic regression. I want to show you some real examples with real data. Various examples where I've had to draw linear in the logit plots. This example was from a data set where the outcome is heart disease. The predictor that we're looking at here called tobacco, this is the number of cigarettes smoked per day. And I grouped the data set-up into four bins, as well as ten bins, just to give us a sense of the relationship between the number of cigarettes per day and the logit of heart disease. And again, I have to group people, and what I'm graphing here is the, I'm graphing their mean cigarettes per day. The mean cigarettes per day for the group against the logit for that group. The observed logit for that group. What you can see here is that, for ", ['line', 'predictor', 'coding', 'series', 'zero', 'ones', 'lines', 'worry', 'predictor', 'regression', 'examples', 'data', 'examples', 'logit', 'plots', 'example', 'data', 'set', 'outcome', 'heart', 'disease', 'predictor', 'tobacco', 'number', 'cigarettes', 'day', 'data', 'bins', 'bins', 'sense', 'relationship', 'number', 'cigarettes', 'day', 'logit', 'heart', 'disease', 'people', 'cigarettes', 'day', 'cigarettes', 'day', 'group', 'logit', 'group', 'logit', 'group']), 0.13699915608779778, 0.10389356947443192)
((4, '/Users/jag/Downloads/Stanford medstats/Unit 1 Homework Solutions with questions.srt', "So, we can't just have one expression that has 11 minus the mean of our data set, we need ten of those. So, what we're going to do is multiple this by ten and similarly we're going to multiple this by 9 because there are 12 nines in our full data set. So, yes so I'll multiply the last one by six as well. And doing out this calculation, we're going to get 148 on the top divided by 59 in denominator and that's going to leave us with 1.58. So, the answer to the question is 1.58, the standard deviation of our data is 1.58. The next part of the question asks us about the median value of our data. Okay? the median if you remember, is the middle number in our data set. Okay? So, the first step whenever you're thinking about the median the interquartile range, is to arrange the numbers in your data set in order. ", ['expression', 'minus', 'data', 'ten', 'nines', 'data', 'set', 'yes', 'calculation', 'denominator', 'answer', 'question', 'deviation', 'data', 'part', 'question', 'value', 'data', 'Okay', 'middle', 'number', 'data', 'Okay', 'step', 'interquartile', 'range', 'numbers', 'data', 'order']), 0.21821789023599236, 0.10153531023688334)
((0, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit4 Mod3.srt', "In this next module, I'm going to tell you about some tests that you can use to check the logistic regression model. The example data that I'm going to use in this module is sort of a classic data set, it's referred to in a lot of statistics textbooks, but it was some data on 81 children who have corrective spinal surgery. Some of them went on to get kyphosis, curvature of their spine, and some did not. So we have a binary outcome here, kyphosis or not. The predictor I'm going to focus on for the purposes of this module is age. Can we use age as, as a predictor of who's likely to go on to get spine curvature. And so I went ahead and I ran the logistic regression model with kyphosis as the outcome in age as the predictor. And here is some of the results from the computer. So this is showing you the test of global fit. And remember what we talked about earlier, these are a test which are testing the null hypothesis that all the data's in the model are equal to 0 and ", ['module', 'tests', 'regression', 'model', 'example', 'data', 'module', 'sort', 'data', 'lot', 'statistics', 'textbooks', 'data', 'children', 'surgery', 'kyphosis', 'curvature', 'spine', 'outcome', 'kyphosis', 'predictor', 'purposes', 'module', 'age', 'Can', 'age', 'predictor', 'spine', 'curvature', 'regression', 'model', 'kyphosis', 'outcome', 'age', 'predictor', 'results', 'computer', 'test', 'fit', 'remember', 'test', 'null', 'hypothesis', 'data', 'model']), 0.1465525954401373, 0.09925404550928431)
((0, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 9 Module 3.srt', "In this next module, I'm going to show you how to fit a mixed model with a time independent predictor, and this is very similar to what we did with generalized estimating equations, the interpretation of the beta coefficients is pretty similar so hopefully if you understood the material last week, this module will be pretty easy. I'm going to use this same data set, on bone density in women runners, these 41 women runners, divided into the three groups. You've seen these pictures before, so I won't explain them again. I'm going to take that same data,. And fit first in a random intercepts only model. So in this model, I just allow the intercepts to be random. And I did not put in a random time slope. And, so what you'll see here is this is very similar to the results. That we got out of the generalized estimating equation for these same data. The effect for time. The main effect for time is interpreted as the rate of change in time for the blue ", ['module', 'model', 'time', 'independent', 'predictor', 'equations', 'interpretation', 'beta', 'coefficients', 'material', 'week', 'module', 'data', 'bone', 'density', 'women', 'runners', 'women', 'runners', 'groups', 'pictures', 'data', 'random', 'model', 'intercepts', 'random', 'random', 'time', 'slope', 'results', 'estimating', 'equation', 'data', 'effect', 'time', 'effect', 'time', 'rate', 'change', 'time']), 0.12751534261266764, 0.09878015190333407)
QUERY: Will it be too out of place to use Fisher's exact test where i should use a chi square since it gives more accurate P values while chi square always calculates an approximation? I understand the difference between the Chi-square and the FET becomes smaller as the expected cell count and table size grows. My point being, why use an approximation (ie., chi-square) when you can get the exact answer (FET) regardless of sample size.
*************************
((14, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod5.srt', "what we care about is the two-sided P value which happens to be at the very end of our output in SAS at least, and that's the P value that we're going to report. I want to just show you that I also asked SAS to run the chi square test for that two by two table. So my two by two table here again looked like this. I have 3, 1, 1, 3. I can ask SAS to run a chi square test on And, and it will run it. When it did that chi square test were clearly violating the assumption of not having sparse data here. It's probably a bad idea to approximate here with a, a, discreet distribution with a continuous one we get a chi-Square value of 0.15. You can see that two sided P value of 0.15 is very different tan the two sided P value we got from the Fisher's exact. Which tells you that, you know, again When you have sparse data it can really make a difference if you correctly chose the Fisher's exact or try to apply the Chi-Squared. Luckily SAS pretty nice, it gives you a little warning down here. Says 100% of the cells have expected counts less than 5, ", ['P', 'value', 'happens', 'end', 'output', 'SAS', 'P', 'value', 'SAS', 'chi', 'square', 'test', 'SAS', 'chi', 'square', 'test', 'chi', 'square', 'test', 'assumption', 'sparse', 'data', 'idea', 'discreet', 'distribution', 'value', 'P', 'value', 'tan', 'P', 'value', 'Fisher', 'tells', 'data', 'difference', 'Fisher', 'try', 'SAS', '%', 'cells', 'counts']), 0.3170476475225524, 0.3513858673482876)
((18, '/Users/jag/Downloads/Stanford medstats/RExercise2.srt', "a p-value associated with that. But what should jump out at you is the low number of cells in the. First column here. So because the cell numbers are low, and because one of them is less than 5, I suspect that maybe the chi square test is not the best test to use here. And that we should instead be using the Fishers exact test. Now I remember seeing that in the menus before, so I'm going to go select that. So I'm going to click on Analysis, and then go to Contingency Tables. The variables are still where we left them, but I'm going to click on statistics and Also select Fisher's exact test. Now the criteria for using Fisher's exact test is really a rule of thumb. There's no hard and fast rule, or statistical rule, I guess I should say, where you need to use Fisher's exact test. ", ['number', 'cells', 'First', 'cell', 'numbers', 'chi', 'square', 'test', 'test', 'Fishers', 'exact', 'test', 'menus', 'select', 'Analysis', 'Contingency', 'Tables', 'variables', 'statistics', 'Fisher', 'test', 'criteria', 'Fisher', 'test', 'rule', 'thumb', 'rule', 'rule', 'Fisher', 'test']), 0.2810913475705226, 0.34774955364237015)
((19, '/Users/jag/Downloads/Stanford medstats/RExercise2.srt', "But I guess it's more accurate to say that Fisher's exact test does a better job of calculating the p-value when the expected cell values are less than five. I'll say that again in a different way. So, the Chi-Square test does not do a good job of approximating the Key value of the relationship between two variables when the expected cell value in any of the cells of the contingency table is less than five. To illustrate that point, I'm going to also select the expected cell counts by clicking on Cells and checking Expected. going to hit Run. You'll notice it generates the same table, as before, but there's additional output this time because we had Fisher's exact selected. We see that the chi squared p value is 3.19, and ", ['Fisher', 'test', 'job', 'cell', 'values', 'way', 'test', 'job', 'Key', 'value', 'relationship', 'variables', 'cell', 'value', 'cells', 'contingency', 'table', 'point', 'cell', 'counts', 'Cells', 'Expected', 'Run', 'table', 'output', 'time', 'Fisher', 'chi', 'p', 'value']), 0.27406406388125953, 0.28699908190932805)
((4, '/Users/jag/Downloads/Stanford medstats/HRP262 Unit 1 Module 4.srt', "But this is one of the tests that you can use, the most commonly used to compare survival curves. And then again it could be used for more than two groups. So what is the Log-rank test actually? I'm going to kind of walk you through the math. It's not something that you'd probably do by hand, but it's good to have an understanding of what's going on behind the scenes. So the Log-Rank test turns out to just be a Mantel-Haenszel chi square test of independence. And we talked about the Mantel-Haenszel test earlier when we talked about stratifying for confounding. So we used the Mantel-Haenszel chi square test To stratify on confounders to see whether or not an exposure and an outcome were related after adjusting for confounding. The idea is similar here, except rather than stratifying on a confounder we're going to stratify on event times. At every time when events cur, occur, when people died. We're going to be comparing the treated control group at each one of those event times. The event times here are the strata. ", ['tests', 'survival', 'curves', 'groups', 'Log-rank', 'test', 'walk', 'math', 'something', 'hand', 'understanding', 'scenes', 'test', 'chi', 'square', 'test', 'independence', 'Mantel-Haenszel', 'test', 'Mantel-Haenszel', 'chi', 'square', 'test', 'confounders', 'exposure', 'outcome', 'adjusting', 'confounding', 'idea', 'confounder', 'event', 'times', 'time', 'events', 'people', 'control', 'group', 'event', 'times', 'event', 'strata']), 0.22377237111420628, 0.2838478018070133)
((4, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod4.srt', "many different things And it's not just contingency tables. But the, the co, one of the common uses of a chi square is for contingencies tables. In the case of contingency table the degrees of freedom is always equal to r minus, the rose minus one times the columns minus one. I'm just going to quickly walk through one chi square test of independance from my contingency table. Again I'm assuming that you've all done this before to the to be pretty much reviewed. So here's some data that I got out of Agresti's discreet data analysis book. They were looking at political ideology and happiness. So they do a big survey of people and said you know, are you liberal, moderate or conservative? And what's your level of happiness? We have two categorical variables they've done this as they, they could have scored this as continues variables, probably but this they've done this as categorical variables but we have three level categories so we don't, we no longer on a 2 by 2 table, we're on a 3 by 3 table. How can I tell, whether or not there are significant differences in happiness between the different political ideology proofs. ", ['things', 'contingency', 'tables', 'co', 'uses', 'chi', 'square', 'contingencies', 'tables', 'case', 'contingency', 'table', 'degrees', 'freedom', 'minus', 'rose', 'minus', 'times', 'columns', 'chi', 'square', 'test', 'independance', 'contingency', 'table', 'done', 'data', 'Agresti', 'discreet', 'data', 'analysis', 'book', 'ideology', 'happiness', 'survey', 'people', 'level', 'happiness', 'variables', 'continues', 'variables', 'variables', 'level', 'categories', 'longer', 'differences', 'happiness', 'ideology', 'proofs']), 0.1799895876803992, 0.2711915619815732)
((14, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit2 Mod5.srt', "Well guess what? That is also the Macnemare statistic. So we got, when we apply the Mantel-Haenszel formula here, we ended up with a, with the Macnemare's chi square as well. It's the exact same mathematical formula, b minus c square, divided by b plus c. So these turn out to be identical. I've shown this here with specific numbers, and now, let's also show it more generally, using b plus c rather than specific numbers. For those of you who want a neat slide where this is all written out, I'm putting that, I'm placing that here just in case you want to see it written out more neatly than my handwritting. But now going to generalizing. Let's generalize this whole thing. So, now if we look at the Cochran Mantel-Haenszel statistic and we think about the general case. Anywhere we have the situation of paired data where we have discorded and concorded pairs. Notice what's going to happen when we plug into the Mantel-Haenszel statistic. Which again is a chi squared with one degree of freedom. ", ['Well', 'guess', 'Macnemare', 'Mantel-Haenszel', 'formula', 'Macnemare', 'chi', 'square', 'same', 'formula', 'b', 'minus', 'c', 'square', 'b', 'c.', 'So', 'turn', 'numbers', 'b', 'c', 'numbers', 'neat', 'slide', 'case', 'handwritting', 'Let', 'generalize', 'whole', 'thing', 'Cochran', 'Mantel-Haenszel', 'case', 'situation', 'data', 'pairs', 'Notice', 'chi', 'degree', 'freedom']), 0.1514564891325506, 0.2627889211978052)
((10, '/Users/jag/Downloads/Stanford medstats/Unit 8 Module 1 part 2.srt', "It's a, it's a bit of an approximation, but it worked pretty well for all most all n. I can just plug in then for example, to be statistically significant when you have a sample size of 10, 2 divided by the square root of 10, gives you that you're going to need to have a correlation of 0.63. It's going to need to be pretty big in order to come out statistically significant with only 10, to be confident that that's really different than 0. With a 100 however, 2 divided by the square root of a 100 is, gives you a value of 0.2. So you can have any correlation coefficient for any variables in your study, that comes out to be .02 or greater, when you have a sample size of a 100 is going to be statistically significant. As you get to very big sample sizes, as I showed you before, say a 1000, 10,000, 100,000. The minimum correlation that's going to come out to be statistically significant, notice all of these, when your sample size is a 1000 or greater, 0.06, 0.02, these are all less than 0.1 correlation. That would be considered essentially no correlation, it's so small, that it's not meaningful. ", ['bit', 'approximation', 'n.', 'example', 'sample', 'size', 'root', 'correlation', 'order', 'however', 'root', 'value', 'correlation', 'coefficient', 'variables', 'study', 'sample', 'size', 'sample', 'sizes', 'correlation', 'notice', 'sample', 'size', 'correlation', 'correlation']), 0.22019275302527208, 0.24707482262645383)
((10, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit4 Mod4 pt1.srt', "which comprises five variables, whether or not that adds anything significantly to the model, we can do that by using a chi square. This is a chi square with five degrees of freedom, because we have five interaction terms in the model. It's a six level categorical variable, we end up with five terms in the model. So we have a chi square with five degrees of freedom, the negative 2 log likelihood for the reduced model is this, for the full model is this. We just subtract those, we get that we have a chi square with five degrees of freedom with a value of 20. We can then look to see whether or not that comes out to be significant. Indeed, I'll show you in a minute where got this from. But the p-value here comes out to be significant. So the expected value here is 5, and the p value does indeed come out to be significant. So that's telling us that the p value for interaction overall, adding the interacting term to our model improves our model. So, in case you don't want to get into the fine level of details of which particular interacting terms came out significant when you have a categorical variable, ", ['comprises', 'variables', 'anything', 'model', 'chi', 'square', 'chi', 'square', 'degrees', 'freedom', 'interaction', 'terms', 'model', 'level', 'terms', 'model', 'chi', 'square', 'degrees', 'freedom', 'log', 'likelihood', 'model', 'model', 'chi', 'square', 'degrees', 'freedom', 'value', 'minute', 'value', 'p', 'value', 'come', 'p', 'value', 'interaction', 'term', 'model', 'improves', 'model', 'case', 'fine', 'level', 'details', 'interacting', 'terms']), 0.1953982147859498, 0.23891260618044824)
QUERY: only some of them are right for the CI range (CI values), not all of the values are right. How can you say which one is statistically significant?
*************************
((5, '/Users/jag/Downloads/Stanford medstats/Unit 5 Homework with answers.srt', "these values, on which are on the left or the right data correspondingly. This translates to a two sided B value of .025 In this question we know that the mean is zero, and x is 27, and that the standard deviation is 12.3. This means that the z-score is 2.2. This corresponds to a probability Value of .014 now because we are looking a two sided 'p' values and not one sided 'p' values we can multiply this by '2' and so the final probability comes out to .028. The next question asks us how would the histogram would change if I ran the simulation of a 10,000 times rather than 1000 times. And the correct answer is 1. ", ['values', 'left', 'right', 'data', 'B', 'value', 'question', 'x', 'deviation', 'corresponds', 'probability', 'Value', 'values', 'values', 'probability', 'question', 'histogram', 'simulation', 'times', 'times', 'correct', 'answer']), 0.3253956867279842, 0.299378284539164)
((48, '/Users/jag/Downloads/Stanford medstats/HRP 262 Final Exam Example Analysis.srt', "And then I asked [INAUDIBLE] to generate the predicted values from those models. And they're very informative, to look at these graphics. This is the predicted values, and I'm graphing here by clinic, so predicted values by clinic. And this is from my proc mixed with the random statement. So, what you can see is, well gee, each clinic is allowed, all the clinics have different trajectories of growth over time. All the individuals are predicted to have different growth trajectories over time. There is variation, so a specific individual has a different predicted value even two individuals who are, are from the same intervention group, they 're predicted to have different values over time. So, you get variation, you get scatter. Now look over here. Here, is the predicted values by clinic from a proc mixed with a repeated statement. ", ['INAUDIBLE', 'values', 'models', 'graphics', 'values', 'values', 'proc', 'random', 'statement', 'clinics', 'trajectories', 'growth', 'time', 'individuals', 'growth', 'trajectories', 'time', 'variation', 'value', 'individuals', 'intervention', 'group', 'values', 'time', 'variation', 'values', 'proc', 'statement']), 0.38924947208076144, 0.26717913693112355)
((17, '/Users/jag/Downloads/Stanford medstats/RExercise1 full.srt', "So the first three values for milk are 16, 0 and 4, first three variables for milkFixed are the first three values I should say, are 16, 0 and 4. So, you know, you should always check. What the, you should always check the composition of your variables after making a change like this. But for this exercise, I'm pretty satisfied that my recoding worked. Moving on to question eight, question eight is asking for the mean, the standard deviation. The median number of hours of sleep in this Dataset. So, that's a lot to ask for. but, there's a really handy function in this program that allows us to generate all three. So, what I'm going to do is go to Analysis, and then Descriptive, and then drag over Sleep. ", ['values', 'milk', 'variables', 'milkFixed', 'values', 'composition', 'variables', 'change', 'exercise', 'recoding', 'question', 'question', 'deviation', 'number', 'hours', 'sleep', 'Dataset', 'lot', 'function', 'program', 'Analysis', 'Descriptive', 'Sleep']), 0.23488808780588138, 0.26153104869202565)
((3, '/Users/jag/Downloads/Stanford medstats/Unit 4 module 1 part 1.srt', 'this is the same distinction we talked about in the first week of the course, so, discrete means you can only take on certain values. So, these would be things like whole numbers, counts, you\'re either dead or alive, treatment or placebo, you throw in dice, die, can only be, the outcomes are only, can be one, two, three, four, five, and six. So, can only have discrete values. That\'s of course as up close to continuous. Continuous variables can take on any value in a given range. So any, to say any number between 1 and 6. You can take on 1.2, 1.235. So these are the things like blood pressure, the instantaneous speed of a car. So continuous things have an infinite number of possible values. For the most part this week, I\'m going to focus on a lot of discreet examples, because they\'re easier to deal to with, they\'re easier to get your head around. We can then take, kind of understand it, using the discrete, examples. And we can, then kind of generalize, extend it to the continuous case. Whenever I\'m talking about the continuous case I"ll be throwing in that little bit ', ['distinction', 'week', 'course', 'means', 'values', 'things', 'numbers', 'counts', 'treatment', 'placebo', 'dice', 'outcomes', 'values', 'course', 'close', 'variables', 'value', 'range', 'number', 'things', 'blood', 'pressure', 'speed', 'car', 'things', 'number', 'values', 'part', 'week', 'lot', 'discreet', 'examples', 'head', 'kind', 'understand', 'examples', 'generalize', 'case', 'case', 'll', 'bit']), 0.2788866755113585, 0.25977348827195135)
((16, '/Users/jag/Downloads/Stanford medstats/RExercise1 full.srt', "change that into five, for milk fixed, okay. Were going to hit OK, hit OK hm-mm. So I'm going to scroll overall the way to the right here and you see that there's this new variable, milk fixed. And I'm going to scroll down, actually let me go find that 50 first. Where is that guy, row 30. Okay, subject 30, or patient 30. And I'm going to scroll over to milkFixed, and I notice that when I go to patient 30 and I look at this value that it's indeed recoded as fixed. Now, how do I know that nothing else has changed? Well, I don't really unless I run some tests, but just as a sanity check let me compare the first three values with the first three values for each variable. ", ['change', 'milk', 'okay', 'OK', 'OK', 'hm-mm', 'way', 'right', 'milk', 'guy', 'row', 'Okay', 'patient', 'milkFixed', 'patient', 'value', 'nothing', 'Well', 'tests', 'sanity', 'check', 'let', 'values', 'values']), 0.22360679774997896, 0.24897014087213917)
((6, '/Users/jag/Downloads/Stanford medstats/RExercise1 full.srt', "So I'm going to scroll to the right, and we see that WritingLove is sorted in descending order until we hit between row 31 and 30 here. Where it goes from 20 to 19 to 10, and then there's two cells With missing values denoted as na. And then we start over at 98. And then, again WritingLove is sorted in descending value. So, I'd like to talk a little bit about missing values, in R. So, note that missing values are denoted as NA. The important distinction here, between R other statistical programming packages, like for example, SAS or STATA. Or SPSS, whatever you might use, is that na is not given a value in r. In those other packages missing values are given either a very, very, very large positive value, or a very, very, very large negative value. ", ['right', 'WritingLove', 'descending', 'order', 'row', 'cells', 'values', 'na', 'WritingLove', 'descending', 'value', 'bit', 'values', 'R.', 'So', 'values', 'NA', 'distinction', 'R', 'programming', 'packages', 'example', 'SAS', 'STATA', 'SPSS', 'na', 'value', 'r.', 'In', 'packages', 'values', 'value', 'value']), 0.31378581622109447, 0.24620387222776866)
((40, '/Users/jag/Downloads/Stanford medstats/HRP 262 Final Exam Example Analysis.srt', "And just for fun, I thought it would be very useful for your understanding of what these models are doing to go ahead and generate some graphics that actually graph the predicted values for the individuals in the data set. So if you take, if you fit a model, you can ask your computer to calculate predicted values for everybody in the data set and output them and then graph those. And that's really, that picture will show you what, what we've modeled, what the model looks like. So I went ahead and did this. So here on the left-hand side, this is the observed weight growth trajectories for the, for the girls. And then I went and got the predicted values for the girls out of my model, the model that I just showed you on for, where the outcome was weight. This had a cubic term in it as well as a quadratic. And you can see that indeed, and then I just went ahead and got the predictive values at each time point for each woman, for each girl, and I just connected those lines here. And you can see that it indeed does appear to have a nice cubic shape to it. ", ['fun', 'understanding', 'models', 'graphics', 'values', 'individuals', 'data', 'set', 'model', 'computer', 'values', 'everybody', 'data', 'set', 'output', 'picture', 'model', 'left-hand', 'side', 'weight', 'growth', 'trajectories', 'girls', 'values', 'girls', 'model', 'model', 'outcome', 'term', 'values', 'time', 'point', 'woman', 'girl', 'lines', 'shape']), 0.31139957766460913, 0.24433157226296742)
((2, '/Users/jag/Downloads/Stanford medstats/Unit2 Module3 DataSteps2 pt2.srt', "If weight is missing and gender is male then the imputed weight is equal to 154. In this case, we didn't have any missing values for weight for any males. If weight is missing and gender is female, then the imputed weight is equal to 140, as we can see in these rows.  Now we're going to walk you through another way you can use if then L statements. Within the if then L statements, you can also flag items that are likely incorrect, or just interesting to keep track of. Earlier in this module, we used if then L statements that we believed were way too extreme for our data set. Say instead that we want to flag all observations that have unrealistic weight values. This way, we would not be removing or changing any values in our dataset. It is always important to think carefully about the realistic range of values that your variable could have. When it comes to weight, it is impossible for a person to have a negative measurement, furthermore, it is impossible for a person to have a weight of zero. ", ['gender', 'weight', 'case', 'values', 'weight', 'males', 'gender', 'weight', 'rows', 'way', 'L', 'statements', 'Within', 'L', 'statements', 'items', 'incorrect', 'track', 'module', 'L', 'statements', 'way', 'data', 'Say', 'observations', 'weight', 'values', 'way', 'values', 'dataset', 'range', 'values', 'weight', 'person', 'measurement', 'person', 'weight', 'zero']), 0.28180093098831727, 0.22785144980774424)
QUERY: Dear Madam, In Unit 1 Module 5 part 1, while calculating Standard Deviation, in order to get rid off negatives, why denominator \n\" is not being squared. Ideally if we apply squaring on both sides of \" = \" , we have to square denominator too... May be a silly question here..!!!,"
*************************
((20, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 9 mod2.srt', "I, I notice I have a typo on this line, so this top one represents the 16 discordant strata where it was the control who had diabetes. The bottom one here represents the 37 discordant strata where the case had diabetes. The nice thing, these terms look really, really complicated and awful. But you're going to see that a lot of things are going to start to cancel. So, first of all, if you look at the numerator and the denominator here, all of these terms cancel. They all in the numerator and denominator, these fractions have the exact same denominator. So, all of these denominators cancel. So we can simplify this quite quickly to something much nicer, which is this e raised to alpha sub j, which is just the pair-specific intercepts and then in the denominator, all we're left with is the following. So that one's quite easy. It reduces down quite nicely and you'll see there's even more we can cancel in just a second. For this bottom one, again all of these cancel and we're left with only e raised to alpha sub i plus beta, ", ['typo', 'line', 'represents', 'strata', 'control', 'bottom', 'discordant', 'strata', 'case', 'diabetes', 'thing', 'terms', 'lot', 'things', 'numerator', 'denominator', 'terms', 'numerator', 'denominator', 'fractions', 'same', 'denominator', 'denominators', 'quite', 'something', 'nicer', 'e', 'sub', 'j', 'intercepts', 'denominator', 'bottom', 'cancel', 'sub', 'beta']), 0.231229325206432, 0.2260186250949516)
((7, '/Users/jag/Downloads/Stanford medstats/MedStatsUnit3_Mod1_Part2.srt', "And all you have to know is that if you consider order in the denominator, you're going to consider it in the numerator. So, you have to be consistent. So, there's 36 possible outcomes here. How many of those outcomes involve getting 2 sixes? Well, there's only one way you can get 2 sixes. You have to get a six and you have to get a six again. There's only 1 branch if you did the entire tree. So that probabilities going to be 1 out of Finally, what's the probability of getting a sum of 6 when rolling two dice? So, what is, what I mean by sum of 6, so that is you roll 2 dice. And what's the probability that if you add up the values from both of those rules that that value will equal 6. So, notice that our denominator is going to be the same here. There are 36 possible things that can happen when you roll two dice. So, the denominator is the same. But to get the numerator here, there's no to get the numerator other than just to count out all the ways that you can get 6. You can get a 6 by rolling a 1 and then a You can get a 6 by rolling a 5 and then a You can get a 6 by rolling a 3 and a 3. ", ['order', 'denominator', 'numerator', 'outcomes', 'outcomes', 'sixes', 'Well', 'way', 'sixes', 'branch', 'tree', 'probabilities', 'Finally', 'probability', 'sum', 'dice', 'sum', 'dice', 'probability', 'values', 'rules', 'value', 'notice', 'denominator', 'things', 'dice', 'denominator', 'numerator', 'numerator', 'ways', 'rolling', 'rolling', 'rolling']), 0.18946618668626838, 0.213203797307039)
((7, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 8 mod1 Part 2.srt', "The term for the amenorrheic woman is going to look different for, from the term from, for the oligomenorrheaic women and it's going to look different from the term for the eumenorrheaic women. And I'll be representing the interseper-, remember, trying to estimate two intercepts here, one for amenorrhea, one for oligomenorrhea, and two betas, one for amenorrhea and one for oligomenorrhea. So four things we're trying to estimate. So what is the term for the amenorrheaic women going to look like? So first of all, let's do the denominator. In the denominator, we're going to put the following: e raised to al, alpha a plus b the 8 times, eba, ebai, that's your score. That's one terminated denominator. That represents the odds being amenorrheic. We are also going to put in the denominator the odds of being oligomenorrheic. And then, we are going to put a one. Basically now we have three things in the denominator. This represents your odds of being eumenorrheic, of being oligomenorrheic, ", ['term', 'woman', 'term', 'women', 'term', 'women', 'interseper-', 'remember', 'intercepts', 'amenorrhea', 'oligomenorrhea', 'betas', 'amenorrhea', 'oligomenorrhea', 'things', 'term', 'women', 'denominator', 'denominator', 'e', 'alpha', 'plus', 'b', 'times', 'eba', 'ebai', 'score', 'denominator', 'odds', 'denominator', 'odds', 'things', 'denominator', 'odds']), 0.23809523809523808, 0.20359399599564693)
((1, '/Users/jag/Downloads/Stanford medstats/MedStatsUnit3_Mod1_Part3.srt', "then ace of diamonds versus is it's ace of diamonds, and then ace of clubs. So, we're considering order in the numerator right now. We're considering the order. The order matters. I'm counting it differently if there, if they have a different order. I don't have to do that to solve this problem, as we'll see in a minute. But if I count order in the denominator, I've gotta count an order in the numerator, and vice versa. So, for this first iteration of solving this problem, I'm going to consider order. So, that's the numerator. There's 12 ways you can draw ace aces, and literally, I've just written them out. How about the denominator? The denominator, turns out that there are 52 times 51 different two-card sequences that you can draw from a deck of cards. And again, I'm considering order here. And why is that? Where did I get that? Well, you can again, draw out some kind of tree, just like we did for the dice. So, the first thing that can happen is I can get any one of 52 cards. Let's say that in the first card, I draw, say an ace of hearts. ", ['diamonds', 'ace', 'diamonds', 'clubs', 'order', 'numerator', 'order', 'order', 'matters', 'order', 'problem', 'minute', 'order', 'denominator', 'ta', 'count', 'order', 'numerator', 'vice', 'versa', 'iteration', 'problem', 'order', 'numerator', 'ways', 'ace', 'aces', 'denominator', 'denominator', 'times', 'sequences', 'deck', 'cards', 'order', 'Well', 'draw', 'kind', 'tree', 'dice', 'thing', 'cards', 'Let', 'say', 'card', 'ace', 'hearts']), 0.2721655269759087, 0.1935573007993554)
((4, '/Users/jag/Downloads/Stanford medstats/Unit 1 Homework Solutions with questions.srt', "So, we can't just have one expression that has 11 minus the mean of our data set, we need ten of those. So, what we're going to do is multiple this by ten and similarly we're going to multiple this by 9 because there are 12 nines in our full data set. So, yes so I'll multiply the last one by six as well. And doing out this calculation, we're going to get 148 on the top divided by 59 in denominator and that's going to leave us with 1.58. So, the answer to the question is 1.58, the standard deviation of our data is 1.58. The next part of the question asks us about the median value of our data. Okay? the median if you remember, is the middle number in our data set. Okay? So, the first step whenever you're thinking about the median the interquartile range, is to arrange the numbers in your data set in order. ", ['expression', 'minus', 'data', 'ten', 'nines', 'data', 'set', 'yes', 'calculation', 'denominator', 'answer', 'question', 'deviation', 'data', 'part', 'question', 'value', 'data', 'Okay', 'middle', 'number', 'data', 'Okay', 'step', 'interquartile', 'range', 'numbers', 'data', 'order']), 0.1649572197684645, 0.19162745139525367)
((42, '/Users/jag/Downloads/Stanford medstats/Unit 9 Module 1.srt', "in the denominator the odds for a non-drinker, so they're, I put in a 1 for drinking and 0, in the numerator, and a 0 for drinking in the denominator. Now I'm comparing the odds of lung cancer a drinker compared with the odds of lung cancer for a non-drinker. Now, I've also got smoking in the model, I have to put in something for smoking, but what if I hold it fixed? So what if I say I'm only comparing a drinking smoker to a non-drinking smoker to a non-drinking smoker, so I put in a one for smoking for both the numerator and the The denominator, I hold smoking fixed. And actually, it wouldn't matter if I chose to compare a drinking non-smoker to a drinking non-smoker. I just have to hold smoking fixed for both the numerator and the denominator. I have to make it the same. If I make it the same, what you can show is that the intercepts cancel, and now, as long as I hold smoking fixed, The smoking betas cancel. What am I left with? Well, if I simplify this, I just have e raised to beta alcohol, e raised to 0 in the denominator is 1. ", ['denominator', 'odds', 'non-drinker', 'drinking', 'numerator', 'drinking', 'denominator', 'odds', 'cancer', 'drinker', 'odds', 'cancer', 'non-drinker', 'smoking', 'model', 'something', 'smoking', 'smoker', 'smoker', 'smoker', 'smoking', 'numerator', 'The', 'denominator', 'drinking', 'non-smoker', 'drinking', 'non-smoker', 'smoking', 'numerator', 'denominator', 'intercepts', 'smoking', 'cancel', 'Well', 'alcohol', 'e', 'denominator']), 0.20437977982832195, 0.17476408351902603)
((11, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit2 Mod4.srt', "So, to calculate the expected value of a in a particular stratum, we're just going to do the row one, the row times the column, total divided by the total in that stratum. That will give me the expected value of cell a, as we talked about when we talked about the chi-squared statistic last week. So I calculated that expected value. I plug that in here and then, notice, I am going to view this as a summation sign here. So I'm going to be doing this for all six strata, or all K strata, if we want to be more general, and I'm taking the observed, I'm subtracting from the expected. And then I'm adding all of those observed minus expecteds up across all the strata, and then I'm squaring that after the fact. That's the numerator. What's the denominator? The denominator is calculated by typing the variance of a. Now I haven't told you how to calculate the variance of a. So I'm going to give you the formula now. Again it's a little bit tedious, but basically we're going to calculate this ", ['value', 'stratum', 'row', 'row', 'column', 'stratum', 'value', 'cell', 'week', 'value', 'notice', 'summation', 'sign', 'strata', 'K', 'strata', 'minus', 'expecteds', 'strata', 'fact', 'numerator', 'denominator', 'denominator', 'typing', 'variance', 'variance', 'formula', 'bit']), 0.12598815766974242, 0.17475585476439628)
((3, '/Users/jag/Downloads/Stanford medstats/MedStatsUnit3_Mod1_Part3.srt', "the ace of hearts followed by the ace of diamonds will be different, a different branch, than the ace of diamonds followed by the ace of hearts. So again, we're considering order here. So, the denominator here will be 52 times Then, we can just divide the numerator by the denominator to get the probability. So, the probability that you draw two aces, when you draw two cards out of a deck is 12 divided by 52 times 51. So, that's one way to solve this problem. Again, it's not the only way. We can also try to solve the problem ignoring order. A lot of people feel like oh, I don't, why should the order matter. It doesn't matter for the hands that you have in a card deck. So, there are card games, so lets ignore order. So, we can solve the same thing ignoring order. But if we ignore order in the numerator, we've got to ignore order in the denominator. So, if I ignore order, there's actually I can write out that's there's 6 possible different pairs of aces where I'm not considering order. So, my numerator will now be 6. For the denominator, the only way to get rid of the order is simply to divide out the order. So, I'm literally going to just divide by 2 here. ", ['ace', 'hearts', 'ace', 'diamonds', 'branch', 'ace', 'diamonds', 'ace', 'hearts', 'order', 'denominator', 'times', 'numerator', 'denominator', 'probability', 'probability', 'aces', 'cards', 'deck', 'times', 'way', 'problem', 'way', 'problem', 'order', 'lot', 'people', 'oh', 'order', 'matter', 'hands', 'card', 'deck', 'card', 'games', 'lets', 'order', 'thing', 'order', 'order', 'numerator', 'order', 'denominator', 'order', 'pairs', 'aces', 'order', 'numerator', 'denominator', 'way', 'order', 'order']), 0.28342428970164046, 0.16733395568001982)
QUERY: Not sure how to look for residual confounding effect? Applying the 0.6 to 1.6 rule? Appreciate hints or pointers to the question. Thank
*************************
((0, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit4 Mod4 pt1.srt', "In this next module, I'm going to show you how to evaluate confounding in the framework of regression analysis. I'm also going to tell you a little bit about residual confounding. So first of all, it's important to understand that when we evaluate confounding within regression models, we need to focus on the effect sizes, the betas, and not on the p-values. You almost want to exclusively focus on those effect sizes. When we talk about interaction, for interaction we're actually going to focus on p-values. But for confounding, the effect sizes are what matter. And here's the rule of thumb. It's a rule of thumb, so it's not you know, it's not set in stone. But a general good rule of thumb is that you would identify a variable as a confounder if it changed the beta, the effect size relating the predictor and the outcome, the coefficient, by more than 10%. So if you have a model, where there's, you do the, ", ['module', 'confounding', 'framework', 'regression', 'analysis', 'bit', 'confounding', 'regression', 'models', 'effect', 'betas', 'p-values', 'effect', 'interaction', 'interaction', 'p-values', 'confounding', 'effect', 'matter', 'rule', 'thumb', 'rule', 'thumb', 'stone', 'rule', 'thumb', 'confounder', 'beta', 'effect', 'size', 'predictor', 'outcome', 'coefficient', '%', 'model']), 0.3578281334822566, 0.23557682934928376)
((0, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit4 Mod4.srt', "[BLANK_AUDIO] In this next module, I'm going to show you how to evaluate confounding in the framework of regression analysis. I'm also going to tell you a little bit about residual confounding. So first of all, it's important to understand that when we evaluate confounding within regression models we need to focus on the effect sizes, the betas and not on the p-values. You almost want to exclusively focus on those effect sizes. When we talk about interaction, for interaction we are actually going to focus on p-values but for confounding the effect sizes are what matter. And here's the rule of thumb. The rule of thumb is not set in stone. But a general good rule of thumb is that you would identify a variable as a confounder if it changed the beta, the effect size, relating the predictor and the outcome, the coefficient, by more than 10%. So if you have a model where there's, you know, ", ['[', 'BLANK_AUDIO', ']', 'In', 'module', 'confounding', 'framework', 'regression', 'analysis', 'bit', 'confounding', 'regression', 'models', 'effect', 'betas', 'p-values', 'effect', 'interaction', 'interaction', 'p-values', 'confounding', 'effect', 'matter', 'rule', 'thumb', 'rule', 'thumb', 'stone', 'rule', 'thumb', 'confounder', 'beta', 'effect', 'size', 'predictor', 'outcome', 'coefficient', '%', 'model']), 0.34815531191139565, 0.22920870894927675)
((25, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit4 Mod4.srt', 'So these are small effect sizes. For the Down Syndrome example the effect size was .8, so those were both within the range of 0.6 to 1.6. And statisticians have done simulations where they actually show that residual confounding can cause spurious associations in that general relative risk range, 0.6 to 1.6. In other words, if you see an effect size of two or three in your data set, that is usually beyond what residual confounding can cause. So residual confounding is kind of limited in its scope. But if you pick up a lot of observational studies and papers in the literature, you will see plenty of risk ratios and odds ratios that are 1.3, 0.8, 1.4. These are the, the when the effect sizes are this, this small, this is where you start to worry that there might be residual confounding that can completely explain the association that you are seeing. ', ['effect', 'Down', 'Syndrome', 'example', 'effect', 'size', 'range', 'statisticians', 'simulations', 'confounding', 'associations', 'risk', 'range', 'words', 'effect', 'size', 'data', 'confounding', 'confounding', 'scope', 'lot', 'studies', 'papers', 'literature', 'plenty', 'risk', 'ratios', 'odds', 'ratios', 'effect', 'confounding', 'association']), 0.30151134457776363, 0.17987370534489844)
((10, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit4 Mod4 pt2.srt', "general relative risk range, 0.6 to 1.6. In other words, if you see an effect size of two or three in your data set, that is usually beyond what residual confounding can cause. So residual confounding is kind of limited in its scope, but if you pick up a lot of observational studies, and papers, and the literature. You will see plenty of risk ratios and odds ratios that are 1.3, 0.8, 1.2, 4. These are the, the, when the effect sizes are these, this small, this is where you start to worry that there might be residual confounding that can completely explain the association that you're seeing. I've given you examples here where the residual, failure to, to completely perfectly adjust for confounding, creates spirious association. however, it can also go in the other direction. Residual confounding might also obscure relationships. It's causing you to miss associations so you might end up with a risk ratio of one, ", ['risk', 'range', 'words', 'effect', 'size', 'data', 'confounding', 'confounding', 'scope', 'lot', 'studies', 'papers', 'literature', 'plenty', 'risk', 'ratios', 'odds', 'ratios', 'effect', 'confounding', 'association', 'failure', 'creates', 'association', 'direction', 'Residual', 'confounding', 'relationships', 'associations', 'risk', 'ratio']), 0.2439346884545225, 0.16585288358968464)
((9, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit4 Mod4 pt2.srt', "All other deaths also, which is sort of a catch-all category, also is elevated in a way that makes you suspicious that there might be residual confounding. Some people actually wrote letters to the editor on this particular one worried about the issue of residual confounding. And just to help you recognize when there might be residual confounding if you're talking about a binary predictor, like you know, you're a high red meat eater or you're not. Incomplete adjustment for confounding can generate spurious relative risks in the range of 0.6 to 1.6. So notice the, the relative risk we were talking about with the red meat eating example was about 1.2, 1.3, in that range. So these are small effect sizes. For the down syndrome example. The, the effect size was 0.8. So, those were both within the range of 0.6 to 1.6. So people, statisticians have done simulations where they actually show that residual confounding can cause spurious associations in that ", ['deaths', 'sort', 'category', 'way', 'confounding', 'people', 'letters', 'editor', 'issue', 'confounding', 'confounding', 'predictor', 'meat', 'eater', 'Incomplete', 'confounding', 'risks', 'range', 'notice', 'risk', 'red', 'meat', 'example', 'range', 'effect', 'down', 'syndrome', 'example', 'effect', 'size', 'range', 'people', 'statisticians', 'simulations', 'confounding', 'associations']), 0.2522624895547565, 0.15505440386855165)
((18, '/Users/jag/Downloads/Stanford medstats/Unit 6 Module 3 part 1.srt', "So if I want to switch that, what's the probability that the null hypothesis is true? That is that the effect is not real if I get a p-value of less than 0.05. That's a harder question to answer. We have to apply Bayes' rule to answer that question. We don't have that probability in, with the conditionals in that direction. We don't have it directly and you have to make some assumptions. So you'd have to apply Bayes' rule here and then make some assumptions, and of course those assumptions can really effect the estimate here. But somebody's done that, and tried to make some reasonable assumptions. And one paper said that it turns out that about one in two p-values under 0.05 in the medical literature is a false positive. In other words, a half of all the significant p-values you see out there are just a false positive. That's their estimate based on applying Bayes' rule here but if you look at p-values less than 0.01, it goes down. Only one in six are a false positive. If you look at p values less than 0.0001, then only about 1 in 56 are false positives. So, again, those modest p-values, 0.01 to 0.05 are much, much, much more likely to be false positives. ", ['probability', 'null', 'hypothesis', 'effect', 'question', 'Bayes', 'rule', 'question', 'probability', 'conditionals', 'direction', 'assumptions', 'Bayes', 'rule', 'assumptions', 'course', 'assumptions', 'estimate', 'somebody', 'done', 'assumptions', 'paper', 'p-values', 'literature', 'false', 'words', 'half', 'p-values', 'false', 'estimate', 'applying', 'Bayes', 'rule', 'p-values', 'false', 'p', 'values', 'positives', 'p-values']), 0.19395245515057077, 0.1548955855912962)
((10, '/Users/jag/Downloads/Stanford medstats/Unit 9 Module 6.srt', "cardiovascular disease are also likely to be due simply to residual confounding. We can't adjust away all the effects of smoking and BMI, and all of those things that cluster with red meat eating. So, I think this is just a really great example of residual confounding, cause you can see the residual confounding in that mortality from injuries and sudden deaths category. The one good thing about residual confounding is, it's usually pretty contained. That is that, you worry about residual confounding when you have very moderate and small elevations in risk. So the relative risks would usually that if they're just due to residual confounding, spurious effect due to residual confounding are usually going to be in the range of 0.6 to 1.6. And people have tried to model this and kind of say well, how, how much effect, how bad can the residual confounding be? Well, there's really no relationship. You might get relative risks all the way down to 0.6. ", ['disease', 'confounding', 'effects', 'smoking', 'BMI', 'things', 'cluster', 'meat', 'example', 'confounding', 'confounding', 'mortality', 'injuries', 'deaths', 'category', 'thing', 'confounding', 'confounding', 'elevations', 'risk', 'risks', 'confounding', 'effect', 'confounding', 'range', 'people', 'kind', 'say', 'effect', 'confounding', 'Well', 'relationship', 'risks', 'way']), 0.31098520678556146, 0.14842044906881768)
((6, '/Users/jag/Downloads/Stanford medstats/Unit 9.srt', "key decision, and if you have knowledge and experience and know what to do, then the third part of this two challenge rule is actually, you know, making it happen, providing it is safe. And for the patient and you do know what needs to be done, very precisely, so, that's the two challenge rule. The analogy that the airline industry makes reference to, is many times you have a pilot and a copilot, and they're dealing with all kinds of issues and events. At some time a pilot is so focused on what is going on, that sometime a question to cause a pause in what is going on, can be a rebooting or retriggering of the thought process that needs to be done to address the crisis situation on hand. The same could be said in an operating room, an ICU, and emergency department, that sometimes what it takes is that pause, that question. We've all seen the television shows and ", ['decision', 'knowledge', 'experience', 'part', 'challenge', 'rule', 'patient', 'challenge', 'rule', 'analogy', 'airline', 'industry', 'reference', 'times', 'pilot', 'copilot', 'kinds', 'issues', 'events', 'time', 'pilot', 'sometime', 'question', 'pause', 'rebooting', 'thought', 'process', 'address', 'crisis', 'situation', 'hand', 'operating', 'room', 'ICU', 'emergency', 'department', 'pause', 'question', 'television']), 0.1722921969015792, 0.1458580237081939)
QUERY: I am confused in applying the concept of the 0.6 to 1.6 to residual confounding.  I searched it online, but could not find any papers/discussion useful.  Could you help me understand how to use this 0.6 to 1.6 generalization for residual confounding?
*************************
((11, '/Users/jag/Downloads/Stanford medstats/Unit 9 Module 6.srt', "Or maybe all the way up to 1.6, just purely by residual confounding. But it's hard to get much bigger than that, if you know, if you've adjusted for the things, and your, you, you've, you have some error, it, it can make errors in that kind of range. But your not going get, we're not worried about getting the relative risk of like two or three just do to residual confounding usually. Now, what we notice here is of course that in this case what were our hazard ratios, they were like 1.22, 1.27, so they were totally within that range. So, when you're talking about these kind of moderate size relative risks, that's where you start to think hm, they're kind of moderate size could this be due to residual confounding? And one last thing to keep in mind is that usually we think about confounding, sort of creating these spurious associations, but confounding can also obscure relationships. And similarly, residual confounding can also obscure relationships. So, sometimes you may fail to find an effect because of residual confounding. And those examples are harder to detect because you, you know you just don't see anything. ", ['way', 'confounding', 'things', 'error', 'errors', 'kind', 'range', 'get', 'risk', 'like', 'confounding', 'course', 'case', 'hazard', 'ratios', 'range', 'kind', 'size', 'risks', 'hm', 'size', 'confounding', 'thing', 'mind', 'sort', 'associations', 'relationships', 'confounding', 'relationships', 'effect', 'confounding', 'examples', 'anything']), 0.35511041211421757, 0.34130990348881113)
((10, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit4 Mod4 pt2.srt', "general relative risk range, 0.6 to 1.6. In other words, if you see an effect size of two or three in your data set, that is usually beyond what residual confounding can cause. So residual confounding is kind of limited in its scope, but if you pick up a lot of observational studies, and papers, and the literature. You will see plenty of risk ratios and odds ratios that are 1.3, 0.8, 1.2, 4. These are the, the, when the effect sizes are these, this small, this is where you start to worry that there might be residual confounding that can completely explain the association that you're seeing. I've given you examples here where the residual, failure to, to completely perfectly adjust for confounding, creates spirious association. however, it can also go in the other direction. Residual confounding might also obscure relationships. It's causing you to miss associations so you might end up with a risk ratio of one, ", ['risk', 'range', 'words', 'effect', 'size', 'data', 'confounding', 'confounding', 'scope', 'lot', 'studies', 'papers', 'literature', 'plenty', 'risk', 'ratios', 'odds', 'ratios', 'effect', 'confounding', 'association', 'failure', 'creates', 'association', 'direction', 'Residual', 'confounding', 'relationships', 'associations', 'risk', 'ratio']), 0.2991830368027063, 0.32870742164465877)
((0, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit4 Mod4 pt2.srt', "What's interesting about this particular example, one of the reasons I like to actually use this example, is that the authors made a point of illustrating something called residual confounding. And it's very important as we start talking about statistical adjustment for confounding, it's really important to keep in the back of your mind That adjustment for confounding is not a panacea. You cannot adjust away all confounding and that is kind of well illustrated here. So, this first regression analysis was actually included in the paper just as an illustration of residual confounding. What they did, in that very first regression analysis is they model age crudely. They put into their regression as a predictor you're either younger than 35 or you're 35 and older. So they model age as a binary predictor. When they do that, we are indeed attenuating the odds ratio. It's going from 0.8 to 0.87, but we are not completely wiping out the effect ", ['example', 'reasons', 'example', 'authors', 'point', 'something', 'confounding', 'adjustment', 'confounding', 'back', 'mind', 'adjustment', 'confounding', 'panacea', 'regression', 'analysis', 'paper', 'illustration', 'confounding', 'regression', 'analysis', 'age', 'regression', 'predictor', 'age', 'predictor', 'odds', 'effect']), 0.2964997266644405, 0.32575931346831194)
((10, '/Users/jag/Downloads/Stanford medstats/Unit 9 Module 6.srt', "cardiovascular disease are also likely to be due simply to residual confounding. We can't adjust away all the effects of smoking and BMI, and all of those things that cluster with red meat eating. So, I think this is just a really great example of residual confounding, cause you can see the residual confounding in that mortality from injuries and sudden deaths category. The one good thing about residual confounding is, it's usually pretty contained. That is that, you worry about residual confounding when you have very moderate and small elevations in risk. So the relative risks would usually that if they're just due to residual confounding, spurious effect due to residual confounding are usually going to be in the range of 0.6 to 1.6. And people have tried to model this and kind of say well, how, how much effect, how bad can the residual confounding be? Well, there's really no relationship. You might get relative risks all the way down to 0.6. ", ['disease', 'confounding', 'effects', 'smoking', 'BMI', 'things', 'cluster', 'meat', 'example', 'confounding', 'confounding', 'mortality', 'injuries', 'deaths', 'category', 'thing', 'confounding', 'confounding', 'elevations', 'risk', 'risks', 'confounding', 'effect', 'confounding', 'range', 'people', 'kind', 'say', 'effect', 'confounding', 'Well', 'relationship', 'risks', 'way']), 0.4577036541524985, 0.32447024511938966)
((9, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit4 Mod4 pt2.srt', "All other deaths also, which is sort of a catch-all category, also is elevated in a way that makes you suspicious that there might be residual confounding. Some people actually wrote letters to the editor on this particular one worried about the issue of residual confounding. And just to help you recognize when there might be residual confounding if you're talking about a binary predictor, like you know, you're a high red meat eater or you're not. Incomplete adjustment for confounding can generate spurious relative risks in the range of 0.6 to 1.6. So notice the, the relative risk we were talking about with the red meat eating example was about 1.2, 1.3, in that range. So these are small effect sizes. For the down syndrome example. The, the effect size was 0.8. So, those were both within the range of 0.6 to 1.6. So people, statisticians have done simulations where they actually show that residual confounding can cause spurious associations in that ", ['deaths', 'sort', 'category', 'way', 'confounding', 'people', 'letters', 'editor', 'issue', 'confounding', 'confounding', 'predictor', 'meat', 'eater', 'Incomplete', 'confounding', 'risks', 'range', 'notice', 'risk', 'red', 'meat', 'example', 'range', 'effect', 'down', 'syndrome', 'example', 'effect', 'size', 'range', 'people', 'statisticians', 'simulations', 'confounding', 'associations']), 0.33149677206589795, 0.318613950537369)
((18, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit4 Mod4.srt', "We can probably get a good gauge of that. Smoking is relatively easy to measure although there might be some confounding, some leftover confounding with smoking because we're not going to measure that totally precisely. But age is going to be measured precisely. So we were able to wipe out most of the confounding in that example. But I'll just tell you that residual confounding comes up a lot with observational studies. You say, oh I've adjusted for smoking, I've adjusted for age, therefore I've taken care of those. They're all done with. There can't be any confounding by those. But in fact that's not true. If you've measured your confounders at all imperfectly or if you've measured your predictors or your outcome at all imperfectly it's impossible to completely adjust away all the confounding. In most cases you end up with some leftover or residual confounding. And my favorite example to illustrate that is an example that was in the American Archives, the Archives of Internal Medicine a few years back. Made a lot of headlines in the news media. I've shown this table before. ", ['gauge', 'leftover', 'smoking', 'age', 'confounding', 'example', 'confounding', 'lot', 'studies', 'smoking', 'age', 'care', 'done', 'confounding', 'fact', 'confounders', 'predictors', 'outcome', 'confounding', 'cases', 'leftover', 'confounding', 'example', 'example', 'American', 'Archives', 'Archives', 'Internal', 'Medicine', 'years', 'Made', 'lot', 'headlines', 'news', 'media', 'table']), 0.3268602252303068, 0.3141575918979384)
((15, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit4 Mod4.srt', "So if you wanted to you could convert the ln of 0.8 here and then of course ln of 1 would be 0. And you can compare whatever this value turns out to be to 0, rather comparing the odds ratios, but clearly we have a major change in the effects size here. So that's how we ach, evaluate confounding in, in the context of regression. What's interesting about this particular example, one of the reasons I like to actually use this example, is that the authors made of point of illustrating something called residual confounding. And it's very important as we start talking a lot about statistical adjustment for confounding, it's really important to keep in the back of your mind that adjustment for confounding is not a panacea. You can not adjust away all confounding. And that's kind of illustrated here. So this first regression analysis was actually included in the paper just as an illustration of residual confounding. What they did in that very first regression analysis is they modeled age crudely. They put into their regression, as a predictor, ", ['ln', 'course', 'ln', 'value', 'odds', 'change', 'effects', 'confounding', 'context', 'regression', 'example', 'reasons', 'example', 'authors', 'point', 'something', 'confounding', 'lot', 'adjustment', 'confounding', 'back', 'mind', 'adjustment', 'confounding', 'panacea', 'kind', 'regression', 'analysis', 'paper', 'illustration', 'confounding', 'regression', 'analysis', 'age', 'regression', 'predictor']), 0.31814238148788887, 0.3057785461002101)
((25, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit4 Mod4.srt', 'So these are small effect sizes. For the Down Syndrome example the effect size was .8, so those were both within the range of 0.6 to 1.6. And statisticians have done simulations where they actually show that residual confounding can cause spurious associations in that general relative risk range, 0.6 to 1.6. In other words, if you see an effect size of two or three in your data set, that is usually beyond what residual confounding can cause. So residual confounding is kind of limited in its scope. But if you pick up a lot of observational studies and papers in the literature, you will see plenty of risk ratios and odds ratios that are 1.3, 0.8, 1.4. These are the, the when the effect sizes are this, this small, this is where you start to worry that there might be residual confounding that can completely explain the association that you are seeing. ', ['effect', 'Down', 'Syndrome', 'example', 'effect', 'size', 'range', 'statisticians', 'simulations', 'confounding', 'associations', 'risk', 'range', 'words', 'effect', 'size', 'data', 'confounding', 'confounding', 'scope', 'lot', 'studies', 'papers', 'literature', 'plenty', 'risk', 'ratios', 'odds', 'ratios', 'effect', 'confounding', 'association']), 0.2773500981126146, 0.3047199353872793)
QUERY:  I have a question about interpreting hazard ratios for predictors that are continuous variables (using an example not from the modules).  In a study published last month looking at the relationship of incident cancers and height of women (abstract linked to below), an estimated HR for thyroid cancer (outcome variable) per 10 centimeter increase in height (predictor variable) was 1.29. Does this mean the test group had a 29% increase in rate of thyroid cancer for every 10 centimeters that they were taller than controls? This is where I am confused because if that is true, an individual would have over 100% incidence rate if she were >34 cm (13 inches) taller than a reference person, right? Or can it not be interpreted that way because a hazard rate is instantaneous and not an overall incidence rate?  http://cebp.aacrjournals.org/content/early/2013/07/25/1055-9965.EPI-13-0305.abstract  Thanks for your input.
*************************
((5, '/Users/jag/Downloads/Stanford medstats/HRP262 Unit 2 Module 2 Part 1.srt', "So just keep that in the back of your mind. This week we are talking about parametric regression, trying to actually estimate that hazard function, but eventually we may try to, want to try to avoid estimating it and, and giving it any particular mathematical form and that's what cox regression does for next week. Alright, so, I now just kind of want to go through an example to show you how How this works. When the hazard function is constant what does that tell us about survival times? For the pregnancy data, we have this pregnancy data that I showed you last week, some women who had sub-frudoil or sub-frutile they were given some treatment they were followed over 2 years. When we look at that data, we cal, we calculate an average incidence rate of 10.4% per month. In other words, about 10.4%of women who were at risk of getting pregnant in any given month, got pregnant. Now, for the purposes of just making this, really, a really simple calculation. What I'm going to say is that we imagine we have a case where the hazard rate is ", ['back', 'mind', 'week', 'regression', 'hazard', 'function', 'estimating', 'form', 'cox', 'regression', 'week', 'Alright', 'example', 'show', 'hazard', 'function', 'tell', 'survival', 'times', 'pregnancy', 'data', 'pregnancy', 'data', 'week', 'women', 'treatment', 'years', 'data', 'incidence', 'rate', '%', 'month', 'words', '%', 'women', 'risk', 'month', 'purposes', 'calculation', 'case', 'hazard', 'rate']), 0.31157637660636084, 0.33670161105758756)
((7, '/Users/jag/Downloads/Stanford medstats/HRP262 Unit 1 Module 2.srt', "So I did a rough estimation from the curve kind of taking the, the mid point, of the ranges that are shown here. I got that there was about maybe a total of 1781, Person years represented here you know everybody died eventually so there's a 100 events I started with a 100 people. There's 100 deaths and they were alive in cumulative total for about 70 net 181 person years. So we can calculate that the rate of death was .05 Six per year so about your chances of dying in any given year were about 5 to 6% which is really, really high. The last thing I want to point out is this module has talked about incidence rates which I'm assuming that many of you are already actually familiar with. Again, the incidence rate just gives the average rate of the events over a given time period. What we're going to be focusing on more in this class, is something called the hazard rate. So I want to just put in your mind right now what the concept of the hazard rate is. It's very close to the incidence rate, But it's an instantaneous incidence rate. ", ['estimation', 'curve', 'kind', 'mid', 'point', 'ranges', 'total', 'Person', 'years', 'everybody', 'events', 'people', 'deaths', 'total', 'person', 'years', 'rate', 'death', 'year', 'chances', 'year', '%', 'thing', 'point', 'module', 'incidence', 'rates', 'incidence', 'rate', 'rate', 'events', 'time', 'period', 'class', 'something', 'hazard', 'rate', 'mind', 'concept', 'hazard', 'rate', 'incidence', 'rate', 'incidence', 'rate']), 0.43756979945874225, 0.3315210878321964)
((4, '/Users/jag/Downloads/Stanford medstats/Unit 2 Module 6.srt', "also presenting the data on on the things that were beneficial from the hormone. So, this was very well done coverage. It's very unusual because if you look at the media coverage most of the medical literature you'll see that they almost always talk about relative risks and not, and not absolute risks. And that's not really the, the fault of the media necessarily. That's just the way that these data are usually presented in medical studies. So just to, to drill down a little bit more on, on the data from this study, so if you looked at breast cancer, you can see that the rate of breast cancer in the hormone group, the incidence rate was 38 breast cancer cases per 10,000 women-years. In the placebo group, that rate was 30 per 10,000. So the rate difference was eight additional breast cancer cases per 10,000 women years. So that's exactly what they presented in the press release. However, if you look at the paper, the paper presented that information. It also presented the relative risk. And the relative risk here was 1.26. The interpretation would be that there ", ['data', 'things', 'hormone', 'coverage', 'media', 'coverage', 'literature', 'risks', 'risks', 'fault', 'media', 'way', 'data', 'studies', 'bit', 'data', 'study', 'cancer', 'rate', 'cancer', 'hormone', 'group', 'incidence', 'rate', 'cancer', 'cases', 'women-years', 'placebo', 'group', 'rate', 'rate', 'difference', 'cancer', 'cases', 'women', 'years', 'press', 'release', 'paper', 'paper', 'information', 'risk', 'risk', 'interpretation']), 0.35033281020408535, 0.3233641534639121)
((5, '/Users/jag/Downloads/Stanford medstats/Unit 9 Homework with Answers.srt', "then dividing by 1 plus e raised to the predicted logit. If you calculate that all that out, you get a a percentage, a predicted probability of 54% so the correct answer here is e. In including continuous predictors in the model, the authors are assuming that these predictors have a linear relationship with our outcome here, which is the logit of death. So the correct answer here is b. The correct answer here is a. Our hazard ratio is 1.35 so that indicates a 35% increase in the rate of dementia after accounting for confounders. And it is statistically significant, because that confidence interval does not cross one, it goes from 1.14 to 1.60, so that is statistically significant. So, how do we interpret these hazard ratios for the underweight for women? So, what you'll notice is that they are all elevated by similar amounts as ", ['e', 'logit', 'percentage', 'probability', '%', 'correct', 'answer', 'e.', 'In', 'predictors', 'model', 'authors', 'predictors', 'relationship', 'outcome', 'logit', 'death', 'correct', 'answer', 'b', 'correct', 'answer', 'hazard', 'ratio', 'indicates', '%', 'increase', 'rate', 'dementia', 'confounders', 'confidence', 'interval', 'hazard', 'ratios', 'underweight', 'women', 'amounts']), 0.2840095485615366, 0.3233608852009715)
((0, '/Users/jag/Downloads/Stanford medstats/HRP262 Unit 1 Module 2.srt', "[BLANK_AUDIO] In this next module, I'm going to review the concepts of incidence rates and rate ratios. So if you have time-to-event outcome data. The most basic statistic that you can calculate is the incidence rate, which just gives you the average rate of events over the time period of the study. You can also compare incidence rates between different groups with something like a rate ratio. Just as an example, here's some data on 38 women who were having trouble with fertility. They were all given a treatment, and then the researchers followed them for about two years To see which women got pregnant or did not. Everybody got the same treatment so there's really only one group here. The left column here shows all the women who got pregnant, shows which month into the study they get, got pregnant. Remember, women can only get pregnant once a month. So the unit of time here is months. On, on the right hand column, these are the women who are censored that is that ", ['[', 'BLANK_AUDIO', ']', 'In', 'module', 'concepts', 'incidence', 'rates', 'rate', 'ratios', 'outcome', 'data', 'incidence', 'rate', 'rate', 'events', 'time', 'period', 'study', 'incidence', 'rates', 'groups', 'something', 'rate', 'ratio', 'example', 'data', 'women', 'trouble', 'fertility', 'treatment', 'researchers', 'years', 'women', 'Everybody', 'treatment', 'group', 'column', 'women', 'month', 'study', 'Remember', 'women', 'once', 'month', 'unit', 'time', 'months', 'right', 'hand', 'column', 'women']), 0.3679524918397825, 0.3181240396707269)
((2, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 3 Module 1 Part 2.srt', "was chosen as the reference groups so it's set to 1.00. The non-significant trend group was had a decreased rate of publication so the hez, the hazard ratio's below 1. It does cross one here, it's not a statistically significant difference from the null group but you can see there's some suggestion of that group having a worse publication rate. And finally, the significant group, the ones who actually found significant results, did have a statistically significant increase in the publication rate compared with the null group. And we can interpret this hazard ratio as basically having about a twofold higher incidence of publication compared to those will null results. So it gives us an effect size we can measure. Second example a, of a use of Cox regression in a literature has to do with a series of studies that were published that were looking at Academy Award winners versus nominees, and trying to get at this question, people always say, well, it was just an honor to be nominated. Is it really as good to just be nominated as it is to actually win, ", ['reference', 'groups', 'trend', 'group', 'rate', 'publication', 'hez', 'hazard', 'ratio', 'difference', 'null', 'group', 'suggestion', 'group', 'worse', 'publication', 'rate', 'group', 'ones', 'results', 'increase', 'publication', 'rate', 'group', 'hazard', 'ratio', 'twofold', 'incidence', 'publication', 'results', 'effect', 'size', 'example', 'use', 'Cox', 'regression', 'literature', 'series', 'studies', 'Academy', 'Award', 'winners', 'nominees', 'question', 'people', 'honor']), 0.31642963502304483, 0.3178953210482779)
((1, '/Users/jag/Downloads/Stanford medstats/HRP262 Unit 2 Module 2 Part 2.srt', "increasing, changing over time, but we can still do this kind of same calculation. So imagine that that's the hazard rate and we are starting with a 100 women and we're looking for the probabilities for pregnancy within 6 months. So the number remaining at risk, this is those that haven't gotten pregnant yet, the instance of pregancies. [BLANK_AUDIO] And women to our left. So again, let's just go through this calculation. Sometimes it's helpful to write something out like this to really understand what's going on. So in the first month, the hazard rate is 10%, the time is one, so 1 raised to .2 is still 1. And so the hazard rate in the first month is just 10%. There were 100 women at risk, so we're expecting about 10 of them, 10% of 100, to get pregnant. Means 90 are left, that one's easy. For 10.2, it's just slightly more complicated. Which is 10% times month two raised to 0.2. Well, 2 raised to 0.2 is 1.14, ", ['time', 'kind', 'calculation', 'imagine', 'hazard', 'rate', 'women', 'probabilities', 'pregnancy', 'months', 'number', 'risk', 'instance', 'pregancies', '[', 'BLANK_AUDIO', ']', 'women', 'left', 'calculation', 'something', 'month', 'hazard', 'rate', '%', 'time', 'hazard', 'rate', 'month', '%', 'women', 'risk', '%', 'Means', '%', 'times', 'month', 'Well']), 0.38357064118830725, 0.3163095667034606)
((20, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 8 mod3 Part 2.srt', "The technology and technology will cancel out and we're left with 1.1549 in the numerator. Greater. So we can interpret this as for every one unit increase in technology, this is the increase in the rate of world records per year. So we call that an incidence rate ratio and again that turns out to be 1.1676. We could also use the standard error to calculate confidence intervals. The confidence interval here turns out to be 1.085 to 1.256, so for every new technology introduced in a year, we have a 16% increase in the rate of world records. So it has, the Poisson regression has a nice, easy interpretation. These are not odds ratios, these are actually rate ratios, so that's great, because we don't have to worry about the confusion. With odds ratios being misleading. We're actually getting out rate ratios here. Now one more thing just for fun. As I was playing around with these data, I also tried an interaction between the new technol, the number of ", ['technology', 'technology', 'numerator', 'unit', 'increase', 'technology', 'increase', 'rate', 'world', 'records', 'year', 'incidence', 'rate', 'ratio', 'turns', 'error', 'confidence', 'intervals', 'confidence', 'interval', 'technology', 'year', '%', 'increase', 'rate', 'world', 'records', 'Poisson', 'regression', 'interpretation', 'ratios', 'ratios', 'confusion', 'odds', 'ratios', 'rate', 'ratios', 'thing', 'fun', 'data', 'interaction', 'technol', 'number']), 0.333518672982535, 0.30143044999340635)
QUERY: **Its necessary to do univariate analysis before regression analysis, then insert variables in regression models that have significant association with outcome variable? if its true; then we do univariate analysis in regression or we can use also another statistical tests like:  t test, chi square,....? 2- For determination the interaction, its necessary that both variables have a significant relationship with outcome variable in univariate analysis?
*************************
((3, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 7 mod2.srt', "And what you can do is just simply run a univariate logistic regression that contains only one predictor at a time. So you take each of your candidate predictors, you put it in a model, one at at time, with delirium as the outcome. And you see whether or not this predictor has a significant Relationship with the outcome, if they're, if we haven't adjusted for anything. And so they assess the association between each potential predictor, and, and delirium, using this univariate regression, and then which one of these variables are we going to move on to the multivariate regression. Well in this case they said we're going to exclude anything, any of these predictors that have a p value above .15 in the univariate analysis, so if you run this regression. And P is greater than .145 and we're not going to bother with this predictor any more, we're just going to drop it now because it's not related to the [UNKNOWN]. They also excluded variables that had a prevalence below 10% so if there was a particular predictor that was a binary predictor and ", ['univariate', 'regression', 'predictor', 'time', 'candidate', 'predictors', 'model', 'time', 'delirium', 'outcome', 'predictor', 'Relationship', 'outcome', 'anything', 'association', 'potential', 'predictor', 'delirium', 'univariate', 'regression', 'variables', 'multivariate', 'regression', 'Well', 'case', 'anything', 'predictors', 'p', 'value', 'univariate', 'analysis', 'regression', 'P', 'predictor', '[', 'UNKNOWN', ']', 'variables', 'prevalence', '%', 'predictor', 'predictor']), 0.41756313817071355, 0.41482449166703855)
((0, '/Users/jag/Downloads/Stanford medstats/Unit6 Module5.srt', "Now we're ready to do some simple analysis on the data. Remember, you have two types of variables in your dataset. Continuous variables and categorical variables.  Let's look at the distributions of our continuous variables first. We can do this using proc univariate. As you can see from the code. We start out with proc univariate and then specify our data set. Then using a var statement, we can specify the variables that we want to have in our univariate analysis. We're going to analyze age, BMI, weight, and height. We then include a histogram statement, so they get a histogram for each of these variables. As you can see from the output, we still have age versus BMI by deceit status. This is carried over from the previous graphic. That's why you usually want to include G options, reset equals Alt before and after each of your graphics. Here's the output for the variable age. You can see the mean, median, mode, standard deviation, variance and range. ", ['analysis', 'data', 'Remember', 'types', 'variables', 'dataset', 'variables', 'variables', 'Let', 'look', 'distributions', 'variables', 'proc', 'univariate', 'code', 'proc', 'univariate', 'data', 'var', 'statement', 'variables', 'univariate', 'analysis', 'age', 'BMI', 'histogram', 'statement', 'histogram', 'variables', 'output', 'age', 'versus', 'BMI', 'deceit', 'status', 'G', 'options', 'equals', 'Alt', 'graphics', 'output', 'age', 'mode', 'deviation', 'variance', 'range']), 0.3561567943220792, 0.34321601034202104)
((0, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit3 Mod1 pt1.srt', "In this next module, we're going to dive right in to the logistic regression model. We've talked about, when you have binary or categorical outcomes that, that is when you're looking at proportions, we've talked about some statistical tests, when you don't want to adjust for anything that you can use. So this is like the risk difference, relative risk, the chi square test, the McNemar's chi-square test. So we talked about those, but what about when you want to do a regression analysis when you have a binary or categorical outcome? In that case, you're going to be using logistic regression, which we're going to talk about in this module. I want to just give you a little overview of logistic regression. So to remind you, when we did linear regression, the model looked like the following. We were predicting the value of some continuous outcome variable, call it y, as a function of x. Here's the simple linear regression model. We have the intercept alpha, and beta is our slope. ", ['module', 'regression', 'model', 'outcomes', 'proportions', 'tests', 'anything', 'risk', 'difference', 'risk', 'chi', 'square', 'test', 'McNemar', 'test', 'regression', 'analysis', 'outcome', 'case', 'regression', 'module', 'overview', 'regression', 'regression', 'model', 'value', 'outcome', 'call', 'function', 'x', 'linear', 'regression', 'model', 'intercept', 'alpha', 'beta', 'slope']), 0.42723075247394027, 0.2873397984255384)
((4, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit5 Unit 4 part1 retake.srt', "There's a lot of opportunities for observations to get kicked out. If you were putting more than 11 variables, which is very typical, you can imagine that a little bit of missing data here and there can result in, say you start with a sample size of 500, that suddenly might turn out to go down to a sample size of 200. In other words, your analysis might only run on 200 of your observations even if you started with 500. So you've got to be very careful and make sure that you always fill in your missing data in some way. The other thing that you're going to need to do is you need to standardize your variables. We already talked about that. They all have to have a mean of zero and a standard deviation of one. One other thing I just want to point out now, is that principal components analysis typically run numeric variables. So, we saw in the last module that principal components analysis is based on the linear relationship. That is, the linear correlation between pairs of variables. So technically, all the assumptions of linear regression would apply here. ", ['lot', 'opportunities', 'observations', 'variables', 'bit', 'data', 'sample', 'size', 'sample', 'size', 'words', 'analysis', 'observations', 'missing', 'data', 'way', 'thing', 'variables', 'zero', 'deviation', 'thing', 'point', 'components', 'analysis', 'variables', 'module', 'principal', 'components', 'analysis', 'linear', 'relationship', 'linear', 'correlation', 'pairs', 'variables', 'assumptions', 'regression']), 0.35836865605893176, 0.2840356036909532)
((2, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit6 Mod2.srt', "So first of all imagine that you've got some variables in your dataset that aren't necessarily that important you maybe there's there potential confounders and you've collected that data but its not your primary predictor of interest, it's not your primary outcome of interest. But those variables are missing a huge amount of data. 40, 50, 60% of the observations in your data set don't have a value for that variable. The first thing you might consider doing is just not including that variable in your analysis. Especially if you've measured the same construct with other variables. If there's other variables available to get at it. It's worth considering, is there enough information here? Is it really even worth it for me? To use that variable. If I use that variable, I'm going to have to impute those values. That adds a lot of noise so, I might just consider not even using that at all in my analysis. Now consider the situation where you're doing some kind of observational study, and you have a particular hypothesis that you're trying to test. ", ['imagine', 'variables', 'dataset', 'confounders', 'data', 'predictor', 'interest', 'outcome', 'interest', 'variables', 'amount', 'data', '%', 'observations', 'data', 'value', 'thing', 'analysis', 'construct', 'variables', 'variables', 'considering', 'information', 'values', 'lot', 'noise', 'analysis', 'situation', 'kind', 'study', 'hypothesis', 'test']), 0.32070079957649444, 0.2813982777680281)
((7, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit6 Mod2.srt', "a variable. And that variable's not one of the most important variables, but you want to include it in your model. Just filling in with something is better than not filling in, because that's going to mean that this observation gets included in your regression analysis when it otherwise would've been thrown out. So filling in with the mean is often good enough. And it's not the best thing that you can do, there are other potentially better alternatives but it's very simple and easy to do and it's a lot better than not filling in with anything and just having that observation kicked out. So oftentimes if I've got variables where, you know, one or two people are missing the value and I want to include it in my model and it's not the most important variable I might just fill in with mean. Slightly better strategy, little bit more work, is to use linear regression to predict the missing values. So remember we've got a dataset that may have other variables that are related to height in it. So, we might have weight on the person if, you know, they're young. ", ['variables', 'model', 'something', 'observation', 'regression', 'analysis', 'filling', 'thing', 'alternatives', 'lot', 'better', 'anything', 'observation', 'oftentimes', 'variables', 'people', 'value', 'model', 'strategy', 'bit', 'work', 'regression', 'missing', 'values', 'remember', 'dataset', 'variables', 'height', 'person']), 0.3099355376904239, 0.280059244901887)
((1, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 7 mod3 take2.srt', "like principal components analysis. But somehow you're going to get your variables down to a more manageable set. And then there's a lot of different ways we can approach model building, and everybody has slightly different preferences. So, you know, it's really great to read the literature and see what different people do, but I'm going to give you some general strategies that are typically used just to get you started. And so you might choose to do your prediction modeling with some kind of manual selection, as opposed to an automated model building procedure. So, you know, a typical thing that people might do is they might do, start with some kind of univariate screen. So you might say, well, all the p-values we saw in the delirium example, that if the p-values were greater than 0.15, we, in univariate models, we're just going to eliminate those variables. So some way to, again, get that candidate set of predictors down to a smaller set, before we do any further modeling. So that's very typical, you probably want to also use some clinical judgment there, about what variables to move forward. ", ['principal', 'components', 'analysis', 'variables', 'set', 'lot', 'ways', 'model', 'building', 'everybody', 'preferences', 'literature', 'people', 'strategies', 'prediction', 'kind', 'selection', 'model', 'building', 'procedure', 'thing', 'people', 'kind', 'screen', 'p-values', 'delirium', 'example', 'p-values', 'univariate', 'models', 'variables', 'way', 'candidate', 'set', 'predictors', 'set', 'modeling', 'judgment', 'variables']), 0.22233428588149945, 0.27141144592103444)
((17, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 5 Module 1 Part 2.srt', "And you can see how off the Kaplan\xe2\x80\x93Meier estimates can be, as is illustrated here. Now what about the multivariate situation? I basically just set you up for the univariate, the unadjusted situation, we just want to look at basic Kaplan-Meiers. When we get to the multivariate case, it kind of depends on what the point of your analysis is. If you care about absolute probability, you are going to get things wrong if you treat the competing risks as censored values. However, there's a lot of situations where we're doing some kind of explanatory analysis, and we don't really care about the absolute probabilities. We just want to compare, say the treatment group to the control group. It turns out that it's probably fine if the absolute probabilities don't matter. To just treat the competing events as censored values in a Cox regression. And the hazard ratios will probably be reasonable. But, keep in mind that if you're doing some kind of predictive or prognostic study and you're trying to actually get those absolute probabilities, treating the competing events as censored will give you the wrong probability. ", ['multivariate', 'situation', 'univariate', 'situation', 'Kaplan-Meiers', 'multivariate', 'case', 'depends', 'point', 'analysis', 'probability', 'things', 'risks', 'values', 'lot', 'situations', 'kind', 'analysis', 'probabilities', 'treatment', 'group', 'control', 'group', 'fine', 'probabilities', 'events', 'values', 'Cox', 'regression', 'hazard', 'ratios', 'mind', 'kind', 'study', 'probabilities', 'events', 'probability']), 0.22607125827950406, 0.2697475258614774)
QUERY: cant you use the Mann Whitney U test for comparing the absolute scores ( as often the distribution of the scores are non parametric)between groups and for more than 2 groups Kruskall Wallis?
*************************
((8, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 6 Module 1 Part 1.srt', "Again, we can't say, specifically, which groups differ. But those groups are definitely not all equal. We also want to just plot the outcome variable of the scores here in a histogram, to look at whether or not these are normally distributed. This is a small sample. So we would want to make sure that we met the normality assumption here. So I plotted these scores in a histogram. The average scores around high 20s and low 30s and you can see that it's reasonably normally distributed. That's good enough for us to be able to run linear models, ANOVA linear regression on these data. Alright. So now, I am actually going to walk. Through the mechanics of calculating that one-way ANOVA. And what I've done here is just rearranged the data a little bit to make this easier. So here's my four groups, here are the six kids in each group. Here are their scores, their, their scores on that post test. And I've already calculated for each group. The mean score on that test for each group. And we saw on the plot, on the box plot that groups 1 and ", ['groups', 'groups', 'plot', 'outcome', 'scores', 'histogram', 'sample', 'normality', 'assumption', 'scores', 'histogram', 'scores', 'models', 'ANOVA', 'linear', 'regression', 'data', 'Alright', 'mechanics', 'ANOVA', 'data', 'bit', 'groups', 'kids', 'group', 'scores', 'scores', 'post', 'test', 'group', 'score', 'test', 'group', 'plot', 'box', 'plot', 'groups']), 0.47404546313997725, 0.3957550018827301)
((2, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit6 Mod5 part2.srt', "different treatment groups. Then we're going to take the propensity scores and we're going to stratify or, match on the resulting scores. And we're going to come up with now, comparing apples and apples. So we're going to be now comparing only people who are in quintile one to people who are in quintile one, for example, or we're going to have a matched cohort where every Ross patient has been matched to a mechanical valve patient. So our groups are now comparable, and we want to assess whether or not those covariates that we put in the regression model, whether or not those are balanced. Hopefully they'll be reasonably balanced. I'll show you an example in a minute. If the balance isn't great, we're going to go back and actually try to refit the model, stuffing more things in the model, other additional confounders, higher-order terms to get a better fit. To get better balance and just to say again what I've said already about missing data. After an individual is missing one data point for one variable, they're going to be omitted from that logistic regression model, so it's going to be very important to impute missing data before doing propensity scores. ", ['treatment', 'groups', 'propensity', 'scores', 'match', 'scores', 'apples', 'apples', 'people', 'people', 'example', 'cohort', 'Ross', 'patient', 'valve', 'patient', 'groups', 'assess', 'covariates', 'regression', 'model', 'example', 'minute', 'balance', 'try', 'model', 'things', 'model', 'confounders', 'terms', 'fit', 'balance', 'data', 'individual', 'data', 'point', 'regression', 'model', 'missing', 'data', 'propensity', 'scores']), 0.24693239916239737, 0.2832381121029699)
((14, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit6 Mod6.srt', "Or even whole groups, whole treatment groups that you really can't compare. It optimizes stratification and matching as we talked about. And, it makes statistical adjustment for a large set of confounders possible when you don't. Really have enough data to fit a regression model on so many variables. Now the disadvantages of propensity scores are, as I keep saying, they're inferior to randomization. So they don't solve the problem of either residual or unmeasured confounding. However they might give researchers or readers a False sense of security, because you kind of feel like you've dealt with this confounding pretty well and you might forget about the fact that it doesn't totally take care of confounding. So you might get a false sense of security. It's not as good as a randomized trial, don't, don't let your reader think it is. also, as I've mentioned, you don't really get a different answer. There's not too much benefit over traditional statistical adjustment. In other words, if you just threw all of the confounders into a regression model, ", ['groups', 'treatment', 'groups', 'stratification', 'adjustment', 'set', 'confounders', 'data', 'regression', 'model', 'variables', 'disadvantages', 'propensity', 'scores', 'randomization', 'problem', 'researchers', 'readers', 'False', 'sense', 'security', 'feel', 'confounding', 'fact', 'care', 'false', 'sense', 'security', 'trial', 'reader', 'think', 'answer', 'adjustment', 'words', 'confounders', 'regression', 'model']), 0.18786728732554484, 0.24436692987303352)
((11, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit6 Mod5 part2.srt', "The, you can see that those who did get diuretics, not surprisingly, had higher propensity scores. Those who didn't get diuretics had lower propensity scores. There's places where there's no overlap, so nobody who's in the untreated group got propensity scores higher than a certain amount. These we can't find matches for. They're going to have to be excluded. And similarly, among the untreated group, there were some who had propensity scores that were so low that there's no overlap with the treated group. Those people will be excluded as well. Then, where there's overlap, we can develop a matched cohort. So among the matched cohort, we're going to find for every no-diuretics patient, we're going to find a patient who got diuretics who has the same propensity score, and that's, those will become a matched pair in our data set and taken forth for further analysis. And finally, you may end up with a situation where there is really no overlap and that's informative too. If you plot the distributions of the propensity scores in the two groups, so ", ['diuretics', 'propensity', 'scores', 'diuretics', 'propensity', 'scores', 'places', 'overlap', 'nobody', 'group', 'propensity', 'scores', 'amount', 'matches', 'group', 'propensity', 'scores', 'overlap', 'group', 'people', 'overlap', 'cohort', 'cohort', 'no-diuretics', 'patient', 'patient', 'diuretics', 'propensity', 'score', 'pair', 'data', 'analysis', 'situation', 'overlap', 'distributions', 'propensity', 'scores', 'groups']), 0.2491364395612199, 0.23555389893019396)
((9, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit6 Mod5 part2.srt', "Those patients are going to get one treatment or the other all the time anyway. We don't really need to figure out which treatment to give them. So this is just illustrated here. A really good thing to do is actually to plot the distribution of the propensity scores in your different groups. So imagine, this is just a made up graph, imagine we've got, a group of treated subjects. Then we calculate their propensity scores. These are the ones that we know actually did get the treatment, so not surprisingly they have higher propensity scores. The propensity scores for the untreated group are given with the dotted line here. So you can see that the distributions do overlap. There are some people that are comparable, but some parts of the distributions do not overlap. So, there is a point at which none of the untreated people got propensity scores higher than a value of around, that looks like around 0.75 or 0.8. Above that propensity score, there were nobody, there was nobody in the untreated group who got a propensity score that high. That means there's a certain group of patients that are always going to ", ['patients', 'treatment', 'time', 'anyway', 'treatment', 'thing', 'distribution', 'propensity', 'scores', 'groups', 'imagine', 'made', 'graph', 'imagine', 'group', 'subjects', 'propensity', 'scores', 'ones', 'treatment', 'propensity', 'scores', 'propensity', 'scores', 'group', 'line', 'distributions', 'overlap', 'people', 'parts', 'distributions', 'point', 'none', 'people', 'propensity', 'scores', 'value', 'looks', 'propensity', 'score', 'nobody', 'nobody', 'group', 'propensity', 'score', 'group', 'patients']), 0.24655905358853653, 0.23415103008804317)
((1, '/Users/jag/Downloads/Stanford medstats/Unit 8 Module 5 part 2.srt', "And once you account for the fact that they're much older, then their, their deficient their deficiency of vitamin D is actually associated with a greater DSST score. That's a quirk, I think, of the fact that I made up the data a certain way. So that's the final model. But what's great about this, is now I can report age-adjusted means for those groups. And this is just showing the picture here. Again, this is showing the slopes in each of the three groups, for the relationship between age and DSST scores here, so that you can see that in all three groups. Certainly age is when we built this in, in the model, this is just kind of showing you the model, so age, as age goes up, you go down in your cognitive function, for sure. Interestingly, as I mentioned, though, once you account for age, the deficient group, that's this blue line, actually seems to, across all different ages, have a little bit higher of a, of a, of a cognitive score. Might be a quirk of my made up data. ", ['fact', 'deficient', 'deficiency', 'vitamin', 'D', 'DSST', 'score', 'quirk', 'fact', 'data', 'way', 'model', 'means', 'groups', 'picture', 'slopes', 'groups', 'relationship', 'age', 'DSST', 'scores', 'groups', 'model', 'showing', 'model', 'age', 'age', 'function', 'age', 'deficient', 'group', 'line', 'ages', 'bit', 'score', 'Might', 'quirk', 'made', 'data']), 0.20655911179772887, 0.229134712321412)
((9, '/Users/jag/Downloads/Stanford medstats/Unit 4 module 5 part 1.srt', "we put everything into this common standard deviation units. And the reason is that this is really nice, is that somebody went through and did all of the integrals for the standard normal curve and put them in a chart. And of course, now you can get all of this on a computer. But and so all you have to do is, if you can get back to the standard normal, then you can figure out the probabilities that are associated with all possible different, different z scores. Again, well all we're doing here, is we're taking the original curve, where the mean was 500, the standard deviation was 50. And we were talking about figuring out if we are 575, what's the area to the left of 575. Instead of figuring that out, which is trickier, we're going to convert into these Z scores, standard deviation unit scores, and say well, 575 is 1.5 standard deviations above the mean. Then we can go to a standard normal chart, where somebody has already done the integral to tell us what is the area on a standard normal to the left of 1.5. ", ['deviation', 'units', 'reason', 'somebody', 'integrals', 'curve', 'chart', 'course', 'computer', 'probabilities', 'z', 'scores', 'curve', 'deviation', 'area', 'left', 'Z', 'scores', 'deviation', 'unit', 'scores', 'deviations', 'chart', 'somebody', 'area', 'left']), 0.19364916731037085, 0.22600744111292487)
((19, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit4 Mod5.srt', "higher scores is, the woman is more likely to have some kind of eating disorder. We also looked at menstrual cycle and so we divided women into two groups. Those with a normal menstrual cycle, those are the eumenorrheic women, those are all the bars in the back of these graphics. And women with menstrual irregularities we call that the oligo and amenorrheic group, and that those are the bars in the front. So we looked at those two menstrual groups and as well as this disordered eating score and we looked at the interaction between those two things. And we did find a statistically significant interaction here. And this kind of nicely pictured in these graphics. So what's happening here is that if you look at the women who already have menstrual irregularities. If you look at the effect of eating disorder score. That doesn't seem that their bone density changes much at all across different eating disorder scores. Once you already have menstrual irregularities, it doesn't seem to be much additional effect of eating disorder score, so the bone density is pretty flat across those different eating disorder score groups. ", ['scores', 'woman', 'kind', 'disorder', 'cycle', 'women', 'groups', 'cycle', 'women', 'bars', 'back', 'graphics', 'women', 'irregularities', 'oligo', 'group', 'bars', 'front', 'groups', 'score', 'interaction', 'things', 'interaction', 'kind', 'graphics', 'women', 'irregularities', 'effect', 'disorder', 'score', 'bone', 'density', 'changes', 'across', 'eating', 'disorder', 'scores', 'irregularities', 'effect', 'disorder', 'score', 'bone', 'density', 'eating', 'disorder', 'score', 'groups']), 0.20161945963637795, 0.22555615090206751)
QUERY: *Its necessary to do univariate analysis before regression analysis, then insert variables in regression models that have significant association with outcome variable? if its true; then we do univariate analysis in regression or we can use also another statistical tests like:  t test, chi square,....? 2- For determination the interaction, its necessary that both variables have a significant relationship with outcome variable in univariate analysis?
*************************
((3, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 7 mod2.srt', "And what you can do is just simply run a univariate logistic regression that contains only one predictor at a time. So you take each of your candidate predictors, you put it in a model, one at at time, with delirium as the outcome. And you see whether or not this predictor has a significant Relationship with the outcome, if they're, if we haven't adjusted for anything. And so they assess the association between each potential predictor, and, and delirium, using this univariate regression, and then which one of these variables are we going to move on to the multivariate regression. Well in this case they said we're going to exclude anything, any of these predictors that have a p value above .15 in the univariate analysis, so if you run this regression. And P is greater than .145 and we're not going to bother with this predictor any more, we're just going to drop it now because it's not related to the [UNKNOWN]. They also excluded variables that had a prevalence below 10% so if there was a particular predictor that was a binary predictor and ", ['univariate', 'regression', 'predictor', 'time', 'candidate', 'predictors', 'model', 'time', 'delirium', 'outcome', 'predictor', 'Relationship', 'outcome', 'anything', 'association', 'potential', 'predictor', 'delirium', 'univariate', 'regression', 'variables', 'multivariate', 'regression', 'Well', 'case', 'anything', 'predictors', 'p', 'value', 'univariate', 'analysis', 'regression', 'P', 'predictor', '[', 'UNKNOWN', ']', 'variables', 'prevalence', '%', 'predictor', 'predictor']), 0.41756313817071355, 0.41482449166703855)
((0, '/Users/jag/Downloads/Stanford medstats/Unit6 Module5.srt', "Now we're ready to do some simple analysis on the data. Remember, you have two types of variables in your dataset. Continuous variables and categorical variables.  Let's look at the distributions of our continuous variables first. We can do this using proc univariate. As you can see from the code. We start out with proc univariate and then specify our data set. Then using a var statement, we can specify the variables that we want to have in our univariate analysis. We're going to analyze age, BMI, weight, and height. We then include a histogram statement, so they get a histogram for each of these variables. As you can see from the output, we still have age versus BMI by deceit status. This is carried over from the previous graphic. That's why you usually want to include G options, reset equals Alt before and after each of your graphics. Here's the output for the variable age. You can see the mean, median, mode, standard deviation, variance and range. ", ['analysis', 'data', 'Remember', 'types', 'variables', 'dataset', 'variables', 'variables', 'Let', 'look', 'distributions', 'variables', 'proc', 'univariate', 'code', 'proc', 'univariate', 'data', 'var', 'statement', 'variables', 'univariate', 'analysis', 'age', 'BMI', 'histogram', 'statement', 'histogram', 'variables', 'output', 'age', 'versus', 'BMI', 'deceit', 'status', 'G', 'options', 'equals', 'Alt', 'graphics', 'output', 'age', 'mode', 'deviation', 'variance', 'range']), 0.3561567943220792, 0.34321601034202104)
((0, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit3 Mod1 pt1.srt', "In this next module, we're going to dive right in to the logistic regression model. We've talked about, when you have binary or categorical outcomes that, that is when you're looking at proportions, we've talked about some statistical tests, when you don't want to adjust for anything that you can use. So this is like the risk difference, relative risk, the chi square test, the McNemar's chi-square test. So we talked about those, but what about when you want to do a regression analysis when you have a binary or categorical outcome? In that case, you're going to be using logistic regression, which we're going to talk about in this module. I want to just give you a little overview of logistic regression. So to remind you, when we did linear regression, the model looked like the following. We were predicting the value of some continuous outcome variable, call it y, as a function of x. Here's the simple linear regression model. We have the intercept alpha, and beta is our slope. ", ['module', 'regression', 'model', 'outcomes', 'proportions', 'tests', 'anything', 'risk', 'difference', 'risk', 'chi', 'square', 'test', 'McNemar', 'test', 'regression', 'analysis', 'outcome', 'case', 'regression', 'module', 'overview', 'regression', 'regression', 'model', 'value', 'outcome', 'call', 'function', 'x', 'linear', 'regression', 'model', 'intercept', 'alpha', 'beta', 'slope']), 0.42723075247394027, 0.2873397984255384)
((4, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit5 Unit 4 part1 retake.srt', "There's a lot of opportunities for observations to get kicked out. If you were putting more than 11 variables, which is very typical, you can imagine that a little bit of missing data here and there can result in, say you start with a sample size of 500, that suddenly might turn out to go down to a sample size of 200. In other words, your analysis might only run on 200 of your observations even if you started with 500. So you've got to be very careful and make sure that you always fill in your missing data in some way. The other thing that you're going to need to do is you need to standardize your variables. We already talked about that. They all have to have a mean of zero and a standard deviation of one. One other thing I just want to point out now, is that principal components analysis typically run numeric variables. So, we saw in the last module that principal components analysis is based on the linear relationship. That is, the linear correlation between pairs of variables. So technically, all the assumptions of linear regression would apply here. ", ['lot', 'opportunities', 'observations', 'variables', 'bit', 'data', 'sample', 'size', 'sample', 'size', 'words', 'analysis', 'observations', 'missing', 'data', 'way', 'thing', 'variables', 'zero', 'deviation', 'thing', 'point', 'components', 'analysis', 'variables', 'module', 'principal', 'components', 'analysis', 'linear', 'relationship', 'linear', 'correlation', 'pairs', 'variables', 'assumptions', 'regression']), 0.35836865605893176, 0.2840356036909532)
((2, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit6 Mod2.srt', "So first of all imagine that you've got some variables in your dataset that aren't necessarily that important you maybe there's there potential confounders and you've collected that data but its not your primary predictor of interest, it's not your primary outcome of interest. But those variables are missing a huge amount of data. 40, 50, 60% of the observations in your data set don't have a value for that variable. The first thing you might consider doing is just not including that variable in your analysis. Especially if you've measured the same construct with other variables. If there's other variables available to get at it. It's worth considering, is there enough information here? Is it really even worth it for me? To use that variable. If I use that variable, I'm going to have to impute those values. That adds a lot of noise so, I might just consider not even using that at all in my analysis. Now consider the situation where you're doing some kind of observational study, and you have a particular hypothesis that you're trying to test. ", ['imagine', 'variables', 'dataset', 'confounders', 'data', 'predictor', 'interest', 'outcome', 'interest', 'variables', 'amount', 'data', '%', 'observations', 'data', 'value', 'thing', 'analysis', 'construct', 'variables', 'variables', 'considering', 'information', 'values', 'lot', 'noise', 'analysis', 'situation', 'kind', 'study', 'hypothesis', 'test']), 0.32070079957649444, 0.2813982777680281)
((7, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit6 Mod2.srt', "a variable. And that variable's not one of the most important variables, but you want to include it in your model. Just filling in with something is better than not filling in, because that's going to mean that this observation gets included in your regression analysis when it otherwise would've been thrown out. So filling in with the mean is often good enough. And it's not the best thing that you can do, there are other potentially better alternatives but it's very simple and easy to do and it's a lot better than not filling in with anything and just having that observation kicked out. So oftentimes if I've got variables where, you know, one or two people are missing the value and I want to include it in my model and it's not the most important variable I might just fill in with mean. Slightly better strategy, little bit more work, is to use linear regression to predict the missing values. So remember we've got a dataset that may have other variables that are related to height in it. So, we might have weight on the person if, you know, they're young. ", ['variables', 'model', 'something', 'observation', 'regression', 'analysis', 'filling', 'thing', 'alternatives', 'lot', 'better', 'anything', 'observation', 'oftentimes', 'variables', 'people', 'value', 'model', 'strategy', 'bit', 'work', 'regression', 'missing', 'values', 'remember', 'dataset', 'variables', 'height', 'person']), 0.3099355376904239, 0.280059244901887)
((1, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 7 mod3 take2.srt', "like principal components analysis. But somehow you're going to get your variables down to a more manageable set. And then there's a lot of different ways we can approach model building, and everybody has slightly different preferences. So, you know, it's really great to read the literature and see what different people do, but I'm going to give you some general strategies that are typically used just to get you started. And so you might choose to do your prediction modeling with some kind of manual selection, as opposed to an automated model building procedure. So, you know, a typical thing that people might do is they might do, start with some kind of univariate screen. So you might say, well, all the p-values we saw in the delirium example, that if the p-values were greater than 0.15, we, in univariate models, we're just going to eliminate those variables. So some way to, again, get that candidate set of predictors down to a smaller set, before we do any further modeling. So that's very typical, you probably want to also use some clinical judgment there, about what variables to move forward. ", ['principal', 'components', 'analysis', 'variables', 'set', 'lot', 'ways', 'model', 'building', 'everybody', 'preferences', 'literature', 'people', 'strategies', 'prediction', 'kind', 'selection', 'model', 'building', 'procedure', 'thing', 'people', 'kind', 'screen', 'p-values', 'delirium', 'example', 'p-values', 'univariate', 'models', 'variables', 'way', 'candidate', 'set', 'predictors', 'set', 'modeling', 'judgment', 'variables']), 0.22233428588149945, 0.27141144592103444)
((17, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 5 Module 1 Part 2.srt', "And you can see how off the Kaplan\xe2\x80\x93Meier estimates can be, as is illustrated here. Now what about the multivariate situation? I basically just set you up for the univariate, the unadjusted situation, we just want to look at basic Kaplan-Meiers. When we get to the multivariate case, it kind of depends on what the point of your analysis is. If you care about absolute probability, you are going to get things wrong if you treat the competing risks as censored values. However, there's a lot of situations where we're doing some kind of explanatory analysis, and we don't really care about the absolute probabilities. We just want to compare, say the treatment group to the control group. It turns out that it's probably fine if the absolute probabilities don't matter. To just treat the competing events as censored values in a Cox regression. And the hazard ratios will probably be reasonable. But, keep in mind that if you're doing some kind of predictive or prognostic study and you're trying to actually get those absolute probabilities, treating the competing events as censored will give you the wrong probability. ", ['multivariate', 'situation', 'univariate', 'situation', 'Kaplan-Meiers', 'multivariate', 'case', 'depends', 'point', 'analysis', 'probability', 'things', 'risks', 'values', 'lot', 'situations', 'kind', 'analysis', 'probabilities', 'treatment', 'group', 'control', 'group', 'fine', 'probabilities', 'events', 'values', 'Cox', 'regression', 'hazard', 'ratios', 'mind', 'kind', 'study', 'probabilities', 'events', 'probability']), 0.22607125827950406, 0.2697475258614774)
QUERY: Anti-inflammatory drugs may prevent the Alzheimer's Disease  - I'm really not sure about this one:  p53 plays a role against malignant transformation and in developmental processes as aging, differentiation and fertility.
*************************
((6, '/Users/jag/Downloads/Stanford medstats/Kaplan Meier Curves 1 How to Draw Them.srt', "Some censoring events. Now of course less the follow up and so on they tend to get lost to follow up as you get further and further out because say this one, this one wasn't really lost to fall up. This one may have, I mentioned very quickly that this one enrolled three and a half months ago. This one moved to England, okay? This patient moved, she had pancreatic cancer and wanted to move to be closer to her family. She had, she had children who had moved to England and she wanted to move there too. But because we saw her at this point but never saw her again and really, she, she, she was in, no longer in any communication and we don't know what happened to her. ", ['events', 'course', 'follow', 'say', 'one', 'half', 'months', 'one', 'England', 'okay', 'patient', 'cancer', 'family', 'children', 'England', 'point', 'longer', 'communication']), 0.11396057645963795, 0.09647601436381104)
((0, '/Users/jag/Downloads/Stanford medstats/Unit 10 Conclusion.srt', "[BLANK_AUDIO] Okay, now you've done a root cause analysis. If you figure out what the system process issues, you have implemented solutions. Now how do you, how do you roll things out. And then, double check to make sure things are maintained or sustained. Some implementations of how you think about permanent changes to the way work is done. And as such involves building the change into the organization. Hard wiring is a phrase we often time use because you're talking about changing behavior. You're talking about changing process. You're talking about changing culture sometimes in order to make sure that the bad event never occurs again. Sustaining is making sure you lock in those processes that the organization's have already have put into place, and then continuing building on it. ", ['[', 'BLANK_AUDIO', ']', 'Okay', 'root', 'cause', 'analysis', 'system', 'process', 'issues', 'solutions', 'things', 'check', 'things', 'implementations', 'changes', 'way', 'work', 'involves', 'change', 'organization', 'Hard', 'wiring', 'phrase', 'time', 'use', 'behavior', 'process', 'culture', 'order', 'event', 'sure', 'processes', 'organization', 'place', 'building']), 0.08247860988423225, 0.08247860988423225)
((4, '/Users/jag/Downloads/Stanford medstats/Geriatric Anesthesia - 3. Pharmacology: Discussion.srt', "and when you pour it in your concentration goes higher. And that's actually a common physiologic finding. That elderly patients have smaller volumes of distribution and thus they tend to have slightly higher levels. So its those two processes, end organ sensitivity and the kinetics, primarily around volume of distribution, that result in recurring of overdose.  Does clearance of medication have an effect as well on how the medication should be dosed?  Absolutely! Clearance is, is really a fundamental concept. However, what's interesting about clearance is, where clearance is very important are, is drugs people take, every single day. Because their steady state levels, will be determined by clearance, and only clearance. Since our drugs are so rarely at steady state, when we give our drugs, particularly our induction drugs, clearance hasn't had much chance to operate. Now the clearance of the elderly is reduced a little bit, but not that much because hepatic blood flow, for example, which is very important for [UNKNOWN] clearance, isn't reduced very much in the elderly. ", ['concentration', 'physiologic', 'finding', 'volumes', 'distribution', 'levels', 'processes', 'end', 'sensitivity', 'kinetics', 'volume', 'distribution', 'result', 'recurring', 'overdose', 'clearance', 'medication', 'effect', 'medication', 'Clearance', 'concept', 'clearance', 'clearance', 'drugs', 'people', 'day', 'state', 'levels', 'clearance', 'clearance', 'drugs', 'state', 'drugs', 'induction', 'drugs', 'clearance', 'chance', 'clearance', 'bit', 'blood', 'flow', 'example', '[', 'UNKNOWN', ']', 'clearance']), 0.12098347962395682, 0.08193713496785364)
((25, '/Users/jag/Downloads/Stanford medstats/BWH Module 7   HD 1080p.srt', "then recheck with our surgeons. If the surgeons are happy, they will tell us to continue to full dose Protamine. Again, this is very important and very institution-specific, so please check with your attending, your surgeons, and your perfusionists for how this is normally done at your institution. Excellent communication regarding both heparinization and Protamine is absolutely essential as mistakes in these drugs can be catastrophic. So make sure you know very clearly what your surgeons, and what your perfusionist, and what your attending wants before you do any of these, do anything with these drugs. Once you're sure exactly what the surgeon the profusionist would like you to do with the Protamine, you can continue to reverse the anticoagulation with full dose Protamine. After administering the full dose of Protamine, in approximately three minutes, you should obtain another ACT, activated clotting time, and give this to the profusionist. ", ['surgeons', 'surgeons', 'dose', 'Protamine', 'check', 'attending', 'surgeons', 'perfusionists', 'institution', 'Excellent', 'communication', 'heparinization', 'Protamine', 'mistakes', 'drugs', 'make', 'sure', 'surgeons', 'perfusionist', 'attending', 'wants', 'anything', 'drugs', 'surgeon', 'profusionist', 'Protamine', 'anticoagulation', 'dose', 'Protamine', 'dose', 'Protamine', 'minutes', 'ACT', 'time', 'profusionist']), 0.09020771689863956, 0.08098088614344029)
((15, '/Users/jag/Downloads/Stanford medstats/SciWrite Demo Edit 7.6.srt', "what we can pharmalogical options, these aren't all, you know, drugs you would take by mouth, so as a pharmalogical options include, and here's, the list of them, intra, intraligamen, I can't even pronounce that word, that's just anaesthetics that you take. That are ejected into the teeth, so we could just say, local anesthetics, or anesthetic injections. I'm not sure what computer controlled injections are, and how that differs from anesthetic injections, so I think I'll just get rid of that for now. sedation, so anesthetic injections, sedation with nitrous oxide anti-anxiety drugs, which I took away from, I had mit, took that out of an earlier place, I'm going to put that back. So anti-anxiety drugs and then finally general anaesthesia is the other one, right, I mean, general anesthesia would be, would put somebody completely under. So that's another option. ", ['options', 'drugs', 'mouth', 'options', 'list', 'intraligamen', 'word', 'anaesthetics', 'teeth', 'anesthetics', 'injections', 'computer', 'injections', 'differs', 'injections', 'sedation', 'injections', 'sedation', 'oxide', 'anti-anxiety', 'drugs', 'place', 'drugs', 'anaesthesia', 'anesthesia', 'somebody', 'option']), 0.11454053224818188, 0.08012538950887348)
((2, '/Users/jag/Downloads/Stanford medstats/Geriatric Anesthesia - 3. Pharmacology: Discussion.srt', "Because they are typically on, on multiple drugs. So the bottom line is, [LAUGH] they'll need less. You can pretty well assume that. But how much less is really a function of the polypharmacy, the comorbidities, and requires very careful assessment.  Okay. Are there some specific causes that, or changes in geriatric physiology that lead to these differences in dosing requirements for geriatric patients?  Well, my perspective is that of a clinical pharmacologist. Where I study the drugs, and I really ask two questions, what is the end organ sensitivity? Of course, in anesthesia the end organ is typically the brain. And what do the pharmacokinetics look like? Pharmacokinetics basically says how high do the blood levels go for any given dosage you might give. And what we see is certain categories of drugs, different behaviors in the elderly patients for the opioids. ", ['multiple', 'drugs', 'bottom', 'line', 'LAUGH', 'less', 'function', 'polypharmacy', 'comorbidities', 'assessment', 'Okay', 'Are', 'causes', 'changes', 'physiology', 'lead', 'differences', 'dosing', 'requirements', 'patients', 'Well', 'perspective', 'pharmacologist', 'drugs', 'questions', 'end', 'sensitivity', 'course', 'anesthesia', 'end', 'brain', 'pharmacokinetics', 'Pharmacokinetics', 'blood', 'levels', 'dosage', 'categories', 'drugs', 'behaviors', 'patients', 'opioids']), 0.11227217828476796, 0.07853859100798366)
((1, '/Users/jag/Downloads/Stanford medstats/Geriatric Anesthesia - 3. Pharmacology: Discussion.srt', "Elderly people need less drug. And we can talk and we will talk a little bit about why that is the case. That's the general rule. They're going to need less drug. The key thing though in terms of assessing the patient and figuring out what dose to use is to establish, not just was is the physical age, but what is the physiologic age of the patient? Some elderly patients are, in fact, very, very robust. And they would need drug doses similar to what you or I might need. A lot of elderly, though, are on multiple medications. They have multiple comorbidities. And these have to be taken into account, when figuring out what drugs to give. Additionally because the elderly are typically on many drugs, you have to look for the possibility of interactions with drugs. So the elderly patient requires a very careful pre-operative assessment of exactly where they are in terms of their physiologic age, not just their physical age, and also of the many, many drugs that they're on, what kind of interactions might affect your dosing. ", ['people', 'drug', 'bit', 'case', 'rule', 'drug', 'thing', 'terms', 'patient', 'age', 'age', 'patient', 'fact', 'drug', 'doses', 'lot', 'multiple', 'medications', 'comorbidities', 'account', 'drugs', 'drugs', 'possibility', 'interactions', 'drugs', 'patient', 'assessment', 'terms', 'age', 'age', 'drugs', 'kind', 'interactions', 'dosing']), 0.12427395320024001, 0.07413855843895248)
((5, '/Users/jag/Downloads/Stanford medstats/Unit 7 Homework with Answers.srt', "You can do that by, calculating the other tail which is the probability that x equals 1 and the probability that x equals It turns out that those are exactly the same as the ones that you see here. So this one comes out to be .015625. This one comes out to be .09375. This one will also be 0.09375 and this one will 0.019625 when you add all of those up, You get the P value here is 0.21875. These is a situation where we have carried the data, we have repeated measures. It's the same women measured before and after they smoked. We have a continuous outcome which is blood pressure and also heart rate, and, so we are going to be using a paired t test here because it's a before and after with a continuous outcome. The correct answer here is false. ", ['tail', 'probability', 'x', 'equals', 'probability', 'x', 'equals', 'ones', 'one', 'one', 'You', 'get', 'P', 'value', 'situation', 'data', 'measures', 'women', 'outcome', 'blood', 'pressure', 'heart', 'rate', 't', 'test', 'outcome', 'correct', 'answer']), 0.08671099695241201, 0.0734072400067592)
QUERY: I would write \Anti-inflammatory drugs may prevent Alzheimer's Disease. Is that correct?
*************************
((15, '/Users/jag/Downloads/Stanford medstats/SciWrite Demo Edit 7.6.srt', "what we can pharmalogical options, these aren't all, you know, drugs you would take by mouth, so as a pharmalogical options include, and here's, the list of them, intra, intraligamen, I can't even pronounce that word, that's just anaesthetics that you take. That are ejected into the teeth, so we could just say, local anesthetics, or anesthetic injections. I'm not sure what computer controlled injections are, and how that differs from anesthetic injections, so I think I'll just get rid of that for now. sedation, so anesthetic injections, sedation with nitrous oxide anti-anxiety drugs, which I took away from, I had mit, took that out of an earlier place, I'm going to put that back. So anti-anxiety drugs and then finally general anaesthesia is the other one, right, I mean, general anesthesia would be, would put somebody completely under. So that's another option. ", ['options', 'drugs', 'mouth', 'options', 'list', 'intraligamen', 'word', 'anaesthetics', 'teeth', 'anesthetics', 'injections', 'computer', 'injections', 'differs', 'injections', 'sedation', 'injections', 'sedation', 'oxide', 'anti-anxiety', 'drugs', 'place', 'drugs', 'anaesthesia', 'anesthesia', 'somebody', 'option']), 0.19166296949998196, 0.13407542102509443)
((1, '/Users/jag/Downloads/Stanford medstats/Unit 8 Homework with Answers.srt', "Just kind of eyeballing this graphic here. Which Pearson's coefficient looks the most reasonable? So actually, it's D, negative 0.3. You can see that's there an awful lot of scatter around this line. This is really not a high correlation at all. So the correct answer is negative 0.3, it's negative certainly, because there our line is going down. But a negative 0.5 is actually too big here. So negative 0.3 is the correct answer. There is a little bit of correlation. So it's not 0.01 here. But it's, it's very weak. So it's point, negative 0.3. The correct answer to this is, is B. Actually, linear regression is probably not totally appropriate here. We have a fairly small sample size, and we're violating probably two of the assumptions of linear regression. So you can see that we have a lot of zeroes in terms of the alcohol exposure time in the movies. ", ['Pearson', 'coefficient', 'looks', 'D', 'lot', 'scatter', 'line', 'correlation', 'correct', 'answer', 'line', 'correct', 'answer', 'bit', 'correlation', 'point', 'correct', 'answer', 'B', 'regression', 'sample', 'assumptions', 'regression', 'lot', 'zeroes', 'terms', 'alcohol', 'exposure', 'time', 'movies']), 0.18973665961010275, 0.13272789515619995)
((2, '/Users/jag/Downloads/Stanford medstats/Geriatric Anesthesia - 3. Pharmacology: Discussion.srt', "Because they are typically on, on multiple drugs. So the bottom line is, [LAUGH] they'll need less. You can pretty well assume that. But how much less is really a function of the polypharmacy, the comorbidities, and requires very careful assessment.  Okay. Are there some specific causes that, or changes in geriatric physiology that lead to these differences in dosing requirements for geriatric patients?  Well, my perspective is that of a clinical pharmacologist. Where I study the drugs, and I really ask two questions, what is the end organ sensitivity? Of course, in anesthesia the end organ is typically the brain. And what do the pharmacokinetics look like? Pharmacokinetics basically says how high do the blood levels go for any given dosage you might give. And what we see is certain categories of drugs, different behaviors in the elderly patients for the opioids. ", ['multiple', 'drugs', 'bottom', 'line', 'LAUGH', 'less', 'function', 'polypharmacy', 'comorbidities', 'assessment', 'Okay', 'Are', 'causes', 'changes', 'physiology', 'lead', 'differences', 'dosing', 'requirements', 'patients', 'Well', 'perspective', 'pharmacologist', 'drugs', 'questions', 'end', 'sensitivity', 'course', 'anesthesia', 'end', 'brain', 'pharmacokinetics', 'Pharmacokinetics', 'blood', 'levels', 'dosage', 'categories', 'drugs', 'behaviors', 'patients', 'opioids']), 0.18786728732554484, 0.13142019927337703)
((5, '/Users/jag/Downloads/Stanford medstats/Unit 9 Homework with Answers.srt', "then dividing by 1 plus e raised to the predicted logit. If you calculate that all that out, you get a a percentage, a predicted probability of 54% so the correct answer here is e. In including continuous predictors in the model, the authors are assuming that these predictors have a linear relationship with our outcome here, which is the logit of death. So the correct answer here is b. The correct answer here is a. Our hazard ratio is 1.35 so that indicates a 35% increase in the rate of dementia after accounting for confounders. And it is statistically significant, because that confidence interval does not cross one, it goes from 1.14 to 1.60, so that is statistically significant. So, how do we interpret these hazard ratios for the underweight for women? So, what you'll notice is that they are all elevated by similar amounts as ", ['e', 'logit', 'percentage', 'probability', '%', 'correct', 'answer', 'e.', 'In', 'predictors', 'model', 'authors', 'predictors', 'relationship', 'outcome', 'logit', 'death', 'correct', 'answer', 'b', 'correct', 'answer', 'hazard', 'ratio', 'indicates', '%', 'increase', 'rate', 'dementia', 'confounders', 'confidence', 'interval', 'hazard', 'ratios', 'underweight', 'women', 'amounts']), 0.1777046633277277, 0.12431106340439953)
((1, '/Users/jag/Downloads/Stanford medstats/Geriatric Anesthesia - 3. Pharmacology: Discussion.srt', "Elderly people need less drug. And we can talk and we will talk a little bit about why that is the case. That's the general rule. They're going to need less drug. The key thing though in terms of assessing the patient and figuring out what dose to use is to establish, not just was is the physical age, but what is the physiologic age of the patient? Some elderly patients are, in fact, very, very robust. And they would need drug doses similar to what you or I might need. A lot of elderly, though, are on multiple medications. They have multiple comorbidities. And these have to be taken into account, when figuring out what drugs to give. Additionally because the elderly are typically on many drugs, you have to look for the possibility of interactions with drugs. So the elderly patient requires a very careful pre-operative assessment of exactly where they are in terms of their physiologic age, not just their physical age, and also of the many, many drugs that they're on, what kind of interactions might affect your dosing. ", ['people', 'drug', 'bit', 'case', 'rule', 'drug', 'thing', 'terms', 'patient', 'age', 'age', 'patient', 'fact', 'drug', 'doses', 'lot', 'multiple', 'medications', 'comorbidities', 'account', 'drugs', 'drugs', 'possibility', 'interactions', 'drugs', 'patient', 'assessment', 'terms', 'age', 'age', 'drugs', 'kind', 'interactions', 'dosing']), 0.2079500979640145, 0.12405753654146418)
((10, '/Users/jag/Downloads/Stanford medstats/SciWrite Demo Edit 7.6.srt', "coffee, warm colors of the place, we don't need of the place, just warm colors. Pleasant music, or beautiful paintings on the walls. So those are all the examples, and then, getting back the dash here. And then we get our decisive from the psychological perspective of the patient and can be preferable than the use of anti, these are anxialytics or anti anxiety drugs. I think we can say there's a little bit more crisply, so maybe just our work as well, work as well as anti-anxiety drugs, for many patients, I don't know if that's true, I'm assuming that this study kind of talked about the, the effectiveness of these, if we, if they, if that's even been ever researched. So I'm just kind of guessing here, but maybe they work as well as anti-anxiety drugs for many patients or maybe patients prefer them or it, give something about exactly what the study tells us about the effectiveness of this, of these stimuli. Then we get a list of drugs, I, since I'm going to make this paragraph about the simple non-drug interventions, I'm going to move the drug list down to a lower paragraph. ", ['coffee', 'warm', 'colors', 'place', 'place', 'warm', 'colors', 'Pleasant', 'music', 'paintings', 'walls', 'examples', 'dash', 'perspective', 'patient', 'use', 'anti', 'anxialytics', 'anxiety', 'drugs', 'bit', 'work', 'work', 'drugs', 'patients', 'study', 'kind', 'effectiveness', 'guessing', 'drugs', 'patients', 'patients', 'something', 'study', 'effectiveness', 'stimuli', 'list', 'drugs', 'paragraph', 'non-drug', 'interventions', 'drug', 'list', 'paragraph']), 0.2025478734167333, 0.12083471204779403)
((12, '/Users/jag/Downloads/Stanford medstats/Geriatric Anesthesia - 3. Pharmacology: Discussion.srt', 'First thing to realize is that elderly patients are on a spectrum, from the very healthy elderly. Where the dosing requirements may not be very different from you and me. To the elderly that are very debilitated, on multiple medications, where the dosing has to be very carefully given. The one thing you can always say is that the elderly less drug than people who are at sort of the middle age of life. But how much less is very variable. So the first thing is assessment. You have to assess the patient very, very carefully to know where they are on the spectrum. The next thing to understand is that there are different aspects to why elderly need less drugs. Some of the elderly are more sensitive because their brains are more sensitive, for example, in the case of opioids. Where the dose needs to be reduced by about 50%. However, some of the elderly are more sensitive to drugs because their pharmical kinetics are different. The drugs reach a higher concentration in the blood. ', ['First', 'thing', 'patients', 'spectrum', 'dosing', 'requirements', 'multiple', 'medications', 'dosing', 'thing', 'less', 'drug', 'people', 'sort', 'middle', 'age', 'life', 'less', 'thing', 'assessment', 'assess', 'patient', 'spectrum', 'thing', 'aspects', 'drugs', 'brains', 'example', 'case', 'opioids', 'dose', '%', 'drugs', 'kinetics', 'drugs', 'concentration', 'blood']), 0.17177950029416048, 0.12016619008619747)
((16, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 5 Module 1 Part 2.srt', "So if you compare that to the, the cumulative incidence that we calculated the proper way, you'll see that that's, that, you know, the proper curve is going to be lower than that. So that would be the wrong way to do it and it shows you why you get the different estimates and why the cumulative incidence is the correct way to do it. And just one more example, this was another paperwork Somebody was kind of illustrating this point. So they're showing you here the Kaplan-Meier method. The incorrect method versus the correct cumulative incidence method. And this is the people who had kidney disease. They could either die while on dialysis or they could get a transplant and then they weren't at risk of dying from dialysis, or they could survive this is the event for your survival meeting. Neither of those things happened. And you can see that the Kaplan\xe2\x80\x93Meiers are clearly wrong, because, those three things have to add up to a 100, you know, 100% and two of the Kaplan\xe2\x80\x93Meiers add up to more than a 100%. Well, obviously we can't have higher than 100% here. So the correct analysis is the cumulative incidence. ", ['incidence', 'proper', 'way', 'proper', 'curve', 'way', 'estimates', 'incidence', 'correct', 'way', 'paperwork', 'Somebody', 'illustrating', 'point', 'method', 'incorrect', 'method', 'correct', 'incidence', 'method', 'people', 'kidney', 'disease', 'dialysis', 'transplant', 'risk', 'dialysis', 'event', 'survival', 'meeting', 'things', 'Well', '%', 'correct', 'analysis', 'incidence']), 0.16035674514745463, 0.11217554531242287)
QUERY: 
*************************
((0, '/Users/jag/Downloads/Stanford medstats/Basic Math Review.srt', "This module will briefly review some of the mathematical concepts that will help you understand the topics covered in this statistics course. In this video, we'll be focusing on five concepts, factorials and summations, the equation of a line, and the exponential function, as well as the natural logarithm. So, jumping right into factorials. Factorials are denoted with an exclamation point as shown here. The formal definition for a factorial, and I'll repeat this, is that it's the product of all positive integers less than or equal to the number. Again, it's the product of all positive integers less than or equal to this number. Now, if I say that in another way that's like saying that when you take the factorial of a number, you're multiplying all the positive whole numbers between that number and the number So, let me illustrate that with an example. ", ['module', 'concepts', 'topics', 'statistics', 'video', 'concepts', 'factorials', 'summations', 'equation', 'line', 'function', 'logarithm', 'factorials', 'Factorials', 'exclamation', 'point', 'shown', 'definition', 'product', 'integers', 'number', 'product', 'integers', 'number', 'way', 'number', 'whole', 'numbers', 'number', 'number', 'example']), 0.0, 0.0)
((1, '/Users/jag/Downloads/Stanford medstats/Basic Math Review.srt', "3 factorial, whats 3 factorial? It's 3 times 2 times 1, and that's equal to 6. Okay, the key takeaway here is that 3 factorial is simply a number, 3 factorial is equal to 6. Let's go over another example. What's 6 factorial? So, we're going to be multiplying all the numbers between 6 and 1. So, that means 6 factorial is going to equal 6 times 5 times 4 times 3 times 2 times 1. And we're going to do a little bit of Math here. And we're going to figure out that 6 factorial is equal to 720. Again, the big takeaway here, being that 6 factorial is simply a number. It's equal to 720. It's you can think about it as if it's another way of writing 720. One thing to know about factorials is that can only take the factorial of a positive integer. ", ['whats', 'times', 'times', 'Okay', 'takeaway', 'number', 'Let', 'go', 'example', 'numbers', 'means', 'times', 'times', 'times', 'times', 'times', 'bit', 'Math', 'takeaway', 'number', 'way', 'thing', 'factorials', 'integer']), 0.0, 0.0)
((2, '/Users/jag/Downloads/Stanford medstats/Basic Math Review.srt', "You can't take factorials of negative numbers and get a real solution. One thing you'll have to remember about factorials is what 0 factorial equals. 0 factorial equals 1. Again 0 factorial is defined as 1. Before I move on to summations one last thing I want to go over with factorials is a trick for how to simplify them. Let's say you come across a problem 13 factorial divided by 10 factorial. Because you know the definition of the factorial, you're thinking to yourself, gosh these are going to be really large numbers. I'm going to have to multiply 13 times 12 times 11 times 10, all the way down to 1. Do that again on the bottom. Well, you can also rewrite this top part. But instead of going all the way down to 1, you can stop here and finish off this expression with 10 factorial. ", ['factorials', 'numbers', 'solution', 'thing', 'factorials', 'equals', 'equals', 'summations', 'thing', 'factorials', 'trick', 'Let', 'say', 'problem', 'definition', 'numbers', 'times', 'times', 'times', 'way', 'Do', 'bottom', 'Well', 'part', 'way', 'expression']), 0.0, 0.0)
((3, '/Users/jag/Downloads/Stanford medstats/Basic Math Review.srt', "And the reason I'm going to do that is so that I can cancel out the same expression on the top and bottom of this fraction. Remember, I can do this because 10 factorial is a number. I don't know what number it is, off the top of my head. It's a very large number, but I can cancel that. And that leaves me with something very manageable. 13 time 12 times 11, which happens to equal 1716. So, this is just a trick for simplifying factorials if you ever come across a problem like this. Summations are expressed with the capital Greek letter sigma, okay? And whenever you see a summation symbol, I want you to focus on these three areas, okay? This first part on the bottom of the sigma, it represents the lower bound of the summation. This top part, number 2 represents the upper bound, very intuitive. And then this third part, on the side is the expression that you're going to be summing over. And let me illustrate what I mean by that with an example. ", ['reason', 'expression', 'bottom', 'fraction', 'Remember', 'number', 'number', 'head', 'number', 'something', 'time', 'times', 'happens', 'trick', 'simplifying', 'factorials', 'problem', 'Summations', 'capital', 'Greek', 'letter', 'sigma', 'okay', 'summation', 'symbol', 'areas', 'okay', 'part', 'bottom', 'sigma', 'bound', 'summation', 'part', 'number', 'represents', 'bound', 'part', 'side', 'expression', 'example']), 0.0, 0.0)
((4, '/Users/jag/Downloads/Stanford medstats/Basic Math Review.srt', "Okay? So lets say your index variable right here. Your lower bound is going to be one. You're going to be going all the way to 6 ,and we're going to be iterating over i. What does that mean? Well, it means we're going to start at 1. And every time we add a term to this expression, we're going to iterate i by 1 until we reach 6. So, that means 1 plus 2 plus 3 plus 4 plus 5 plus 6. Okay? And that equals 21. Okay? Just like with factorials, this complicated looking expression is just a number. It's another way of writing 21. Another key point here is to realize that I'm adding all of these because these are all equal to i. And that's the expression that we're summing over. Okay, let me illustrate this with another example. So, very similar to the last one, except we're going to be summing over 7 i. ", ['Okay', 'lets', 'index', 'right', 'bound', 'way', 'Well', 'time', 'term', 'expression', 'means', 'plus', 'plus', 'plus', 'plus', 'plus', 'Okay', 'equals', 'Okay', 'factorials', 'looking', 'expression', 'number', 'way', 'point', 'expression', 'Okay', 'example']), 0.0, 0.0)
((5, '/Users/jag/Downloads/Stanford medstats/Basic Math Review.srt', "Okay, so this is going to look like this. We're going to start at the lower bound, okay, and end when we reach the upper bound. Whatever that definition is. In this case, it's still 1 and 6. So, what we're going to do here is multiply 7 times 1. 7 times 2. Okay. All the way until we reach 7 times 6. And you might realize that you can take out the 7 from this expression, okay? To give yourself something that looks like this. 7 times the quantity 1 plus 2 plus 3 plus 4 plus 5 plus 6. Of course, and because we did this last problem, we know that, that's 21. So, this expression right here results in 7 times 21, okay. So, let me rate, rewrite that in another way. ", ['Okay', 'bound', 'okay', 'end', 'bound', 'definition', 'case', 'times', 'times', 'Okay', 'way', 'times', 'expression', 'okay', 'something', 'times', 'quantity', 'plus', 'plus', 'plus', 'plus', 'course', 'problem', 'expression', 'results', 'times', 'okay', 'rate', 'way']), 0.0, 0.0)
((6, '/Users/jag/Downloads/Stanford medstats/Basic Math Review.srt', "So, that results in 7 times this summation. If you'll look at these two carefully, what you'll realize is that all I did was move this 7 to the outside. Okay? This illustrates one property of summations, there are multiple properties. I won't go into them in this video. But I just wanted to give you a taste of what knowing these properties can do and how they can help you simplify much more complicated expressions. One more example with summations. Now, let's imagine that you have some data. Okay, let's imagine that you have five data points labeled x 1 to x 5, okay? And let's imagine that these values are 2, 4, 6, 8, and 10. Okay, we call this we say that these data, this data is indexed. And what that means is that this first point, okay, corresponds to x sub 1. ", ['results', 'times', 'summation', 'outside', 'Okay', 'property', 'summations', 'properties', 'video', 'taste', 'properties', 'expressions', 'example', 'summations', 'imagine', 'data', 'Okay', 'imagine', 'data', 'points', 'x', 'okay', 'imagine', 'values', 'Okay', 'data', 'data', 'point', 'okay', 'corresponds', 'sub']), 0.0, 0.0)
((7, '/Users/jag/Downloads/Stanford medstats/Basic Math Review.srt', "Second point corresponds to x sub 2, etc. And each of these points corresponds to an actual value in the data set. So, I can use the summation to sum either part or this whole set of numbers. Okay, and let me illustrate that. Lets say with something like this. Okay, so what is this expression going to equal? Well, if I write it out, this is going to be x sub 3 plus x sub 4 plus x sub 5, which corresponds to 6 plus 8 plus 10, which is 24, okay? So, this real illustrates a more practical example of how you can think about summations. And how they can be helpful to you. When thing about large sets of data. Probably the most common form that you'll encounter when you're looking at the equation of a line will be slope intercept form, ", ['point', 'corresponds', 'sub', 'etc', 'points', 'corresponds', 'value', 'data', 'set', 'summation', 'sum', 'part', 'set', 'numbers', 'Okay', 'Lets', 'something', 'Okay', 'expression', 'Well', 'x', 'sub', 'x', 'sub', 'x', 'sub', 'corresponds', 'plus', 'okay', 'illustrates', 'example', 'summations', 'thing', 'sets', 'data', 'form', 'equation', 'line', 'slope', 'form']), 0.0, 0.0)
QUERY: In my anatomy class I was also taught to distinguish between newborn and neonate. Perhaps, someone can clarify and enlighten us!"
*************************
((3, '/Users/jag/Downloads/Stanford medstats/Unit 1 Homework Solutions with questions.srt', "And, so what I'm going to do is write out the first few expressions here in the numerator. The whole thing is in the written solutions provided to you on the class website. So, for those who want to see the whole thing I would encourage you to look there. So, what I'm going to do is write out this expression, okay. And you might have noticed that I did something wrong here, okay? What did I do wrong? Well, if you think about what our data set really looks like, it has ten 11's, right? I said that before. ", ['expressions', 'numerator', 'thing', 'solutions', 'class', 'website', 'thing', 'expression', 'okay', 'something', 'Well', 'data']), 0.1336306209562122, 0.1336306209562122)
((0, '/Users/jag/Downloads/Stanford medstats/RExercise2.srt', "Welcome to the second R module. In this video, I'll be walking through the solutions to the second R exercise. You'll also be able to find a written version of these solutions on the class website. The first thing I'm going to do is to start Deducer. In this exercise, we'll be using the same data set as in the previous exercise. The data set's called mock class data, and it's provided to you in Excel form. We see two familiar windows on the screen. On the left we have the console window and on the right we have the data viewer window. Window. In order to open the class data, I'm going to click on the Open Data icon on the Data Viewer window, and then I'm going to navigate to where I have the mock class data set saved onto my computer. For me, that's on the desktop. So I'm going to select that and hit Open. ", ['Welcome', 'R', 'module', 'video', 'solutions', 'R', 'exercise', 'version', 'solutions', 'class', 'website', 'thing', 'Deducer', 'exercise', 'data', 'exercise', 'data', 'mock', 'class', 'data', 'Excel', 'form', 'windows', 'screen', 'left', 'console', 'window', 'right', 'data', 'window', 'Window', 'order', 'class', 'data', 'Open', 'Data', 'icon', 'Data', 'Viewer', 'window', 'mock', 'class', 'data', 'computer', 'desktop', 'Open']), 0.19069251784911848, 0.11376212001277639)
((7, '/Users/jag/Downloads/Stanford medstats/Unit 6 Module 3 part 1.srt', "increasing your chances of getting a false positive, of getting a type 1 error. So I'd like to illustrate this in my on-campus class with the following little experiment. I do an experiment where I also know that the null hypothesis is true. Just like those [INAUDIBLE] researchers knew for sure that the null hypothesis was true, they knew that any significant p values had to be false positive. I do the same little experiment in my class. So I asked my class to fill out a survey in the begining of quarter that says that asked them a bunch of questions. And one of the questions I like to ask is whether or not you were born on an odd day or an even day. Because presumably being born on a odd day even day really doesn't have anything to do with anything in, in your future. So I know that the null hypothesis is true. I know that there shouldn't be any difference, any real difference between those two groups. And then I asked the students to provide data on lots of other variables about themselves. In one class I asked them to provide data about 28 variables. I, I want to make sure that I'm going to get a false positive. So by doing 28 that's a lot more than 20. kind of guaranteeing myself a false positive. ", ['chances', 'false', 'type', 'error', 'class', 'experiment', 'experiment', 'null', 'hypothesis', '[', 'INAUDIBLE', ']', 'researchers', 'sure', 'null', 'hypothesis', 'p', 'values', 'little', 'experiment', 'class', 'class', 'survey', 'begining', 'quarter', 'bunch', 'questions', 'questions', 'day', 'day', 'born', 'day', 'day', 'anything', 'anything', 'null', 'hypothesis', 'difference', 'difference', 'groups', 'students', 'data', 'lots', 'variables', 'class', 'data', 'variables', 'false', 'doing', 'lot', 'kind', 'guaranteeing', 'myself', 'false']), 0.1889822365046136, 0.11274181133069626)
((1, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit3 Mod2.srt', "The controls were graduates 196 controls, who were matched by medical school graduation year, so they were the, the same graduating class, and speciality choice. So each, for each case, the, they went out found a couple of controls who came from the same graduating class. And were from the same specialty choice. The aim of the study, I'm just quoting from the paper here, was to determine if medical students who demonstrate unprofessional behavior in medical school are more likely to have subsequent state board disciplinary action. So they were going back into the records of these graduates and trying to determine if there were things that could predict subsequent disciplinary action by the medical board in, in particular. They wanted to see if there was some unprofessional behavior in medical school that could flag those who were at risk of this outcome. It's a binary outcome. You we're either disiplined are you weren't. What kind of analysis are we going to analyze we're going to use to ", ['controls', 'graduates', 'controls', 'school', 'graduation', 'year', 'graduating', 'class', 'speciality', 'choice', 'case', 'couple', 'controls', 'graduating', 'class', 'specialty', 'choice', 'aim', 'study', 'paper', 'students', 'behavior', 'school', 'state', 'board', 'disciplinary', 'action', 'records', 'graduates', 'things', 'action', 'board', 'behavior', 'school', 'risk', 'outcome', 'outcome', 'kind', 'analysis']), 0.12216944435630522, 0.10342542513122666)
((0, '/Users/jag/Downloads/Stanford medstats/RExercise3.srt', "Welcome to the third and final R video module. In this session, we'll be reviewing the problems from the third set of R/deducer exercises. And as before we'll be working with the same data set, mock class data which is provided to you in Excel format on the class website. So I'm going to start off by opening Deducer and loading the data. [SOUND] So again we have our 2 familiar windows here, the console window and the demo-viewer window, and I'm going to click on Open Data and navigate to where I saved. Our data set on my computer, which happens to be on the Desktop. And, I'm going to wait for r to load the data. Great. So, now that the data's been loaded, and ", ['Welcome', 'R', 'video', 'module', 'session', 'problems', 'set', 'R/deducer', 'exercises', 'data', 'mock', 'class', 'data', 'Excel', 'format', 'class', 'website', 'Deducer', 'data', '[', 'windows', 'console', 'window', 'window', 'Open', 'Data', 'data', 'computer', 'happens', 'Desktop', 'r', 'data', 'Great', 'data']), 0.12126781251816648, 0.10266212742890282)
((0, '/Users/jag/Downloads/Stanford medstats/RExercise1 full.srt', "Welcome to the first R module. In this video, we'll be going over the solutions to the first R exercise. If you prefer, there's also a written version of these solutions on the class website. Now, this first R exercise was really designed to get you to explore the different options and deducer. So if you found yourself struggling a little bit, don't worry. It's going to get easier from here. So, the first step is actually not question one. The first thing we need to do is open up deducer. So to do that, I'm going to click this short cut. And there are two windows here. There's the console window on the left, and the data viewer window on the. On the right, in order to open up my data, which is provided to you as an Excel file on the class website. I'm going to click on the Open Data icon on the Data Viewer window, on the right hand side of the screen. ", ['Welcome', 'R', 'module', 'video', 'solutions', 'R', 'exercise', 'version', 'solutions', 'class', 'website', 'R', 'exercise', 'options', 'deducer', 'bit', 'step', 'question', 'thing', 'deducer', 'cut', 'windows', 'window', 'left', 'data', 'window', 'right', 'order', 'data', 'Excel', 'file', 'class', 'website', 'Open', 'Data', 'icon', 'Data', 'Viewer', 'window', 'right', 'hand', 'side', 'screen']), 0.11867816581938533, 0.10046980092555897)
((5, '/Users/jag/Downloads/Stanford medstats/RIntro.srt', "So, not shown here, but I've gone to the class website and downloaded this Excel spreadsheet called mock class data, and saved it to my desktop. So what I want to do now is click on the open data icon in the data viewer window, on the right-hand side of the screen here, and then navigate towards the desktop, or wherever you saved mock class data, and hit Open. And I want, there's only one worksheet. [INAUDIBLE] here with data, so I'm just going to hit Ok. And here we go. So the data has been loaded. There should be 50 rows. Good. It's always good to check your data and look across it to make sure everything's been loaded correctly. And so just scrolling left and right and up and down, I see that everything's been loaded. And you're essentially ready to go at this point. So if you, a few things that I want to point out before you jump in the exercises. There are two tabs here data view and variable view. ", ['class', 'website', 'Excel', 'spreadsheet', 'mock', 'class', 'data', 'desktop', 'data', 'icon', 'data', 'viewer', 'window', 'right-hand', 'side', 'screen', 'towards', 'desktop', 'mock', 'class', 'data', 'Open', 'worksheet', '[', 'INAUDIBLE', ']', 'data', 'Ok.', 'data', 'rows', 'Good', 'data', 'everything', 'everything', 'point', 'things', 'point', 'exercises', 'tabs', 'data', 'view']), 0.14237369936287486, 0.09959573168868942)
((1, '/Users/jag/Downloads/Stanford medstats/Unit 9 Module 2.srt', "Then they found controls, the controls were graduates, 196 controls, who were matched by medical school graduation year. So they were the same graduating class, and specialty choice. So each, for each case, the, they went out found a couple of controls who came from the same graduating class. And were from the same specialty choice. The aim of the study, I'm just quoting from the paper here, was, to determine if medical students who demonstrate unprofessional behavior in medical school. Are more likely to have subsequent state board disciplinary action. So they were going back into the records of these graduates and trying to determine if there were things that could predict subsequent disciplinary action. By the medical board in particular. They wanted to see if there was some unprofessional behavior in medical school that could flag those who were at risk of this outcome. It's a binary outcome, you're either disciplined or you weren't. ", ['controls', 'controls', 'graduates', 'controls', 'school', 'graduation', 'year', 'graduating', 'class', 'specialty', 'choice', 'case', 'couple', 'controls', 'graduating', 'class', 'specialty', 'choice', 'aim', 'study', 'paper', 'students', 'behavior', 'school', 'Are', 'state', 'board', 'disciplinary', 'action', 'records', 'graduates', 'things', 'action', 'board', 'behavior', 'school', 'risk', 'outcome', 'outcome']), 0.11547005383792514, 0.09775389804739402)
QUERY: I'm referring to your correction in 2.5 that reads:  \Studies have begun to describe the epidemiology of autism, including recent changes in the disorder's prevalence and characteristics.\"  I think that as is, the sentence sounds like the word \"studies\" is the subject of the predicate \"have begun to describe.\" I think the original sentence, albeit problematic, correctly implied that certain investigators are carrying out those studies.   Therefore,I propose the following correction: \"Studies have begun that aim to describe the epidemiology of autism, including recent changes in the disorder's prevalence and characteristics.\"  Thanks a lot!"
*************************
((10, '/Users/jag/Downloads/Stanford medstats/SciWrite Demo Edit 7.5.srt', "So how about just grew profusely throughout the grass. And then I'm going to fold this last sentence about the plants right into this, this other sentence. I think we can fold both of those ideas together, so that mutants fungal cells grew profusely throughout the gla, grass, then what happened to the plants and, the infected plants. [BLANK_AUDIO] And again, the author originally had showed growth, showed poor growth. We could just say grew poorly or grew slowly, grew slowly and often died. So I'm repeating a lot of what the author had here but just tying it into, folding it into that last sentence. Moving along, the authors then say, this sets the stage for the next step, finding the genetic changes that cause these aborations. Actually, I don't think we need any of that. ", ['grass', 'sentence', 'plants', 'sentence', 'ideas', 'mutants', 'cells', 'gla', 'grass', 'plants', 'plants', '[', 'BLANK_AUDIO', ']', 'author', 'growth', 'growth', 'lot', 'author', 'sentence', 'authors', 'sets', 'stage', 'step', 'finding', 'changes', 'cause', 'aborations']), 0.1840186845246444, 0.29272936883047934)
((19, '/Users/jag/Downloads/Stanford medstats/Unit 9 Module 4.srt', "So, we were looking at kind of two factors here. We were looking at an eating disorder score called the EDI score, this tells us about eating, disorder eating behaviors. So, higher scores says, a woman is more likely to have some kind of eating disorder. We also looked at menstrual cycle, and so we divided women into two groups. Those are the normal menstrual cycle, those eumenorrheic women, those are all the bars in the back of this graphics. And women with menstrual irregularities, we call that the eligo and amenorrheic group, and that, those are the bars in the front. So, we looked at those two menstrual groups as well as this disordered eating score, and we looked at the interaction between those two things. And we did find a statistically significant interaction here. And it's kind of nicely pictured in these graphics. So, what's happening here is that, if you look at the women who already has menstrual irregularities. If you look at the effect of eating disorder score, it doesn't seem that they're bone density changes much at all across different eating disorder scores. Once you already have menstrual irregularities, ", ['kind', 'factors', 'disorder', 'score', 'EDI', 'score', 'disorder', 'behaviors', 'scores', 'woman', 'kind', 'disorder', 'cycle', 'women', 'groups', 'cycle', 'women', 'bars', 'back', 'graphics', 'women', 'irregularities', 'eligo', 'group', 'bars', 'front', 'groups', 'score', 'interaction', 'things', 'interaction', 'graphics', 'women', 'irregularities', 'effect', 'disorder', 'score', 'bone', 'density', 'changes', 'across', 'eating', 'disorder', 'scores', 'irregularities']), 0.1593919285085565, 0.19767783613197448)
((1, '/Users/jag/Downloads/Stanford medstats/Unit 2 Module 4 part 3.srt', "going to divide the 3 by the .9 and get a risk ratio of 3.33. How do I interpret these that 2.56 you would interpret that those with moderate blockage in the median group of coronary calcification have a 156% increased prevalence of depressive disorder compared to those with the least blockage. The 3.33 would, you would interpret that as those with the most severe blockage have a 233% increase in the prevalence of depressive disorder, compared with those with the least blockage. Okay, so those are the risk ratios, and you might think it would be just natural to report the risk ratios here. I mean this is a cross sectional study, it's not a case control study. The, the risk ratios are valid. Why would you report anything else? However, if you look at the paper from the study, you'll notice that when it came to reporting measures of relative risk, the authors actually reported odds ratios not risk ratios. So what's going on here is that the authors wanted to adjust for confounders. ", ['risk', 'ratio', 'blockage', 'group', 'calcification', '%', 'prevalence', 'disorder', 'blockage', 'blockage', '%', 'increase', 'prevalence', 'disorder', 'blockage', 'Okay', 'risk', 'ratios', 'risk', 'ratios', 'cross', 'study', 'case', 'control', 'study', 'risk', 'ratios', 'anything', 'paper', 'study', 'reporting', 'measures', 'risk', 'authors', 'odds', 'ratios', 'ratios', 'authors', 'confounders']), 0.10826639239215337, 0.196702850084858)
((12, '/Users/jag/Downloads/Stanford medstats/SciWrite Demo Edit 7.5.srt', "Since I cut the word aberrant before I can put it back in here, this aberrant growth. And we can end, end the sentence right there. I got rid of the concept that, somehow it was surprising. I'm not sure exactly what was surprising. Is it surprising that it only mutated one event? If that's an important detail, the author need, would need to, like, explain it a little bit better for the reader, but I just cut it out. Then I'm going to make a really short sentence. Sometimes it's nice to have kind of a long sentence. And then just punctuate it with a short sentence, so I'm just going to make a tiny sentence here, they called the gene, or they named the gene, maybe named, they named the gene NoxA. We could just leave it at that. We could just leave it at that, they named the gene NoxA. So it's a real short sentence, sometimes nice to to break things up with kind of a, just a short sentence in the middle of, of a lot of long sentences. So they named the gene, NoxA, and then, I'm going to fold this paragraph right into the next paragraph. I want to combine the two, we don't need, this doesn't need to be a whole separate paragraph. And the next paragraph talks about what NoxA does, so ", ['word', 'aberrant', 'aberrant', 'growth', 'end', 'sentence', 'concept', 'event', 'detail', 'author', 'explain', 'bit', 'reader', 'sentence', 'kind', 'sentence', 'sentence', 'sentence', 'gene', 'gene', 'gene', 'NoxA', 'gene', 'NoxA', 'short', 'sentence', 'things', 'kind', 'sentence', 'middle', 'lot', 'sentences', 'gene', 'NoxA', 'paragraph', 'paragraph', 'paragraph', 'paragraph', 'talks', 'NoxA']), 0.19460170216420797, 0.19282731311410187)
((19, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit4 Mod5_2.srt', "so higher scores is, women is more likely to have some kind of eating disorder. We also looked at menstrual cycle, and so we divided women into two groups. Those with the normal menstrual cycle, those are the eumenorrheic women, those are all the bars in the back of these graphics, and women with menstrual irregularities, we call that the oligo and amenorrheic group and that, those bars in the front. So we looked at those two menstrual groups as well as this disordered eating score and we looked at the interaction between those two things and we did find a statistically significant interaction here. And it's kind of nicely pictured in these graphics. So what's happening here is that if you look at the women who already have menstrual irregularities. If you look at the effect of eating disorder score, that doesn't seem that their bone density changes much at all across different eating disorder scores. Once you already have menstrual irregularities, there doesn't seem to be much additional effect of eating disorder score so the bone density is pretty flat across those different eating disorder ", ['scores', 'women', 'kind', 'disorder', 'cycle', 'women', 'groups', 'cycle', 'women', 'bars', 'back', 'graphics', 'women', 'irregularities', 'oligo', 'group', 'bars', 'front', 'groups', 'score', 'interaction', 'things', 'interaction', 'graphics', 'women', 'irregularities', 'effect', 'disorder', 'score', 'bone', 'density', 'changes', 'across', 'eating', 'disorder', 'scores', 'irregularities', 'effect', 'disorder', 'score', 'bone', 'density', 'eating', 'disorder']), 0.15450786078738143, 0.19162061637387312)
((19, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit4 Mod5.srt', "higher scores is, the woman is more likely to have some kind of eating disorder. We also looked at menstrual cycle and so we divided women into two groups. Those with a normal menstrual cycle, those are the eumenorrheic women, those are all the bars in the back of these graphics. And women with menstrual irregularities we call that the oligo and amenorrheic group, and that those are the bars in the front. So we looked at those two menstrual groups and as well as this disordered eating score and we looked at the interaction between those two things. And we did find a statistically significant interaction here. And this kind of nicely pictured in these graphics. So what's happening here is that if you look at the women who already have menstrual irregularities. If you look at the effect of eating disorder score. That doesn't seem that their bone density changes much at all across different eating disorder scores. Once you already have menstrual irregularities, it doesn't seem to be much additional effect of eating disorder score, so the bone density is pretty flat across those different eating disorder score groups. ", ['scores', 'woman', 'kind', 'disorder', 'cycle', 'women', 'groups', 'cycle', 'women', 'bars', 'back', 'graphics', 'women', 'irregularities', 'oligo', 'group', 'bars', 'front', 'groups', 'score', 'interaction', 'things', 'interaction', 'kind', 'graphics', 'women', 'irregularities', 'effect', 'disorder', 'score', 'bone', 'density', 'changes', 'across', 'eating', 'disorder', 'scores', 'irregularities', 'effect', 'disorder', 'score', 'bone', 'density', 'eating', 'disorder', 'score', 'groups']), 0.1500468969841066, 0.18608813000554927)
((9, '/Users/jag/Downloads/Stanford medstats/Unit 3 homework with questions.srt', "resistance of 50% in the general population is 59%. Please note that this value for the PPV is going to change. If the prevalence in the general population changes. The next part of this question is asking about the negative predictive value. The prevalence of resistance is still 50% in the general population, but now it's asking us to predict the negative predictive value of the test. And so again, we can't calculate this directly. We're going to use Bayes' rule for this, and we're going to set that up in a similar way. So, the probability that the NPV is the probability that you're a responder, in this situation, given that the test result is negative. Okay? So, if I set that up using Bayes' Rule [SOUND] it's going to look like this. ", ['resistance', '%', 'population', '%', 'Please', 'value', 'PPV', 'prevalence', 'population', 'changes', 'part', 'question', 'value', 'prevalence', 'resistance', '%', 'population', 'value', 'test', 'Bayes', 'rule', 'way', 'probability', 'NPV', 'probability', 'responder', 'situation', 'test', 'result', 'Okay', 'Bayes', 'Rule', '[', 'SOUND', ']']), 0.10482848367219183, 0.18454181131949657)
((3, '/Users/jag/Downloads/Stanford medstats/SciWrite Demo Edit 7.6.srt', "Health Organization, which is mentioned in the next sentence, but it's unclear, so we actually need the reference right there for that particular statistic. Actually going to delete this next sentence about the World Health Organization recognize it as this severe fear. I've already said what it is, that it's a severe fear in the first sentence, I don't think we need a second, a separate sentence for that. The main thing that the author is trying to achieve in this first paragraph, I think, is to differentiate for the reader, which is very important, what's the difference between just having a normal fear of the dentist and having odontophobia, which is as more so your fear? What's that what's the line between those two things, and I think the other's done a good job of, of giving us some sense of why those two things are different and how odontophobia maybe is even diagnosed. They may, the author may want to give a little bit more if there's some diagnostic criteria or something but I think I would probably just recommend moving the sentence about the symptoms to right after the first line, because they mentioned, you know, most people are afraid, but some people have a more severe fear. ", ['Health', 'Organization', 'sentence', 'reference', 'sentence', 'World', 'Health', 'Organization', 'recognize', 'fear', 'fear', 'sentence', 'sentence', 'thing', 'author', 'paragraph', 'reader', 'difference', 'fear', 'dentist', 'odontophobia', 'fear', 'line', 'things', 'done', 'job', 'sense', 'things', 'odontophobia', 'author', 'bit', 'criteria', 'sentence', 'symptoms', 'line', 'people', 'people', 'fear']), 0.1445787329915601, 0.17742325904379588)
QUERY: Well, mathematically may be not! But do we not take 'p' in binomial distribution as happening of some thing or success ?
*************************
((3, '/Users/jag/Downloads/Stanford medstats/Unit 1 Homework Solutions with questions.srt', "And, so what I'm going to do is write out the first few expressions here in the numerator. The whole thing is in the written solutions provided to you on the class website. So, for those who want to see the whole thing I would encourage you to look there. So, what I'm going to do is write out this expression, okay. And you might have noticed that I did something wrong here, okay? What did I do wrong? Well, if you think about what our data set really looks like, it has ten 11's, right? I said that before. ", ['expressions', 'numerator', 'thing', 'solutions', 'class', 'website', 'thing', 'expression', 'okay', 'something', 'Well', 'data']), 0.30304576336566325, 0.2720489477296241)
((13, '/Users/jag/Downloads/Stanford medstats/Unit 6 Module 3 part 1.srt', "Coffee is not bad for breast cancer but it does affect these other subgroups. And we worry about caffeine for those subgroups. However, my conclusion from this study would be completely different. So here is what I did. I went through that study and so in recent time when I went through that study they had multiple tables with all these different results and they had p-values for each of these 50 tests that I talked about, the 50 different comparisons they did. I extracted the p-values from all those 50 tests and put them in a little data set and graphed it, just so I could see the distribution of the p-values. So notice what the distribution looks like, it looks like a uniform distribution. Remember, if the null hypothesis is true, if there's no effects, we expect the p-value to follow uniform distribution. Equal probability from 0 to 1. Notice that the most surprising thing is as I mentioned earlier, the most surprising thing on this graphic, the thing that your eye gets drawn to, is that there are no P-values between 0.8 and 0.85. That's the surprising thing, that there's a gap. ", ['Coffee', 'cancer', 'subgroups', 'caffeine', 'subgroups', 'conclusion', 'study', 'study', 'time', 'study', 'tables', 'results', 'p-values', 'tests', 'comparisons', 'p-values', 'tests', 'data', 'set', 'distribution', 'p-values', 'notice', 'distribution', 'uniform', 'distribution', 'Remember', 'null', 'hypothesis', 'effects', 'uniform', 'distribution', 'probability', 'Notice', 'thing', 'thing', 'thing', 'eye', 'gets', 'P-values', 'thing', 'gap']), 0.33189592545800894, 0.19800034384977858)
((0, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod4.srt', "[BLANK_AUDIO] In this next module, I'm going to talk about the chi-squared test. This should be review, for everybody in this course. So, I'm going to just go over this quickly. And say a little bit about what the chi-squared distribution actually is. And then, I'll just do one practice problem on the chi squared. So the chi squared test is used when you have either a two by two table or something bigger than a 2x2 table, a bigger contingency table. And it's used when you have independent observations. But what is a chi-squared distribution? I've mentioned the chi-squared distribution a couple of times already this week. So, let me just say a little bit about what it actually that distribution actually is. So, I mentioned earlier that a chi-square with one degree of freedom is simply equal to a standard normal distribution squared. Well what exactly do I mean by that? One is equivilant to the other. Well here's the idea, take a z distribution, that's just a standard normal distribution that has a mean of zero and ", ['[', 'BLANK_AUDIO', ']', 'In', 'module', 'test', 'review', 'everybody', 'course', 'bit', 'distribution', 'practice', 'problem', 'chi', 'chi', 'test', 'something', 'bigger', 'contingency', 'observations', 'distribution', 'distribution', 'couple', 'times', 'week', 'bit', 'distribution', 'degree', 'freedom', 'distribution', 'Well', 'Well', 'idea', 'z', 'distribution', 'distribution', 'zero']), 0.36469840431289846, 0.18798408951408233)
((16, '/Users/jag/Downloads/Stanford medstats/Unit 4 module 4 part 2.srt', "If you just look at this graphic, if you look at the, all the different outcomes here; the mean is 50 as expected. The standard deviation for this data set of 30,000 observations, turns out to be exactly five. Exactly what the formula predicted. The other really interesting thing to notice here, is that it makes a perfect normal distribution. So even though we're talking about a binomial distribution, which is a discrete distribution, it turns out that in many cases the binomial distribution ends up looking like a normal. And in a few modules we're going to see that we often approximated a binomial as a normal for that reason. The other thing to notice, is because we're on a normal distribution, it's actually pretty much a perfect normal distribution, 95% of the observations are expected to fall between two standard deviations below, all the way up to 2 standard deviations above the mean. And indeed, 95% of these observations fell between 40 heads, and 60 has two standard deviations below to above the mean. So it really is on a perfect normal curve. ", ['outcomes', 'mean', 'deviation', 'data', 'set', 'observations', 'formula', 'thing', 'notice', 'perfect', 'distribution', 'distribution', 'distribution', 'cases', 'distribution', 'ends', 'modules', 'reason', 'thing', 'notice', 'distribution', 'perfect', 'distribution', '%', 'observations', 'deviations', 'way', 'deviations', '%', 'observations', 'heads', 'deviations', 'perfect', 'curve']), 0.3223291856101521, 0.1807020384954652)
((7, '/Users/jag/Downloads/Stanford medstats/HRP262 Unit 1 Module 4.srt', "I'm not expecting you to calculate this by hand. Just realize that we the reason that's the variances that were working on a hyper-geometric distribution, that's the variance of a hyper-geometric distribution. That's what a follows. But that will be the variance for a give a, and then we add, of all the variances, over all the different strata, that's our denominator. Again, this is identical to the Mantle-Haenszel statistic for stratified unconfounders. Now, a couple notes about this. So first of all, how do you know that this is a chi-square statistic with one degree of freedom? How do you know that this complicated formula follows a chi-square? Well, we can do a little bit of algebraic rearrangement and show that this statistic, what you see here, is just the same as saying, I want to add up all the different observed a's over different, all the different strata, so it's the sum of observed a's. We can rearrange that expected value as well. And show you that this is, we are subtracted the expected value of the sum of ace. ", ['hand', 'reason', 'variances', 'distribution', 'variance', 'distribution', 'variance', 'variances', 'strata', 'denominator', 'Mantle-Haenszel', 'unconfounders', 'couple', 'notes', 'degree', 'freedom', 'formula', 'Well', 'bit', 'rearrangement', 'show', 'strata', 'sum', 'value', 'value', 'sum', 'ace']), 0.1815682598006407, 0.16299668232042372)
((8, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit3 Mod1 pt1_2.srt', "How could we, what could we do. So what we could do here, is to transform the outcome variable. Again, I still want to fit a line ultimately but I don't want a 0-1 variable. As my outcome, because a 0-1 variable doesn't fit well to a line. It has only two possible values. So what could I do? How could I transform that binary, that 0-1 variable, into something that better fits a line. Well, here is one thing. This is really just a starting point because we're going to go further with this but let's just start with, with some simple thing that we can do. So, recognize that instead of modelling it as a yes no, a 0-1. What if we could somehow model it as a probability? A probability, as you know, is a value that ranges from 0-1 but it can take on any value between 0-1. So we would do a little bit better. We would put in all these values between 0-1 back into play, ", ['transform', 'outcome', 'line', 'outcome', 'line', 'values', 'something', 'fits', 'line', 'Well', 'thing', 'starting', 'point', 'thing', 'yes', 'model', 'probability', 'probability', 'value', 'value', 'bit', 'values', 'back', 'play']), 0.17928429140015903, 0.16094632796767533)
((10, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod3.srt', "we're going to expect to see about 26 or 27 in each cell. Instead we're seeing here a 37 to 16 split, and we say, well, is that discrepant enough from 26 to Is that a big enough discrepancy from the null hypothesis, where we're sure that that sort of rises above just random chance? How do we evaluate that? Well we can recognize here that what we are, we have under the null hypothesis, we are on a binomial distribution, and that binomial distribution has an n of 53 because I have 53 pairs, and we're expecting half of them, so my p is 0.5. We're expecting half of them to have a diabetic case rather than a diabetic control. So our null hypothesis here is that we're on a binomial with an n of 53 and a p of 0.5. Now, how do we then calculate a p-value for this null hypothesis? So what we would have to do, if we wanted to just stick with the binomial distribution here, is we would have to calculate the probability of what we observed, and everything more extreme. ", ['cell', 'split', 'Is', 'discrepancy', 'null', 'hypothesis', 'sort', 'rises', 'random', 'chance', 'Well', 'null', 'hypothesis', 'distribution', 'distribution', 'n', 'pairs', 'half', 'p', 'half', 'case', 'control', 'hypothesis', 'n', 'p', 'null', 'distribution', 'probability', 'everything']), 0.207669652660466, 0.16087193442928951)
((10, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod5.srt', "you do Is not only calculate the probability for the table that you have observed and the more extreme tables but the best thing to do is, I will give you problems that are pretty small, is to calculate all out the probabilities for all possible outcomes. So here's the probability. Complete probability distribution where X is the number of correct identifications of milk-poured first cups. She can get four right, which we already calculated. She can get three right, which we already calculated. She could have also gotten 2 right, 1 right, or 0 right. And that goes through all possible outcomes. I can calculate the probability for each one of those tables. This is a really good thing to do because it's a little bit of a check To make sure that you've calculated the probabilities correctly, cause they better. You've now accounted for all possible outcomes, they better add up to one. And indeed, if you add those up, you'll notice they add up to one. The distribution here turned out to be symmetrical. I'll warn you. That in Fischer's exact tests, the distributions are not always symmetrical. So just keep that in the back of your mind. The one tail test and the two tail test, ", ['probability', 'table', 'tables', 'thing', 'problems', 'probabilities', 'outcomes', 'probability', 'Complete', 'probability', 'distribution', 'X', 'number', 'identifications', 'cups', 'right', 'outcomes', 'probability', 'tables', 'thing', 'bit', 'check', 'probabilities', 'outcomes', 'distribution', 'Fischer', 'tests', 'distributions', 'back', 'mind', 'tail', 'test', 'tail', 'test']), 0.1889822365046136, 0.15998737045684966)
QUERY: Is the general distribution of a sample mean always normal? Irrespective of the variable being measured (i.e. vit.D levels, blood pressure levels etc.)?  Thanks.
*************************
((2, '/Users/jag/Downloads/Stanford medstats/RExercise1 mp4.srt', "Along with their type and something called factor levels. And so I'm going to go to varsity. Click on Type, and then select Factor. And notice when I do this that The factor levels column gets populated for that variable. It shows two factor levels. Level 0 and level one. That's because varsity only has two values in it. I do want to caution you. That when you're doing variable conversions in Deducer, like I did just now, that you don't go back and forth too many times, because Deducer will make assumptions about what types of variables you want, and what you want certain values to be. So just be aware that Deducer makes certain assumptions and to always check your data. After changing anything. ", ['type', 'something', 'factor', 'levels', 'varsity', 'Click', 'Type', 'Factor', 'notice', 'factor', 'levels', 'gets', 'factor', 'levels', 'Level', 'level', 'varsity', 'values', 'caution', 'conversions', 'Deducer', 'times', 'Deducer', 'assumptions', 'types', 'variables', 'values', 'Deducer', 'assumptions', 'data', 'anything']), 0.20225995873897262, 0.2555392227312903)
((2, '/Users/jag/Downloads/Stanford medstats/RExercise1 full.srt', "Along with their type, and something called factor levels. And so, [COUGH] I'm going to go to Varsity, click on Type, and then select Factor. And notice when I do this, that the factor levels column gets populated for that variable. It shows two factor levels, level zero and level one. And that's because varsity only has two values in it. I do want to caution you, that, when you're doing variable conversions in deducer like I did just now. That you don't go back and forth too many times. Because deducer will make assumptions about what types of variables you want. And what you want certain values to be. So just be aware, that deducer makes certain assumptions. And to always check your data, after changing anything. ", ['type', 'something', 'factor', 'levels', 'COUGH', 'Varsity', 'click', 'Type', 'Factor', 'notice', 'factor', 'levels', 'gets', 'factor', 'levels', 'level', 'zero', 'level', 'varsity', 'values', 'caution', 'conversions', 'deducer', 'times', 'deducer', 'assumptions', 'types', 'variables', 'values', 'deducer', 'assumptions', 'data', 'anything']), 0.19867985355975656, 0.2510160472075881)
((0, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 8 mod1 Part 1.srt', "In this next module, I'm going to tell you about multinomial logistic regression which is used when you have a categorical outcome that has more than 2 levels. So, so far we've been talking about binary outcome But what if your outcome variable is a categorical variable with more than two levels? What do you do then? So, one of the things that you'll see that people sometimes do in the literature is they'll actually just collapse that multilevel categorical variable into only two categories. Unfortunately, that's not ideal, because you're losing a lot of information when you do that. Now, that would make sense if, say one of the categories had only very few people in it and there wasn't a big enough sample size in one of the categories you may then have to collapse it. But assuming you have a reasonable sample size in each of the categories it's not ideal to collect them and to lose that information. Another thing that you could do, which is conceptually pretty easy is just to think about the outcome variable and dummy coat it. ", ['module', 'regression', 'outcome', 'levels', 'outcome', 'outcome', 'levels', 'things', 'people', 'literature', 'categories', 'lot', 'information', 'sense', 'categories', 'people', 'size', 'categories', 'sample', 'size', 'categories', 'information', 'thing', 'outcome', 'dummy', 'coat']), 0.16413304107465318, 0.23359133753530248)
((4, '/Users/jag/Downloads/Stanford medstats/Unit 5 Module 4 part 1.srt', "nanomoles per liter. It doesn't matter too much what I pick there. But lets say I'm going to pick the value of 62. So I tell the computer, this is the population you're sampling from. And then I have the computer sample a random sample of 100 virtual men from this underlying population. Each of those men is going to have a vitamin D level. I take those vitamin D levels and I average them to calculate the mean. Vitamin D level for that sample of 100. Then I'm going to repeat the experiment over and over and over again. Again this is the though experiment. I'm going to select a different random sample of 100 men, calculate their mean. Then I'm going to do that again and again and again, to get a sense of the distribution of the means. I've arbitrarily decided to do that 10,000 times. Again, 10,000's a pretty large number but not so large that it will hang my computer. Then, then we can explore the distribution of those 10,000 means. We can actually look at the distribution of the statistic here and the statistic here is a sample mean. [SOUND] So here's the results of this computer simulation. ", ['nanomoles', 'liter', 'lets', 'value', 'computer', 'population', 'computer', 'sample', 'random', 'sample', 'men', 'population', 'men', 'vitamin', 'D', 'level', 'vitamin', 'D', 'levels', 'Vitamin', 'D', 'level', 'sample', 'experiment', 'experiment', 'random', 'sample', 'men', 'sense', 'distribution', 'means', 'times', 'pretty', 'number', 'computer', 'distribution', 'means', 'distribution', 'sample', 'mean', '[', 'results', 'computer', 'simulation']), 0.2671036121482476, 0.22631465248131039)
((0, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 8 Module 4.srt', "So before I show you how to analyze time changing predictors with generalized estimating equations, I'm first going to show you how to graph them. So I made up a little data set just for us to have some data to work with. For time changing predictors. So in this data set I've got just six patients and they're all depressed and they're all given the same drug. Our time changing predictor here is levels of some kind of happy chemical in the brain and our outcome here is scores on a depression questionnaire. So for example, subject number one they had a score of 20 at their, on depression score at base line that went down to 18 and then down to 15 and then popped back up to Meanwhile their ca, their levels of happy chemical in their brain starting at a thousand and increased throughout the study. And everybody was given a drug that's supposed to increase that, their happy chemical levels in the brain. ", ['time', 'predictors', 'estimating', 'equations', 'data', 'set', 'data', 'time', 'predictors', 'data', 'set', 'patients', 'drug', 'time', 'predictor', 'levels', 'kind', 'chemical', 'brain', 'outcome', 'scores', 'depression', 'questionnaire', 'example', 'number', 'score', 'depression', 'score', 'base', 'line', 'Meanwhile', 'levels', 'chemical', 'brain', 'thousand', 'study', 'everybody', 'drug', 'happy', 'chemical', 'levels', 'brain']), 0.1656472891122698, 0.20928205350779833)
((0, '/Users/jag/Downloads/Stanford medstats/Unit 5 Module 4 part 1.srt', "In this next module, I'm going to show you the distribution of some common statistics. A mean, an odds ratio, and a correlation coefficient. In order to illustrate all of these, I'm going to use some example data. Some made up hypothetical data. These are actually computer generated data. I have the computer generate some data, it's loosely based on a real example in the literature. About that, that study was looking at middle age and older European men, and it was looking at Vitamin D levels and cognitive function. So I loosely based my made up data on that study. But it's just nice to have some toy data play with this week. There's 2 statistics that we're going to look at from my hypothetical data. I chose a sample of 100, so my sample size here is 100 in my hypothetical data. We're going to look at the Vitamin D levels in those 100 men. ", ['module', 'distribution', 'statistics', 'odds', 'correlation', 'coefficient', 'order', 'example', 'data', 'data', 'computer', 'data', 'computer', 'generate', 'data', 'example', 'literature', 'study', 'age', 'men', 'Vitamin', 'D', 'levels', 'function', 'made', 'data', 'study', 'toy', 'data', 'week', 'statistics', 'data', 'sample', 'sample', 'size', 'data', 'Vitamin', 'D', 'levels', 'men']), 0.16390251702679645, 0.20626105469304218)
((4, '/Users/jag/Downloads/Stanford medstats/Geriatric Anesthesia - 3. Pharmacology: Discussion.srt', "and when you pour it in your concentration goes higher. And that's actually a common physiologic finding. That elderly patients have smaller volumes of distribution and thus they tend to have slightly higher levels. So its those two processes, end organ sensitivity and the kinetics, primarily around volume of distribution, that result in recurring of overdose.  Does clearance of medication have an effect as well on how the medication should be dosed?  Absolutely! Clearance is, is really a fundamental concept. However, what's interesting about clearance is, where clearance is very important are, is drugs people take, every single day. Because their steady state levels, will be determined by clearance, and only clearance. Since our drugs are so rarely at steady state, when we give our drugs, particularly our induction drugs, clearance hasn't had much chance to operate. Now the clearance of the elderly is reduced a little bit, but not that much because hepatic blood flow, for example, which is very important for [UNKNOWN] clearance, isn't reduced very much in the elderly. ", ['concentration', 'physiologic', 'finding', 'volumes', 'distribution', 'levels', 'processes', 'end', 'sensitivity', 'kinetics', 'volume', 'distribution', 'result', 'recurring', 'overdose', 'clearance', 'medication', 'effect', 'medication', 'Clearance', 'concept', 'clearance', 'clearance', 'drugs', 'people', 'day', 'state', 'levels', 'clearance', 'clearance', 'drugs', 'state', 'drugs', 'induction', 'drugs', 'clearance', 'chance', 'clearance', 'bit', 'blood', 'flow', 'example', '[', 'UNKNOWN', ']', 'clearance']), 0.15843755557440742, 0.19938374290141)
((1, '/Users/jag/Downloads/Stanford medstats/Unit 5 Module 4 part 1.srt', "And we're going to look at the correlation between Vitamin D and cognitive function in those 100 men. And cognitive function was measured by a particular test, the Digit Symbol Substitution Test. And for the purposes of generating this made-up data, I assumed that the true mean vitamin D in all middle-aged and older European men is somewhere around 62 nanomoles per liter, and the true correlation coefficient between vitamin D and IQ function. The cognitive function was 0.15, which was a very weak correlation. So let me just illustrate for you this hypothetical, made-up data. So, in the sample, I have a sample of a 100 men, and I've got their vitamin D levels. And I've plotted those vitamin d levels here in histogram. So, notice that the vitamin D levels in the sample follow a right skew distribution. That makes sense for something like vitamin D because you can't get lower than zero. But, you know, vitamin D can get quite high in a handful of people. So you get this tail up to the right. The mean vitamin D turned out to be 63 nanomoles per liter in my sample. And the standard deviation was 33 nanomoles per liter. ", ['correlation', 'Vitamin', 'D', 'function', 'men', 'function', 'test', 'Digit', 'Symbol', 'Substitution', 'Test', 'purposes', 'made-up', 'data', 'mean', 'vitamin', 'D', 'men', 'nanomoles', 'liter', 'correlation', 'coefficient', 'vitamin', 'D', 'IQ', 'function', 'function', 'correlation', 'let', 'data', 'sample', 'sample', 'men', 'vitamin', 'D', 'levels', 'vitamin', 'd', 'levels', 'histogram', 'notice', 'vitamin', 'D', 'levels', 'sample', 'follow', 'right', 'skew', 'distribution', 'sense', 'something', 'vitamin', 'D', 'vitamin', 'D', 'handful', 'people', 'tail', 'right', 'vitamin', 'D', 'nanomoles', 'liter', 'sample', 'deviation', 'nanomoles', 'liter']), 0.19487094073848926, 0.19433200483939855)
QUERY: I am so glad you asked this, as I have been wondering about it myself. Seems like we might bias the simulation by entering a mean. This is where the central limit theorem starts to get fuzzy for me."
*************************
((5, '/Users/jag/Downloads/Stanford medstats/Unit 5 Module 4 part 1.srt', "What you are looking at is the histogram. On the Y axis is the count. Out of 10,000 times, how much did different outcomes, different means occur. On the X axis is the mean vitamin D level. So this represents the mean out of 100 men, and in each sample of a hundred men. So you can see that most often we got a mean of about 62, somewhere around 62, because remember the true mean in the population is 62. So that's not surprising. What else can we say about this distribution? Well, you might have noticed, and this might have surprised you, that it turns out to be normal distribution. The means follow a normal distribution. This is a little surprising because remember, Vitamin D, the trait followed a right skewed distribution. The, that distribution, that underlying distribution is right skewed. But, when we take the means, the means follow a normal distribution. And we can prove this with a theorem called the central limit theorem. ", ['histogram', 'Y', 'count', 'times', 'did', 'outcomes', 'means', 'X', 'vitamin', 'D', 'level', 'represents', 'men', 'sample', 'men', 'remember', 'mean', 'population', 'distribution', 'Well', 'distribution', 'means', 'distribution', 'remember', 'Vitamin', 'D', 'trait', 'right', 'distribution', 'distribution', 'distribution', 'means', 'means', 'distribution', 'theorem', 'limit', 'theorem']), 0.13400504203456162, 0.12372508579268955)
((6, '/Users/jag/Downloads/Stanford medstats/Unit 5 Module 4 part 1.srt', "For those of you who are interested in that, there's an optional module on the central limit theorem. But it turns out that when you take averages, averages follow a normal distribution. Conceptually, what's going here is you can image your on this right skew distribution. There's going to be a very few number of people who have very high values. So you might get a man in a particular sample who has a value of 200 for example. Well if you average that 200 in with 99 other men, that has the effect of bringing everything towards the center. So that 200 really gets diluted, because you averaged it with 99 other men. So it ends up when you take averages, it ends up bringing everything towards the middle and it ends up forming a normal curve. The mean of the means, I'm going to say that again, the mean of the means, that's the, this is the mean of the means we're talking about here. The mean of the means turned out to be 62 nanomoles per liter. Well, that's just the true value, the true mean, so that's not surprising. ", ['module', 'limit', 'theorem', 'averages', 'averages', 'distribution', 'right', 'distribution', 'number', 'people', 'values', 'man', 'sample', 'value', 'example', 'Well', 'men', 'effect', 'everything', 'towards', 'center', 'men', 'averages', 'everything', 'towards', 'middle', 'curve', 'means', 'means', 'means', 'means', 'nanomoles', 'liter', 'Well', 'value', 'mean']), 0.12309149097933272, 0.12309149097933272)
((7, '/Users/jag/Downloads/Stanford medstats/Unit 5 Module 4 part 1.srt', "The mean of the means is the true mean in the population. Well that makes sense because if that's the true mean, then most of our samples should hover around 62, the true mean. The standard deviation of the means turns out to be 3.3 nanomoles per liter. Now that's interesting, you remember that the, the standard deviation of the trait was 33 nanomoles per liter [SOUND] but the standard deviation of the means which we also called the standard error of the mean turns out to be 3.3. It's exactly 33 divided by 10. Well, the sample size here was 100, so what we're actually doing is, we're dividing the standard deviation by the square root of n, and that's what gives us the 3.3. This 3.3, I didn't get by a mathematical theory though. I just had the computer calculate the standard deviation from these 10,000 observations. And it turns out to be 3.3. So, the distribution of mean vitamin D that I got from this computer simulation. ", ['means', 'mean', 'population', 'Well', 'sense', 'mean', 'samples', 'mean', 'deviation', 'means', 'nanomoles', 'liter', 'deviation', 'trait', 'nanomoles', 'liter', 'SOUND', 'deviation', 'means', 'error', 'turns', 'Well', 'sample', 'size', 'deviation', 'root', 'n', 'theory', 'computer', 'calculate', 'deviation', 'observations', 'distribution', 'vitamin', 'D', 'computer', 'simulation']), 0.15194743527951726, 0.11770654754717863)
((9, '/Users/jag/Downloads/Stanford medstats/Unit 5 Module 4 part 1.srt', "real data. They usually follow a T but by the time you're talking about a sample size of greater than 100 where, where it's interchangeable with the normal. So I'm just going to call it a normal distribution. In general, sample means the mean of the means is the true mean in the population. Well, that make sense. The standard error, somebody has worked out the formula for standard error, the standard error of a mean is the standard deviation of the trait, the variability of the trait like vitamin divided by the square root of n, the number of people you're averaging over. And let's remember, we saw that that in fact works out with exactly what we saw in the simulation because the standard deviation here was 33 if I divide by the square root of 100. I get exactly 3.3, exactly what came out in the simulation as well. So both the simulation and mathematical theory are matching up. So the standard error of a mean, again, is the standard deviation of the trait divided by the square root of n. So you can see some things about how standard error works just by ", ['data', 'T', 'time', 'sample', 'size', 'distribution', 'means', 'means', 'mean', 'population', 'Well', 'make', 'sense', 'error', 'somebody', 'formula', 'error', 'error', 'deviation', 'trait', 'variability', 'trait', 'vitamin', 'root', 'n', 'number', 'people', 'remember', 'fact', 'works', 'simulation', 'deviation', 'root', 'simulation', 'simulation', 'theory', 'error', 'deviation', 'trait', 'root', 'n.', 'So', 'things', 'error', 'works']), 0.1382602259640567, 0.10710370880156395)
((1, '/Users/jag/Downloads/Stanford medstats/Unit 8 Module 1 part 1.srt', "enough even when the underlying trait that your looking at is highly skewed. Eventually the central limit theorem kicks You can't guarantee exactly where that's going to kick in. So if you're dealing with small samples, under a 100, you have to worry about the normality assumption. Usually as long as our sample size is above 100. Most times unless you have an extremely skewed distribution, that central limit there will, will apply. But for small symbols we're going to worry about this and I'll show you some ways to evaluate the, this normal assumption for linear regression. We also have an assumption called the homogeneity of variances. For the ttest and ANOVA, that assumption said that in different groups, in the different groups we were comparing the variances across the different groups had to be the same. This week we're going to talk about linear correlation and linear regression. The assumption is the same except we might talking about now a continuous predictor. Instead of comparing groups our predictor variable might also be continuous. ", ['trait', 'looking', 'limit', 'theorem', 'kicks', 'samples', 'normality', 'assumption', 'sample', 'size', 'times', 'distribution', 'limit', 'symbols', 'ways', 'assumption', 'linear', 'regression', 'assumption', 'homogeneity', 'variances', 'ANOVA', 'assumption', 'groups', 'groups', 'variances', 'groups', 'week', 'correlation', 'regression', 'assumption', 'except', 'predictor', 'groups', 'predictor']), 0.11547005383792515, 0.1036592833109044)
((8, '/Users/jag/Downloads/Stanford medstats/Unit 5 Module 4 part 1.srt', "I see that it's normally distributed. The mean is equal to the true mean. And the standard deviation, or what we also call the standard error, is 3.3 nanomoles per liter. I got this all out of the computer simulation. But the distribution of a sample mean, in general, people have long ago worked out with mathematical theory. And it turns out that if you're talking about sample means, means that you calculate from data, they follow, actually they don't exactly follow a normal distribution. If you want to know more about why that watch the optional module on the Central Limit Theorem. It turns out they follow a T-distribution, remember that a T-distribution is essentially the same as a normal distribution as long as you're talking about n's sample sizes at least of 100. So since we have a sample size of over 100, essentially it's, it's on a normal distribution. This means that if you're talking about sample means and you're talking about small samples, actually small samples you have to worry about it being on a T-distribution. You'll have slightly fatter tails than the normal. So it's not exactly, means don't exactly follow a normal distribution for ", ['mean', 'deviation', 'error', 'nanomoles', 'liter', 'all', 'computer', 'simulation', 'distribution', 'sample', 'mean', 'people', 'theory', 'sample', 'means', 'data', 'distribution', 'watch', 'module', 'Central', 'Limit', 'Theorem', 'T-distribution', 'remember', 'T-distribution', 'distribution', 'n', 'sample', 'sample', 'size', 'distribution', 'sample', 'means', 'samples', 'samples', 'T-distribution', 'tails', 'distribution']), 0.10540925533894598, 0.09462754627366858)
((2, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 7 mod1.srt', "Then I could actually look and see what's the distribution, of that beta coefficient. You can see I'm putting it here in a histogram, and we would say, well it looks normally distributed, the mean is the true beta, and the standard error is some value. Now I showed you in, in the fall that there's two ways to figure out the distribution of a statistic. There was simulation which I like to show teaching, because it's very hands on and it's very intuitive. I can, in the computer, virtually do repeated sampling. And in a way the boot strap is going to look a lot like that. Of course more practically statisticians spend their time figuring out formulas so that we don't have to do a simulation every time. So for example, I told you that for an odds ratio or actually for the natural log of the odds ratio, it follows a z distribution, and the standard error there is the square root of 1 over a plus 1 over b plus 1 over c plus 1 over d. So that formula is there for you. You can just apply it and you don't have to run a simulation every time to ", ['distribution', 'beta', 'coefficient', 'histogram', 'beta', 'error', 'value', 'fall', 'ways', 'distribution', 'simulation', 'teaching', 'computer', 'way', 'boot', 'strap', 'lot', 'course', 'spend', 'time', 'simulation', 'time', 'example', 'odds', 'log', 'odds', 'z', 'distribution', 'error', 'square', 'root', 'plus', 'b', 'c', 'd.', 'So', 'formula', 'simulation', 'time']), 0.12598815766974242, 0.08813343197079226)
((3, '/Users/jag/Downloads/Stanford medstats/Unit 5 Module 4 part 1.srt', "levels went up and cognitive function went down, and anything in between positive one and zero or negative one and zero is some kind of in between amount of correlation. So 0.15 is pretty close to zero so it's a small, it's a very weak correlation. But the correlation coefficient in the sample of a hundred came out to be 0.15 here. So how would I figure out, what's the distribution of the mean here? So we're going to use the mean Vitamin D as my example. What's the distribution of the mean? How can I figure that out? Well, I'm first going to figure it out by running a computer simulation. So, here's how the computer simulation works. I'm going to tell the computer that there's some large population of men European men. And I'm going to tell the computer the distribution of vitamin D in this large population. And I'm going to tell the computer it's a right skewed distribution. So vitamin D follows a right skewed distribution. It has a standard deviation of 33. And let's say that the true mean value in the the population is 62 [INAUDIBLE] ", ['levels', 'function', 'went', 'anything', 'kind', 'amount', 'correlation', 'correlation', 'correlation', 'coefficient', 'sample', 'distribution', 'Vitamin', 'D', 'example', 'distribution', 'Well', 'running', 'computer', 'simulation', 'computer', 'simulation', 'works', 'computer', 'population', 'men', 'men', 'computer', 'distribution', 'vitamin', 'D', 'population', 'computer', 'right', 'distribution', 'vitamin', 'D', 'right', 'distribution', 'deviation', 'say', 'mean', 'value', 'population', 'INAUDIBLE', ']']), 0.0944911182523068, 0.08482616290305216)
QUERY: Hi, I'm confused how to go about finding the 95% confidence interval for an odds ratio.   Thanks!
*************************
((6, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 8 mod5.srt', "You have a 6.7% increase in your prevalence of or risk of menstrual irregularity. We can also do the 95% confidence limits, so if I exponentiate those confidence limits from the previous page, I get a lower bound of about 0.04, 1.04. And an upper bound of about 1.096. So that can be interpreted as the adjusted risk ratio. You don't need to worry about this business about odds ratios being misleading, you can avoid them altogether. And just to compare, remember when we did the odds ratio from logistic regression, what we got there was point estimate of 1.12 and a 95% confidence interval of 1.06 to 1.1 about 1.19. So the width of the confidence interval is similar here but these are all inflated. ", ['%', 'increase', 'prevalence', 'irregularity', '%', 'confidence', 'limits', 'confidence', 'limits', 'page', 'bound', 'bound', 'risk', 'ratio', 'business', 'odds', 'ratios', 'remember', 'odds', 'regression', 'point', 'estimate', '%', 'confidence', 'interval', 'width', 'confidence', 'interval']), 0.6111111111111112, 0.4372889450504384)
((3, '/Users/jag/Downloads/Stanford medstats/Unit 5 Homework with answers.srt', "the odd ratio is given by by 10 this one by 40, this one by 5, this one by 55. This turns out to be .586. The 95% confidence in material is now given in new by plus or minus one point. 9 6, times the standard error. In this case it's 1.01, plus, minus 1.96 times .586. Which is an old issue of 1.44 To 2.16. This question wants us to exponentiate the values we found in d. So we have the confidence interval in terms of odds ratio as opposed to log of odds ratio. So that would be an exponential of minus 1.4, which would be the lower bound. Exponential of 2.16 which is an exponential of the upper bound and ", ['ratio', '%', 'confidence', 'material', 'point', 'times', 'error', 'case', 'times', 'issue', 'question', 'values', 'd.', 'So', 'confidence', 'interval', 'terms', 'odds', 'odds', 'minus', 'bound', 'bound']), 0.447213595499958, 0.40147067794292923)
((4, '/Users/jag/Downloads/Stanford medstats/Unit 5 Homework with answers.srt', "in terms of the odd ratio we get the confidence interval to be .87 to 8.66. The answer to 'F' is false. The confidence intervals of the laws of an odd ratio as symmetric as we just found but when we exponentiate the symmetry is lost. And the confidence interval of the actual odds ratio is not symmetric. The answer to G is true. The 95% confidence interval for the oz ratio crosses one, so we cannot reject the null hypothesis. And thus we know that b is greater 0.05. Part A of this question wants us to roughly estimate the two sided p value associated with this distribution. If we down the number of observations we have to the left of negative 25 and to the right of 25, we'll see they're about 25 observation out of a thousand that have ", ['terms', 'ratio', 'confidence', 'interval', 'answer', 'confidence', 'intervals', 'laws', 'ratio', 'symmetry', 'confidence', 'interval', 'odds', 'answer', 'G', '%', 'confidence', 'interval', 'oz', 'ratio', 'crosses', 'null', 'hypothesis', 'b', 'Part', 'question', 'p', 'value', 'distribution', 'number', 'observations', 'left', 'right', 'observation', 'thousand']), 0.4704375361403851, 0.338971500714074)
((14, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod2.srt', "I compared the natural log of the odds ratio that I observed with its standard, with its standard error. Simple to calculate. The 95% confidence interval is similarly going to be based on the standard error. Notice, though, that we're going to be calculating the 95% confidence interval for the natural log of the odds ratio by adding and subtracting 1.96. Times the standard error. That will give me a confidence interval for the natural log of the odds ratio. You don't want to report that in your paper, though, because odds ratios are hard enough to understand. Don't bother with natural logs of odds ratios, nobody can understand that. So what we're going to do, of course, at the end of the day, is take the upper and lower bounds and exponentiate them to get back to an actual odds ratio measure. And that's going to have an interesting effect, that, if you've ever noticed odds ratios and 95% confidence intervals in the literature, a lot of people would remark, well, they're not symmetric. If you look at the confidence interval, this was the confidence interval from a previous module for an odds ratio. ", ['log', 'odds', 'error', 'Simple', '%', 'confidence', 'interval', 'error', 'Notice', '%', 'confidence', 'interval', 'log', 'odds', 'Times', 'error', 'confidence', 'interval', 'log', 'odds', 'paper', 'odds', 'ratios', 'Do', 'logs', 'odds', 'ratios', 'nobody', 'course', 'end', 'day', 'bounds', 'odds', 'measure', 'effect', 'odds', 'ratios', '%', 'confidence', 'intervals', 'literature', 'lot', 'people', 'confidence', 'interval', 'confidence', 'interval', 'module', 'odds']), 0.6713060174670888, 0.3228143181326534)
((8, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod1.srt', "Times the standard error. The standard error here. Again, I'm going to plug in just the observed proportions in each group. So, 1 minus 0.364 and the treatment group divided by 33. Plus 0.629 times 1 minus 0.629, divided by 35 for the placebo group. If you crank out the math on that, this. Standard error comes out to be about 0.117. And so you add and subtract that from the 26.5%. You end up with a confidence interval of 3.6% to 49.4%. So indeed that confidence interval does not cross zero. So we're 95% sure that the true value is not zero. So in other words, we can clearly reject the null hypothesis that the two groups are the same. ", ['Times', 'error', 'error', 'proportions', 'group', 'minus', 'treatment', 'group', 'Plus', 'times', 'minus', 'placebo', 'group', 'math', 'Standard', 'error', 'subtract', '%', 'confidence', 'interval', '%', '%', 'confidence', 'interval', '%', 'sure', 'value', 'words', 'null', 'hypothesis', 'groups']), 0.4181666987914534, 0.30173804618247296)
((9, '/Users/jag/Downloads/Stanford medstats/Unit 7 Homework with Answers.srt', "then we do this for the next person. The next person has a value of 0.5, subtract from the mean square of that. We're going to do that for all 13. I won't write it out completely here. You're then going to divide by n-1. For the standard deviation, you'll always divide by n-1, so that ends up being 13-1 or 12 here. And then we would square-root that whole thing. When you calculate that out, you get a value of 0.83. To calculate the standard error of the change here, we would take this. Standard deviation for change which we calculated already as 0.83 and divided by the square root of N. So, 0.83 divided by the square root of 13 is 0.23. That would be our standard error here. To calculate the 95% confidence interval, we're going to take the observed mean, which we already calculated to to be 0.77. We're going to add and subtract our t value here, for a 99% confidence interval I gave you, is 3.055. ", ['person', 'person', 'value', 'subtract', 'square', 'deviation', 'ends', 'thing', 'value', 'error', 'change', 'Standard', 'deviation', 'change', 'root', 'N.', 'So', 'root', 'error', '%', 'confidence', 'interval', 't', 'value', '%', 'confidence', 'interval']), 0.34992710611188266, 0.29623904655741745)
((5, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 8 mod5.srt', "eating disorders scored, just as before. But now I can take this beta coefficient and I can exponentiate it and what I get out are these relative risks, these risk ratios, rather than odds ratios. And if I had other things in the model, I would be getting out adjusted risk ratios. I am going to use the beta coefficient this computer output also gave me the 95% confidence interval for this beta coefficient, if I exponentiate these, I will get the 95% confidence interval for the odds ratio. I mean starting for the risk ratio. So, let me just do that now, so I'm going to exponentiate the 0.0653. When I exponentiate that, I get that the point estimate for my odds ratio is one, sorry for my risk ratio is 1.067. So this is an incidence rate ratio of course, in this case we're talking about cross sectional data. So it's really technically called a prevalence ratio, so you could interpret this as, for every one unit increase in the eating disorder score. ", ['disorders', 'beta', 'coefficient', 'risks', 'risk', 'ratios', 'odds', 'ratios', 'things', 'model', 'risk', 'ratios', 'beta', 'coefficient', 'computer', 'output', '%', 'confidence', 'interval', 'beta', 'coefficient', '%', 'confidence', 'interval', 'odds', 'risk', 'ratio', 'point', 'estimate', 'odds', 'risk', 'ratio', 'incidence', 'rate', 'ratio', 'course', 'case', 'cross', 'data', 'prevalence', 'ratio', 'unit', 'increase', 'eating', 'disorder', 'score']), 0.3674234614174767, 0.2930428205097133)
((0, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod2.srt', "In this next module, I'm going to talk more about the odds ratio. So I'm going to tell you how you get a p value and a confidence interval from a simple two by two table odds ratio. And I think that's useful to know. Because in fact understanding the shape of the distribution of an odds ratio as well as the standard error, will give you some insight into the measure itself. And I'm also going to tell you a little bit more about this problem of interpreting odds ratios correctly. So, first of all you may be asking yourself why do we ever even use odds ratios when they're so confusing and there's actually two reasons for. So one reason is that when you do a case control study you go out and you sample people based on whether or not they already have a disease. So you may go out and sample 50% of your sample may be cases and 50% of your sample may be controls. When you sample on disease status you can no longer validly estimate things like the risk of the disease or the prevalence of the disease. ", ['module', 'odds', 'p', 'value', 'confidence', 'interval', 'odds', 'fact', 'understanding', 'shape', 'distribution', 'odds', 'error', 'insight', 'measure', 'bit', 'problem', 'odds', 'ratios', 'odds', 'ratios', 'reasons', 'reason', 'case', 'control', 'study', 'people', 'disease', '%', 'sample', 'cases', '%', 'sample', 'controls', 'disease', 'status', 'longer', 'things', 'risk', 'disease', 'prevalence', 'disease']), 0.41079191812887456, 0.2876723355023862)
QUERY:  Your simulation creating a normal distribution for the null hypothesis where the mean is 100 - how did you synthesise the data when you don't know the standard deviation of the population? The variability of vitamin D levels could be larger or smaller than you've predicted, right, changing the shape of your distribution and the p value?  I must have missed something. Did you actually use the SD of your own data (mean = 63 or whatever it was)? I assume not because that doesn't seem valid...
*************************
((3, '/Users/jag/Downloads/Stanford medstats/Unit 5 Module 4 part 1.srt', "levels went up and cognitive function went down, and anything in between positive one and zero or negative one and zero is some kind of in between amount of correlation. So 0.15 is pretty close to zero so it's a small, it's a very weak correlation. But the correlation coefficient in the sample of a hundred came out to be 0.15 here. So how would I figure out, what's the distribution of the mean here? So we're going to use the mean Vitamin D as my example. What's the distribution of the mean? How can I figure that out? Well, I'm first going to figure it out by running a computer simulation. So, here's how the computer simulation works. I'm going to tell the computer that there's some large population of men European men. And I'm going to tell the computer the distribution of vitamin D in this large population. And I'm going to tell the computer it's a right skewed distribution. So vitamin D follows a right skewed distribution. It has a standard deviation of 33. And let's say that the true mean value in the the population is 62 [INAUDIBLE] ", ['levels', 'function', 'went', 'anything', 'kind', 'amount', 'correlation', 'correlation', 'correlation', 'coefficient', 'sample', 'distribution', 'Vitamin', 'D', 'example', 'distribution', 'Well', 'running', 'computer', 'simulation', 'computer', 'simulation', 'works', 'computer', 'population', 'men', 'men', 'computer', 'distribution', 'vitamin', 'D', 'population', 'computer', 'right', 'distribution', 'vitamin', 'D', 'right', 'distribution', 'deviation', 'say', 'mean', 'value', 'population', 'INAUDIBLE', ']']), 0.4175956904881631, 0.36722708197496945)
((7, '/Users/jag/Downloads/Stanford medstats/Unit 5 Module 4 part 1.srt', "The mean of the means is the true mean in the population. Well that makes sense because if that's the true mean, then most of our samples should hover around 62, the true mean. The standard deviation of the means turns out to be 3.3 nanomoles per liter. Now that's interesting, you remember that the, the standard deviation of the trait was 33 nanomoles per liter [SOUND] but the standard deviation of the means which we also called the standard error of the mean turns out to be 3.3. It's exactly 33 divided by 10. Well, the sample size here was 100, so what we're actually doing is, we're dividing the standard deviation by the square root of n, and that's what gives us the 3.3. This 3.3, I didn't get by a mathematical theory though. I just had the computer calculate the standard deviation from these 10,000 observations. And it turns out to be 3.3. So, the distribution of mean vitamin D that I got from this computer simulation. ", ['means', 'mean', 'population', 'Well', 'sense', 'mean', 'samples', 'mean', 'deviation', 'means', 'nanomoles', 'liter', 'deviation', 'trait', 'nanomoles', 'liter', 'SOUND', 'deviation', 'means', 'error', 'turns', 'Well', 'sample', 'size', 'deviation', 'root', 'n', 'theory', 'computer', 'calculate', 'deviation', 'observations', 'distribution', 'vitamin', 'D', 'computer', 'simulation']), 0.34247475971078656, 0.35083671495034924)
((8, '/Users/jag/Downloads/Stanford medstats/Unit 5 Module 4 part 1.srt', "I see that it's normally distributed. The mean is equal to the true mean. And the standard deviation, or what we also call the standard error, is 3.3 nanomoles per liter. I got this all out of the computer simulation. But the distribution of a sample mean, in general, people have long ago worked out with mathematical theory. And it turns out that if you're talking about sample means, means that you calculate from data, they follow, actually they don't exactly follow a normal distribution. If you want to know more about why that watch the optional module on the Central Limit Theorem. It turns out they follow a T-distribution, remember that a T-distribution is essentially the same as a normal distribution as long as you're talking about n's sample sizes at least of 100. So since we have a sample size of over 100, essentially it's, it's on a normal distribution. This means that if you're talking about sample means and you're talking about small samples, actually small samples you have to worry about it being on a T-distribution. You'll have slightly fatter tails than the normal. So it's not exactly, means don't exactly follow a normal distribution for ", ['mean', 'deviation', 'error', 'nanomoles', 'liter', 'all', 'computer', 'simulation', 'distribution', 'sample', 'mean', 'people', 'theory', 'sample', 'means', 'data', 'distribution', 'watch', 'module', 'Central', 'Limit', 'Theorem', 'T-distribution', 'remember', 'T-distribution', 'distribution', 'n', 'sample', 'sample', 'size', 'distribution', 'sample', 'means', 'samples', 'samples', 'T-distribution', 'tails', 'distribution']), 0.33541019662496846, 0.3321293936925737)
((5, '/Users/jag/Downloads/Stanford medstats/Unit 1 Module 5 part 2.srt', "you subtract 0.6, that's a value of 0.4. You then square it, and there's 30 people that have that identical value. So we can just do that once, and multiple by 30. If you're a 0, if didn't play varsity sports, then your deviation is negative 0.6. You square that, and there's 20 of those, so we multiple that by 20. We add all those up, and divide by 50 minus 1 n minus 1. And then we square root it to get back to the standard deviation, we come out with a value of 0.49. So what does that 0.49 mean? Well remember the, the average here is 0.6. Everybody is either 0 or a 1. So you're either a 0.6 away from the mean, or 0.4 away from the mean. So, you know, 0.49 is kind of the middle of those. Just to help you understand standard deviation even a little better, let me just represent to you three simple little mock data sets. And they all have the same mean of 15. They're just a couple, there's just 8 people in each of these data sets. But they all have the same mean. But they have different standard deviations. So in the first data set, data set B, over to the left of your screen. ", ['value', 'people', 'value', 'varsity', 'sports', 'deviation', 'minus', 'n', 'minus', 'root', 'deviation', 'value', 'Well', 'remember', 'Everybody', 'away', 'away', 'middle', 'deviation', 'simple', 'mock', 'data', 'sets', 'mean', 'couple', 'people', 'data', 'mean', 'deviations', 'data', 'data', 'B', 'left', 'screen']), 0.3916747259003201, 0.32417882694713035)
((2, '/Users/jag/Downloads/Stanford medstats/Unit 1 Module 3 part 2.srt', "be talking about this in great detail in a couple of weeks. So it turns out that statistics, things we measure from our data like means and odds ratios and regression coefficients. Those also have a distribution. And it's a tricky concept because, if you think, like I'm, I do a study. I measure one mean in my sample. How does a single mean have a distribution? Well, it's something that is a theoretical construct. But, it turns out that that distribution if often a normal distribution. And, that's really the reason that the normal distribution is so so important in statistics. So I'm going to end here with just showing you some histograms of the data from my Stanford students. I think it's always fun to look at histograms of real data. So for example, I asked my students to rate their optimism on a scale from 0 to a 100. And you can see that, you know, most of my students are feeling pretty optimistic when they took this survey. I did have a handful. Out to the left, we would describe this as having a bit of a left skew. ", ['detail', 'couple', 'weeks', 'statistics', 'things', 'data', 'means', 'odds', 'ratios', 'regression', 'coefficients', 'distribution', 'concept', 'study', 'sample', 'mean', 'distribution', 'Well', 'something', 'construct', 'distribution', 'distribution', 'reason', 'distribution', 'statistics', 'histograms', 'data', 'Stanford', 'students', 'histograms', 'data', 'example', 'students', 'optimism', 'scale', 'students', 'survey', 'handful', 'left', 'bit', 'left', 'skew']), 0.375520472144995, 0.3206709854120865)
((4, '/Users/jag/Downloads/Stanford medstats/Unit 5 Module 4 part 1.srt', "nanomoles per liter. It doesn't matter too much what I pick there. But lets say I'm going to pick the value of 62. So I tell the computer, this is the population you're sampling from. And then I have the computer sample a random sample of 100 virtual men from this underlying population. Each of those men is going to have a vitamin D level. I take those vitamin D levels and I average them to calculate the mean. Vitamin D level for that sample of 100. Then I'm going to repeat the experiment over and over and over again. Again this is the though experiment. I'm going to select a different random sample of 100 men, calculate their mean. Then I'm going to do that again and again and again, to get a sense of the distribution of the means. I've arbitrarily decided to do that 10,000 times. Again, 10,000's a pretty large number but not so large that it will hang my computer. Then, then we can explore the distribution of those 10,000 means. We can actually look at the distribution of the statistic here and the statistic here is a sample mean. [SOUND] So here's the results of this computer simulation. ", ['nanomoles', 'liter', 'lets', 'value', 'computer', 'population', 'computer', 'sample', 'random', 'sample', 'men', 'population', 'men', 'vitamin', 'D', 'level', 'vitamin', 'D', 'levels', 'Vitamin', 'D', 'level', 'sample', 'experiment', 'experiment', 'random', 'sample', 'men', 'sense', 'distribution', 'means', 'times', 'pretty', 'number', 'computer', 'distribution', 'means', 'distribution', 'sample', 'mean', '[', 'results', 'computer', 'simulation']), 0.30906126888455027, 0.31573425494858454)
((0, '/Users/jag/Downloads/Stanford medstats/Unit 4 module 2 part 1.srt', "In this next module we're going to talk about the concept of expected value which is just a fancy word for mean which is a concept we've already alked about in this course but the expected value is a really great tool for making good decisions so we're going to spend a lot of time on expected value. So what is expected value? Again, it's just the mean. It's the mean of a probability distribution. And the way that we calculate this mean is slightly different than the way that we calculated the mean when we had data. So when we had data, we calculated the mean by adding up everybody's values, and dividing by n. That doesn't quite work when we have a probability distribution. Because, for a probability distribution, certain outcomes occur more frequently than other outcomes. And, we have to factor that in. So, what we're doing is we're actually weighting the value of each outcome by it's probability. ", ['module', 'concept', 'value', 'fancy', 'word', 'concept', 'course', 'value', 'tool', 'decisions', 'lot', 'time', 'value', 'value', 'probability', 'distribution', 'way', 'mean', 'way', 'data', 'data', 'adding', 'everybody', 'values', 'n.', 'That', 'work', 'probability', 'distribution', 'probability', 'distribution', 'outcomes', 'outcomes', 'factor', 'value', 'outcome', 'probability']), 0.32986397342519147, 0.3026623749749283)
((10, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod3.srt', "we're going to expect to see about 26 or 27 in each cell. Instead we're seeing here a 37 to 16 split, and we say, well, is that discrepant enough from 26 to Is that a big enough discrepancy from the null hypothesis, where we're sure that that sort of rises above just random chance? How do we evaluate that? Well we can recognize here that what we are, we have under the null hypothesis, we are on a binomial distribution, and that binomial distribution has an n of 53 because I have 53 pairs, and we're expecting half of them, so my p is 0.5. We're expecting half of them to have a diabetic case rather than a diabetic control. So our null hypothesis here is that we're on a binomial with an n of 53 and a p of 0.5. Now, how do we then calculate a p-value for this null hypothesis? So what we would have to do, if we wanted to just stick with the binomial distribution here, is we would have to calculate the probability of what we observed, and everything more extreme. ", ['cell', 'split', 'Is', 'discrepancy', 'null', 'hypothesis', 'sort', 'rises', 'random', 'chance', 'Well', 'null', 'hypothesis', 'distribution', 'distribution', 'n', 'pairs', 'half', 'p', 'half', 'case', 'control', 'hypothesis', 'n', 'p', 'null', 'distribution', 'probability', 'everything']), 0.33995005182504245, 0.2924035173601261)
QUERY: the solution video for HW4 did not show how to get p value from z score using Standard normal distribution table. I also missed 3,4,10 because of the table. I did not read the explanation of the table, instead read the table automatically as it is. For the Q10. z value=1.7. from the table I read 0.4554 then 45%. But the value 0.4554 did not indicate the area under curve of right side of z value. it indicates only the area between 0 to 1.7. So we have to calculate p=(z>1.7). 0.5-0.4554= 0.0446. 4.46%.   But correct answer is 4.45, and explanation 4.55(?)
*************************
((4, '/Users/jag/Downloads/Stanford medstats/Unit 4 Homework with questions.srt', "So there is a 16.9% chance that out of the next 5 births exactly 4 are boys. Question 6 has approximately normal distribution with a mean of 80 and a standard deviation of 10. We need to find. The value of x such that we have 25 percent of the normal distribution area to the right of x. So the first step here would be to find the z square corresponding to 25% in the right tail and we can look this up in our z table and we'll see that the z squared that we would be looking for is 0.674. Now, it's a simple job of plugging this into the formula for z, we know that z is x minus mu by standard deviation. We can flip this around make x the subject of the formula and say that x is z times standard deviation plus mu. ", ['%', 'chance', 'births', 'boys', 'Question', 'distribution', 'deviation', 'value', 'x', 'percent', 'distribution', 'area', 'right', 'x', 'step', 'z', 'square', '%', 'right', 'tail', 'up', 'z', 'table', 'z', 'job', 'formula', 'z', 'z', 'x', 'minus', 'mu', 'deviation', 'around', 'x', 'subject', 'formula', 'x', 'z', 'times', 'deviation', 'mu']), 0.46574757829710844, 0.44399553696199595)
((14, '/Users/jag/Downloads/Stanford medstats/Unit 4 module 5 part 1.srt', "So, I'm asking for the area to the right of 141, that small area in the tail. If I draw the picture then I know the probability that I'm going to get out at the end of the day better be small. If it's not, I know I'm going to have done something wrong. Now I need to convert this into a z score. So the z score here is I'm going to say, well 141 is 2.46 standard deviation above the mean. How did I get that? I subtracted 141 minus 109. That gives me the distance above the mean then I divide it by 13 to get it in standard deviation units. So my z is 2.46. That's pretty far in the tail of a normal distribution. I'm 2.46 standard deviations above the mean. What's the chance that a random baby is 2.46 standard deviations above the mean or even higher than that? So that's, I'm looking for the area to the right of z equals 2.46. So I can go to my standard normal chart. First of all, I need to find the z for 2.46. So i'm going to find, the z for 2.46. Again, I get that by doing the row is 2.4. ", ['area', 'right', 'area', 'tail', 'picture', 'probability', 'end', 'day', 'better', 'something', 'z', 'score', 'z', 'score', 'deviation', 'minus', 'distance', 'deviation', 'units', 'z', 'tail', 'distribution', 'deviations', 'chance', 'random', 'baby', 'deviations', 'area', 'right', 'z', 'equals', 'standard', 'chart', 'First', 'z', 'z', 'row']), 0.44067884452014405, 0.39488118507139036)
((2, '/Users/jag/Downloads/Stanford medstats/Unit 4 Homework with questions.srt', "out the probability Cause it's pointing to z value of minus 1.5. You see that a probability is 0.0668. One thing to note here is that we should be referring to the one tail z values and not the 2 tail z values because we only want to find the probability That this we have values which are less than .85. We don't want to find the probability that we have values greater than one standard deviation about the mean. In the previous question, we found the area of the curve to the left. Of .81, .85. This question wants us to find the area to the right of 1.15. We could argue that because the normal distribution is symmetric, the area that we found in the previous question would be exactly the area that we would find in this question as well. But let us just go through the numbers. To confirm that that is the case. In this case, the z value is [SOUND] and ", ['probability', 'Cause', 'value', 'minus', 'probability', 'thing', 'tail', 'z', 'values', 'tail', 'z', 'values', 'probability', 'values', 'probability', 'values', 'deviation', 'question', 'area', 'curve', 'left', 'question', 'area', 'right', 'distribution', 'area', 'question', 'area', 'question', 'numbers', 'case', 'case', 'z', 'value', 'SOUND']), 0.34129458603944157, 0.3650590501771553)
((15, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod3.srt', "I'm 10.5 away from the expected, and my standard deviation turns out to be 3.64. My z value comes out to be 2.88. Now clearly, if you know anything about z values, clearly a two point, a z value of 2.88 is going to be out, way out in the tail. The p value is going to be quite small, ao it will be statistically significant. So that's much easier, doing that approximation is much easier than doing out the entire binomial probability. Now McNemar was clever here, because he recognized that there's an easy pattern, that all of these situations where you have paired data in a two by two table like this, they all are going to come out to be exactly the same binomial and the exactly, and we're going to, we can generalize it. So we can actually figure out a general formula that can be applied every time. So that is, you don't have to go through that logic that I just went through every time. At the end of all this, I'm going to give you a simple little formula that avoids that logic. So, I'm now going to generalize. Rather than plugging in the 53 and the 37, et cetera, ", ['standard', 'deviation', 'z', 'value', 'anything', 'z', 'values', 'point', 'z', 'value', 'way', 'tail', 'p', 'value', 'ao', 'easier', 'approximation', 'easier', 'probability', 'McNemar', 'pattern', 'situations', 'data', 'formula', 'time', 'time', 'end', 'formula', 'avoids', 'logic', 'et', 'cetera']), 0.35688712648767756, 0.35920610395885055)
((5, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod1.srt', "So I, I want you now to go ahead and run the hypothesis test. Calculate the z score. That z score should, of course, come out ca-, correlating to a p value of 0.029. Calculate the z score here for the difference in proportions. I actually want you to pause the video and try this on your own. And then restart the video and I'll walk you through it again. I'm assuming that this is largely review for most of you. Alright, so this is fairly straight forward. Again, we are just going to apply that formula where we are calculating a z score based on our observed difference. So our observed difference is 62.9% minus 36.4%. I'm putting the larger group first, just that I don't end, end up with a negative. It doesn't matter whether you make it negative or positive. So I'm avoiding dealing with the negative. And then my standard error is going to be p times 1 minus p. Again I'm going to use the pooled proportion. Just half the infants got eczema overall. So that's 0.5 times 1 minus 0.5 divided by the sample size in one group ", ['hypothesis', 'test', 'Calculate', 'z', 'score', 'z', 'score', 'course', 'p', 'value', 'Calculate', 'z', 'score', 'difference', 'proportions', 'video', 'try', 'video', 'review', 'Alright', 'forward', 'formula', 'z', 'score', 'difference', 'difference', '%', 'minus', '%', 'group', 'end', 'standard', 'error', 'p', 'times', 'minus', 'p.', 'Again', 'I', "'m", 'proportion', 'infants', 'eczema', 'overall', 'times', 'minus', 'sample', 'size', 'group']), 0.3610513976905242, 0.3463963333367448)
((1, '/Users/jag/Downloads/Stanford medstats/Unit 4 module 5 part 2.srt', "of 90%. So, the ninetieth percentile for math SAT scores is going to be somewhere over here. That means that 90% of people are lower. So that means our area to the left is 90%. So we're going to go to the z chart. We're going to find the area of 90% and work backward to the z-score. So if I go to my z chart and I just have this, I literally have to scan around with my eyes until I find 0.90. And you'll notice that it comes 0.88, 8997, you also get 0.9015. 0.8997 is the closest to 0.90, so I'm just going to pick that one. So, that corresponds to what z-score, if you scroll over, it's a 1.28. So that corresponds to a z-score of a 1.28. So, in order to have 90% area and to the left, you need a z-score of 1.28. So to be in the ninetieth percentile for SAT scores, you need to be 1.28 standard deviation above the average. So what is 1.28 standard deviations above the average if I want to ", ['%', 'ninetieth', 'percentile', 'math', 'SAT', 'scores', '%', 'people', 'means', 'area', 'left', '%', 'z', 'chart', 'area', '%', 'backward', 'z', 'chart', 'eyes', 'corresponds', 'corresponds', 'order', '%', 'area', 'left', 'ninetieth', 'percentile', 'SAT', 'scores', 'deviation', 'average', 'deviations', 'average']), 0.3117146000796786, 0.3293584792346119)
((6, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod1.srt', "plus 0.5 times 1 minus 0.5 divided by the sample size in the second group. If you just run through the actual math on that, crank that into a calculator, you'll see that the z score comes out to be 2.18. We call know that if the z score's above 1.96. That's going to be statistically significant at the point of five level. Indeed, if you go to a z calculator, you will, in fact, find that the p value that corresponds to a z value of 2.18 is 0.029, exactly what the authors reported in their paper. That's the two tail p value, and of course, we're going to want to use the two tail p value here, because there's no reason. That the probiotics couldn't of actually increased the risk of eczema that could, that's biologically possible. Now, I'm just going to show you some output from a computer program. I plugged these data into SAS and just to show you what you get out of SAS, the key value that's associated with a difference in proportion. Now notice that where I'm actually getting this out of this SAS output point is chi-square. ", ['times', 'minus', 'sample', 'size', 'group', 'math', 'crank', 'calculator', 'z', 'score', 'z', 'score', 'point', 'level', 'z', 'calculator', 'fact', 'p', 'value', 'z', 'value', 'authors', 'paper', 'tail', 'p', 'value', 'course', 'tail', 'p', 'value', 'reason', 'probiotics', 'risk', 'eczema', 'output', 'computer', 'program', 'data', 'SAS', 'SAS', 'value', 'difference', 'proportion', 'SAS', 'output', 'point']), 0.4129483209670112, 0.3268318311751501)
((3, '/Users/jag/Downloads/Stanford medstats/Unit 4 Module 6 part 1.srt', "of course you're never going to want to do that by hand. Of course, a computer could also do it for you nowadays. But in general we, we're just going to avoid that and instead we're going to say well hey I'm close enough to a normal curve. So, let's just pretend we're on a normal curve with a mean of 125. That's 500 times a quarter. And a standard deviation of the square root of n times p times 1 minus p here is 9.68. [SOUND] So, I take the standard deviation the mean from the binomial, and I pretend I'm on a normal. And now, if I want to know what's the chance of getting 120 or less, well that's the, the area to the left on the normal curve, of 120. Now that's below the mean, so we know that that area's going to be less than 50%. I calculate my z score. It turns out to be negative 0.52. I go to my chart, I look up the probability to the left of negative 0.52. It's about 30%. So I have a 30% chance of getting 120 cases more or less. So rather than having to calculate all of those individual binomial probabilities, ", ['course', 'hand', 'course', 'computer', 'curve', 'curve', 'times', 'quarter', 'deviation', 'root', 'n', 'times', 'times', 'minus', 'p', '[', 'deviation', 'chance', 'area', 'left', 'curve', 'area', '%', 'z', 'score', 'chart', 'probability', 'left', '%', '%', 'chance', 'cases', 'probabilities']), 0.27904991033687215, 0.32217110917009595)
QUERY: Hello Seema,  If we know that the 95% CI for an odds ratio crosses/does not cross the null, what conclusions can we make about the p-value we should observe? We are interest in a 95% CI so we know that our alpha = 0.05. Would a p=0.05 be significant?
*************************
((2, '/Users/jag/Downloads/Stanford medstats/Kaplan Meier Curves 2  How to Read Them.srt', "that, this result would occur under the null hypothesis. Null hypothesis, what is that? The null hypothesis is, the hypothesis that nothing is going on. So, the P-value is, .34%, of that, there's a .34% likelihood that what you see here would have occur with Oxaliplatin actually having added nothing to the 5Fu Leucovorin, so the P-value is pretty low so it looks like this is highly statistically significant. The usual cutoff for clinical trials is that something is significant if it ", ['result', 'null', 'hypothesis', 'Null', 'hypothesis', 'null', 'hypothesis', 'hypothesis', 'nothing', '%', '%', 'likelihood', 'Oxaliplatin', 'nothing', 'Leucovorin', 'cutoff', 'trials', 'something']), 0.21821789023599236, 0.2079703880206771)
((2, '/Users/jag/Downloads/Stanford medstats/Kaplan Meier Curves 2 How to Read Them.srt', "[SOUND] that, this result would occur under the null hypothesis. Null hypothesis, what is that? The null hypothesis is, the hypothesis that nothing is going on. So, the p-value is 0.34%. That there's a 0.34% likelihood that what you see here would have occurred with Oxaliplatin, actually having added nothing to the 5FU and Leucavorin. So the p-value is pretty low, so it looks like this is highly statistically significant. The usual cut off for clinical trials is that ", ['[', 'result', 'null', 'hypothesis', 'Null', 'hypothesis', 'null', 'hypothesis', 'hypothesis', 'nothing', '%', '%', 'likelihood', 'Oxaliplatin', 'nothing', 'Leucavorin', 'cut', 'trials']), 0.21821789023599236, 0.2079703880206771)
((6, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 8 mod5.srt', "You have a 6.7% increase in your prevalence of or risk of menstrual irregularity. We can also do the 95% confidence limits, so if I exponentiate those confidence limits from the previous page, I get a lower bound of about 0.04, 1.04. And an upper bound of about 1.096. So that can be interpreted as the adjusted risk ratio. You don't need to worry about this business about odds ratios being misleading, you can avoid them altogether. And just to compare, remember when we did the odds ratio from logistic regression, what we got there was point estimate of 1.12 and a 95% confidence interval of 1.06 to 1.1 about 1.19. So the width of the confidence interval is similar here but these are all inflated. ", ['%', 'increase', 'prevalence', 'irregularity', '%', 'confidence', 'limits', 'confidence', 'limits', 'page', 'bound', 'bound', 'risk', 'ratio', 'business', 'odds', 'ratios', 'remember', 'odds', 'regression', 'point', 'estimate', '%', 'confidence', 'interval', 'width', 'confidence', 'interval']), 0.23756554836659946, 0.19843096932931772)
((11, '/Users/jag/Downloads/Stanford medstats/Unit 2 Homework with questions.srt', "So let me write than in, 31.5, 31.5, okay? And then we have this probability in the reference group, variable. And that's actually just going to be 29%. Okay, so it's the same thing. It's, it's we're still looking at this category of data. The reference group is the placebo group. And the probability it represented by this percentage. So we're going to write that in. So this is going to be 29%, 29% or 0.29 right and then, when we do add this calculation we see that the adjusted, risk ratio is, is 3.2 and of course this is the adjusted risk ratio because we're using the adjusted RR. To calculate this and to make this conversion. The last question in this homework set is asking us what the best way would be to represent this very large odds ratio here, 57.7. ", ['let', 'okay', 'probability', 'reference', 'group', '%', 'Okay', 'thing', 'category', 'data', 'reference', 'group', 'placebo', 'group', 'probability', 'percentage', '%', '%', 'right', 'calculation', 'ratio', 'course', 'risk', 'ratio', 'RR', 'conversion', 'question', 'homework', 'way', 'odds']), 0.22047927592204922, 0.18863574194708962)
((23, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod1.srt', "there are lots of odds ratios floating around in the literature and we have to be very careful again about how we look at these. The reason the odds, this was a randomized trials so, arguably, the authors probably shouldn't have needed to adjust for confounding but perhaps they've adjusted for antibiotic use. Breastfeeding and caesarean section perhaps those were slightly imbalanced in these two groups since it was a small randomized trial. When they do the adjustment the odds ratio goes down slightly so the unadjusted odds ratio was 0.34. It goes down to 0.24 when you adjust for these other things. The p value is about the same as before. So we end up with an odds ratio and again you have to be careful about you interpret this. So, you wouldn't want to look at this odds ratio and say, oh, well that means that after adjusting for these confounders there was a 76% decrease in the risk of eczema. This is a 76% decrease in the odds of getting eczema. Which, again, is a really hard thing for us to understand intuitively. All you have to do is look at the risks. It's really a halving of risk, right? The risk goes down by about half, from about 63% to 36%. ", ['lots', 'odds', 'ratios', 'literature', 'reason', 'odds', 'trials', 'authors', 'confounding', 'use', 'section', 'groups', 'adjustment', 'odds', 'odds', 'things', 'p', 'value', 'odds', 'odds', 'means', 'confounders', '%', 'decrease', 'risk', 'eczema', '%', 'decrease', 'odds', 'eczema', 'thing', 'risks', 'halving', 'risk', 'risk', '%', '%']), 0.3257023745309605, 0.18714449167259412)
((8, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod1.srt', "Times the standard error. The standard error here. Again, I'm going to plug in just the observed proportions in each group. So, 1 minus 0.364 and the treatment group divided by 33. Plus 0.629 times 1 minus 0.629, divided by 35 for the placebo group. If you crank out the math on that, this. Standard error comes out to be about 0.117. And so you add and subtract that from the 26.5%. You end up with a confidence interval of 3.6% to 49.4%. So indeed that confidence interval does not cross zero. So we're 95% sure that the true value is not zero. So in other words, we can clearly reject the null hypothesis that the two groups are the same. ", ['Times', 'error', 'error', 'proportions', 'group', 'minus', 'treatment', 'group', 'Plus', 'times', 'minus', 'placebo', 'group', 'math', 'Standard', 'error', 'subtract', '%', 'confidence', 'interval', '%', '%', 'confidence', 'interval', '%', 'sure', 'value', 'words', 'null', 'hypothesis', 'groups']), 0.2514594403022007, 0.18644044910161875)
((21, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod2.srt', "Notice that's going to distort the odds ratio a little bit. It's going to be closer to, you know, like 3.5, but not a huge amount. And you can kind of see that within this zone of 10% prevalence, most of the alterations and restorations are pretty close to one another. However, if you go out to prevalence as if, say, 30%, and you go out to big odds ratios like 2.5, 3.0, you can get odds ratios that are huge. So for example, with a prevalence of 30%, and a risk ratio of three, your odds ratio might be something, you know, like 15. So huge. A, a big distortion. So you're going to be careful with common outcomes and also the bigger. The effect actually is the bigger this distortion will be, too. So, I'm just going to give you a few examples of where you have to be a little careful about these types of things that pop up in the literature. I just happened to be looking at something the other day that somebody sent me from an, this was in their abstract, and I've changed things a little for anonymity here. But basically the abstract said the outcome was twice as likely, odds ratio is 2.2. To occur in the experimental group compared with the standard group. ", ['Notice', 'odds', 'bit', 'amount', 'see', 'zone', '%', 'prevalence', 'alterations', 'restorations', '%', 'odds', 'ratios', 'odds', 'ratios', 'example', 'prevalence', '%', 'risk', 'ratio', 'odds', 'something', 'distortion', 'outcomes', 'effect', 'distortion', 'examples', 'types', 'things', 'pop', 'literature', 'something', 'day', 'somebody', 'sent', 'abstract', 'things', 'anonymity', 'abstract', 'outcome', 'twice', 'ratio', 'group', 'group']), 0.24708310555370036, 0.1822310422340567)
((6, '/Users/jag/Downloads/Stanford medstats/Unit 2 Homework with questions.srt', "regimen versus the three or more pill per day regimen. And so, if you remember the odds in odds is defined as the probability over one minus improbability so if you take 7.7 as the probability of hospitalization. For the single pill per day regimen group you can represent the odds at 7.7% over 1 minus 7.7% okay and we're going to do that with the three or more pill per day regimen group. So it's 9.9% over 1 minus 9.9%. Okay, and this is going to give us the odds ratio. So if we simplify this, we're going to get .76. The odds ratio here is .76. So let's take a look at this third table presented to us here. On homework two. So there are two groups here. ", ['regimen', 'pill', 'day', 'regimen', 'odds', 'odds', 'probability', 'minus', 'improbability', 'probability', 'hospitalization', 'pill', 'day', 'regimen', 'group', 'odds', '%', 'minus', '%', 'okay', 'pill', 'day', 'regimen', 'group', '%', 'minus', '%', 'Okay', 'odds', 'odds', 'let', 'take', 'look', 'homework', 'groups']), 0.2822753912601658, 0.17983850457717593)
QUERY: Here the null value is not a single value; it is a range of 20 % or more. So, to reject the null hypothesis, I should select the confidence intervals that don't cross this range. Am I thinking on the right track?!!! And when observing the upper limit for the confidence interval in some years, I have this question: do 19.8 or 19.9 % make any difference from 20% to decide to reject null hypothesis???!
*************************
((2, '/Users/jag/Downloads/Stanford medstats/Kaplan Meier Curves 2  How to Read Them.srt', "that, this result would occur under the null hypothesis. Null hypothesis, what is that? The null hypothesis is, the hypothesis that nothing is going on. So, the P-value is, .34%, of that, there's a .34% likelihood that what you see here would have occur with Oxaliplatin actually having added nothing to the 5Fu Leucovorin, so the P-value is pretty low so it looks like this is highly statistically significant. The usual cutoff for clinical trials is that something is significant if it ", ['result', 'null', 'hypothesis', 'Null', 'hypothesis', 'null', 'hypothesis', 'hypothesis', 'nothing', '%', '%', 'likelihood', 'Oxaliplatin', 'nothing', 'Leucovorin', 'cutoff', 'trials', 'something']), 0.4914731871829905, 0.540977892408508)
((2, '/Users/jag/Downloads/Stanford medstats/Kaplan Meier Curves 2 How to Read Them.srt', "[SOUND] that, this result would occur under the null hypothesis. Null hypothesis, what is that? The null hypothesis is, the hypothesis that nothing is going on. So, the p-value is 0.34%. That there's a 0.34% likelihood that what you see here would have occurred with Oxaliplatin, actually having added nothing to the 5FU and Leucavorin. So the p-value is pretty low, so it looks like this is highly statistically significant. The usual cut off for clinical trials is that ", ['[', 'result', 'null', 'hypothesis', 'Null', 'hypothesis', 'null', 'hypothesis', 'hypothesis', 'nothing', '%', '%', 'likelihood', 'Oxaliplatin', 'nothing', 'Leucavorin', 'cut', 'trials']), 0.4914731871829905, 0.540977892408508)
((4, '/Users/jag/Downloads/Stanford medstats/Unit 5 Homework with answers.srt', "in terms of the odd ratio we get the confidence interval to be .87 to 8.66. The answer to 'F' is false. The confidence intervals of the laws of an odd ratio as symmetric as we just found but when we exponentiate the symmetry is lost. And the confidence interval of the actual odds ratio is not symmetric. The answer to G is true. The 95% confidence interval for the oz ratio crosses one, so we cannot reject the null hypothesis. And thus we know that b is greater 0.05. Part A of this question wants us to roughly estimate the two sided p value associated with this distribution. If we down the number of observations we have to the left of negative 25 and to the right of 25, we'll see they're about 25 observation out of a thousand that have ", ['terms', 'ratio', 'confidence', 'interval', 'answer', 'confidence', 'intervals', 'laws', 'ratio', 'symmetry', 'confidence', 'interval', 'odds', 'answer', 'G', '%', 'confidence', 'interval', 'oz', 'ratio', 'crosses', 'null', 'hypothesis', 'b', 'Part', 'question', 'p', 'value', 'distribution', 'number', 'observations', 'left', 'right', 'observation', 'thousand']), 0.4530721928946211, 0.5274840324324169)
((8, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod1.srt', "Times the standard error. The standard error here. Again, I'm going to plug in just the observed proportions in each group. So, 1 minus 0.364 and the treatment group divided by 33. Plus 0.629 times 1 minus 0.629, divided by 35 for the placebo group. If you crank out the math on that, this. Standard error comes out to be about 0.117. And so you add and subtract that from the 26.5%. You end up with a confidence interval of 3.6% to 49.4%. So indeed that confidence interval does not cross zero. So we're 95% sure that the true value is not zero. So in other words, we can clearly reject the null hypothesis that the two groups are the same. ", ['Times', 'error', 'error', 'proportions', 'group', 'minus', 'treatment', 'group', 'Plus', 'times', 'minus', 'placebo', 'group', 'math', 'Standard', 'error', 'subtract', '%', 'confidence', 'interval', '%', '%', 'confidence', 'interval', '%', 'sure', 'value', 'words', 'null', 'hypothesis', 'groups']), 0.47195020093189693, 0.4969809999720506)
((6, '/Users/jag/Downloads/Stanford medstats/Unit 5 Homework with answers.srt', "That it will simply be smoother because as we have more and more observations the center limit harem will start kicking in, and we will have a more and more normal looking distribution. When we have ten times more values, this distribution will look ten times smoother. The correct answer to 'D' is two. The sample size of the hypothetical trial is larger, which is the reason that the 'p' value is lower. This question gives us the percentage of US adults Who had five or more drinks in one day at least once in a given year. And we have trends for, let's see, we have trends for seven years. We have the mean value as well as the 95% confidence intervals given. The question wants us to find for which years we could reset the null hypothesis that more than 20% of US adults had five or more drinks in one day. At a 0.25 significance level. The answer is 2000 and 2003 only. ", ['observations', 'center', 'limit', 'harem', 'distribution', 'times', 'values', 'distribution', 'times', 'correct', 'answer', 'sample', 'size', 'trial', 'reason', 'value', 'question', 'percentage', 'US', 'adults', 'drinks', 'day', 'year', 'trends', 'see', 'trends', 'years', 'value', '%', 'confidence', 'intervals', 'question', 'years', 'null', 'hypothesis', '%', 'US', 'adults', 'drinks', 'day', 'significance', 'level', 'answer']), 0.41429674273976147, 0.48163521881566046)
((0, '/Users/jag/Downloads/Stanford medstats/Unit 6 Module 1 part 1.srt', "In this next module, I'm going to talk about the formal types of errors that can occur with hypothesis tests, and directly related to that, the concept of statistical power. Lets review from last week, the steps of a hypothesis test. First thing that you do is to define your null hypothesis, that's the strong man that you want to shoot down. Once you define the null hypothesis then you can predict the range of values for your statistics that you expect to see when you do an experiment if the null hypothesis is true. Then you got to do your experiment. You calculate that statistic If the statistic is within the range of values that your expecting based on the null hypothesis your P value the probability of your data will end up just a being something high in other words your experiment will sort of match, match with the null hypothesis. On the other hand, let's say you do the experiment and you get a value that's outside of what's expected, based on the null hypothesis. ", ['module', 'types', 'errors', 'hypothesis', 'tests', 'concept', 'power', 'Lets', 'week', 'steps', 'hypothesis', 'test', 'First', 'thing', 'null', 'hypothesis', 'man', 'null', 'hypothesis', 'range', 'values', 'statistics', 'experiment', 'null', 'hypothesis', 'experiment', 'range', 'values', 'null', 'hypothesis', 'P', 'value', 'probability', 'data', 'something', 'words', 'experiment', 'match', 'match', 'hypothesis', 'hand', 'say', 'experiment', 'value', 'null', 'hypothesis']), 0.4825491100403511, 0.41377914191083587)
((8, '/Users/jag/Downloads/Stanford medstats/Unit 8 Module 1 part 2.srt', "To, to, to do a hypothesis test, you'd simply be cam, comparing your observed r, your null value here is 0. You're comparing your observed r to a 0 divided the standard error for r. R divided by standard error gives you Z score, and you'd get an accompanying p value. To build a confidence interval, you take the observed value of r in your sample, and you'd add or subtract the, the confidence limit is usually that Z value's going to be 1.96 through 95% confidence interval. But if you wanted and to make 99% or 90% that Z value might change, times the standard error. And you be just plugging in here into the standard era the observed r, because we don't know the true r. That's for large N, for small n you'd have to worry about the fact that your on a T curve. You might have to worry for really small about the fact that it's an n minus 2 in the standard error. and again you would just build, you'd do a you would be comparing r to its standard error to get a t-statistic, or you would be building a confidence interval. All in the same way that we've talked about already. ", ['hypothesis', 'test', 'cam', 'r', 'null', 'value', 'r', 'error', 'r.', 'R', 'error', 'Z', 'accompanying', 'p', 'value', 'confidence', 'interval', 'value', 'r', 'sample', 'confidence', 'limit', 'Z', 'value', '%', 'confidence', 'interval', '%', '%', 'Z', 'value', 'times', 'error', 'era', 'r', 'r.', 'That', 'N', 'n', 'fact', 'T', 'curve', 'fact', 'n', 'minus', 'error', 'r', 'error', 'confidence', 'interval', 'way']), 0.4377890729616421, 0.40171663757502324)
((5, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit2 Mod4.srt', "So there's a couple of different statistics that we're going to talk about. So there's something called the Mantal-Haenszel test of independence. This is a chi-squared test. The null hypothesis here is that the predictor and the outcome are independent after controlling for or conditioning on the confounder. So once we adjust for program, is gender and, denial of admissions, is, are those independent? The null hypothesis is that they're independent, and when you do that statistic, it comes out to be a p value that is non-significant. So we have no evidence that those things are not independent. So we, we're not going to reject the null hypothesis there. It looks like they are unrelated. We can calculate the p value test, but then it's also nice to always be able to provide an affect size. So we're going to provide an affect size by calculating what's called a Maentel-Haenszel summary risk ratio. All this is, is it's taking the stratum-specific risk ratios that we just calculated and it's pooling them. And it comes out here to be 0.95. ", ['couple', 'statistics', 'something', 'Mantal-Haenszel', 'test', 'independence', 'test', 'null', 'hypothesis', 'predictor', 'outcome', 'confounder', 'program', 'gender', 'admissions', 'null', 'hypothesis', 'p', 'value', 'evidence', 'things', 'null', 'hypothesis', 'p', 'value', 'test', 'affect', 'size', 'affect', 'size', 'calculating', 'Maentel-Haenszel', 'summary', 'risk', 'ratio', 'risk', 'ratios']), 0.3474705540807411, 0.39142905747811974)
QUERY: Hi,  Just a little question, would a decision tree have told us that the feature which most distinguish between hazardness is US/Non-US?  Best regards Romeo
*************************
((6, '/Users/jag/Downloads/Stanford medstats/Unit 9.srt', "key decision, and if you have knowledge and experience and know what to do, then the third part of this two challenge rule is actually, you know, making it happen, providing it is safe. And for the patient and you do know what needs to be done, very precisely, so, that's the two challenge rule. The analogy that the airline industry makes reference to, is many times you have a pilot and a copilot, and they're dealing with all kinds of issues and events. At some time a pilot is so focused on what is going on, that sometime a question to cause a pause in what is going on, can be a rebooting or retriggering of the thought process that needs to be done to address the crisis situation on hand. The same could be said in an operating room, an ICU, and emergency department, that sometimes what it takes is that pause, that question. We've all seen the television shows and ", ['decision', 'knowledge', 'experience', 'part', 'challenge', 'rule', 'patient', 'challenge', 'rule', 'analogy', 'airline', 'industry', 'reference', 'times', 'pilot', 'copilot', 'kinds', 'issues', 'events', 'time', 'pilot', 'sometime', 'question', 'pause', 'rebooting', 'thought', 'process', 'address', 'crisis', 'situation', 'hand', 'operating', 'room', 'ICU', 'emergency', 'department', 'pause', 'question', 'television']), 0.12371791482634838, 0.11106351783311187)
((8, '/Users/jag/Downloads/Stanford medstats/Geriatric Ethics Discussion.srt', "And it's a first degree relative. An ad, an adult son of the patient.  Mm-hm.  Now, when you have a room full of, you know, 20 family members and some friends from high school and some other friends and maybe distant relatives and adopted children and whatnot.  Mm-hm.  How do you decide who's going to be the patient's surrogate decision maker if everyone can't agree on what should be done for the patient? [NOISE].  So again, a fabulous question, and I can't stress enough how variable this is. It really depends on what state you're practicing in. So here in California. Surrogate decision makers are generally signed by the medical team and or the ethics committee if we're consulted. And based on how long a person has known the patient, if a person has had conversations with the patient that might be relevant to the decision that has to be made. And basically if they can act in a way, what we call substituted judgement, if they can put themselves in the patients shoes and decide what the patient would want. ", ['degree', 'ad', 'adult', 'son', 'patient', 'Mm-hm', 'room', 'family', 'members', 'friends', 'school', 'friends', 'relatives', 'children', 'Mm-hm', 'patient', 'surrogate', 'decision', 'maker', 'everyone', 'patient', '[', 'question', 'state', 'California', 'Surrogate', 'decision', 'makers', 'team', 'ethics', 'committee', 'person', 'patient', 'person', 'conversations', 'patient', 'decision', 'way', 'judgement', 'patients', 'patient']), 0.12674485010489558, 0.09818328751510677)
((3, '/Users/jag/Downloads/Stanford medstats/Unit 4 Homework with questions.srt', "if we look up the probability distribution, [SOUND] we'll see that the p value corresponding to this Is 0.0668. Which is exactly the same value that we found in the previous question as well. This question is a straight-forward application of the binomial equation again. We have n equals 5. We are given that b is 0.513. And we want to find the probability Of x being 4. And this, recalling the expression that we had for the binomial distribution. And if we do the math here, we'll find that the answer is 16.9%. ", ['probability', 'distribution', 'SOUND', 'p', 'value', 'value', 'question', 'question', 'application', 'equation', 'equals', 'b', 'probability', 'x', 'expression', 'distribution', 'math', 'answer', '%']), 0.1111111111111111, 0.0940637322533303)
((13, '/Users/jag/Downloads/Stanford medstats/RExercise1 full.srt', "that means that I can specify what variables I want on what axis. But this question, it doesn't really matter. I'm just going to move exercise to the x axis, and homework to the y axis. The questions specifies that they want a smoothed curve. So I'm just going to leave that there. And I'm going to hit OK, great. And so now we see this relationship. So as you can see here the trend line is pretty flat. I wouldn't say there's a really dramatic trend at all. There might be a slight trend or small correlation between having a lot of homework and exercising less. But again, that would be a very weak effect here. And I don't notice any egregious outliers, as well. So, these are all things you sort of want to keep in mind as you look at these different plots. Moving on to question seven, question seven is asking us to create a histogram for the variable milk. ", ['variables', 'axis', 'question', 'exercise', 'x', 'homework', 'y', 'questions', 'curve', 'OK', 'relationship', 'trend', 'line', 'trend', 'slight', 'trend', 'correlation', 'lot', 'homework', 'effect', 'outliers', 'things', 'mind', 'plots', 'question', 'question', 'histogram', 'milk']), 0.1336306209562122, 0.09347962109368572)
((0, '/Users/jag/Downloads/Stanford medstats/Geriatric Anesthesia - Course Introduction.srt', "[BLANK_AUDIO] [MUSIC]  Hi, I'm Doctor Larry Chu.  And I'm Doctor Kevin Jenner.  And we're here today to talk to you about a new online course that we're going to be launching on geriatric anesthesia. And issues associated with it the perioperative care of elderly patients. Kevin, let's start off by just asking this question, let's get it out of the way. Are you going to be an expert in geriatric anaesthesia after completing this course? ", ['[', 'BLANK_AUDIO', ']', '[', 'MUSIC', ']', 'Hi', 'Doctor', 'Larry', 'Chu', 'Doctor', 'Kevin', 'Jenner', 'today', 'online', 'course', 'anesthesia', 'issues', 'care', 'patients', 'Kevin', 'start', 'question', 'get', 'way', 'Are', 'expert', 'anaesthesia', 'course']), 0.09245003270420486, 0.09245003270420486)
((17, '/Users/jag/Downloads/Stanford medstats/RExercise1 full.srt', "So the first three values for milk are 16, 0 and 4, first three variables for milkFixed are the first three values I should say, are 16, 0 and 4. So, you know, you should always check. What the, you should always check the composition of your variables after making a change like this. But for this exercise, I'm pretty satisfied that my recoding worked. Moving on to question eight, question eight is asking for the mean, the standard deviation. The median number of hours of sleep in this Dataset. So, that's a lot to ask for. but, there's a really handy function in this program that allows us to generate all three. So, what I'm going to do is go to Analysis, and then Descriptive, and then drag over Sleep. ", ['values', 'milk', 'variables', 'milkFixed', 'values', 'composition', 'variables', 'change', 'exercise', 'recoding', 'question', 'question', 'deviation', 'number', 'hours', 'sleep', 'Dataset', 'lot', 'function', 'program', 'Analysis', 'Descriptive', 'Sleep']), 0.1072112534837795, 0.09076221578017944)
((20, '/Users/jag/Downloads/Stanford medstats/RExercise1 full.srt', "And so, to get back to the question the questions specifically asks the mean score for those who indicated that they had five drinks. So I'm going to go to alcohol 5, mean optimism is 74.5. The next question asks us, how many people consider themselves street smart? Now, just faced with that question, you might not know where to begin. But, to phrase that a different way, that's like saying, what's the frequency of people who consider themselves street smart. And that sounds a little more statistical. Basically that gives you a hint that we want to go to Analysis and select Frequencies, okay? Then I'm going to move over the IsBookSmart variable, and hit OK. So, this gives me, an output that generates frequencies for the different values of IsBookSmart. Now, IsBookSmart is binary, it's either YesBookSmart or NoBookSmart. ", ['question', 'questions', 'score', 'drinks', 'alcohol', 'optimism', 'question', 'people', 'smart', 'question', 'way', 'frequency', 'people', 'consider', 'street', 'smart', 'gives', 'hint', 'Analysis', 'okay', 'IsBookSmart', 'OK', 'output', 'generates', 'frequencies', 'values', 'IsBookSmart', 'IsBookSmart', 'YesBookSmart', 'NoBookSmart']), 0.1276884796138123, 0.08932287081296464)
((9, '/Users/jag/Downloads/Stanford medstats/RExercise2.srt', "Let me say that a different way. So in the first question, with the two samples What we're doing is we're comparing the two means between each other. And in this question we're calculating a difference between, between the two variables and then we're comparing that mean difference with zero to see if it's statistically significant. To see if there's a statistically significant difference. Between that mean difference and zero. So, third part of question two asks us to perform this statistical test and to report the PRE for it and to state on average how different are these quote-unquote love scores. So let's do that. I'm going to click Analysis. And then go to the Paired Test option here. And what I'm going to do is I'm going to select both Math Love and Writing Love. ", ['Let', 'way', 'question', 'samples', 'means', 'question', 'difference', 'variables', 'difference', 'zero', 'difference', 'difference', 'part', 'question', 'asks', 'test', 'PRE', 'love', 'scores', 'let', 'Analysis', 'Paired', 'Test', 'option', 'Math', 'Love', 'Writing', 'Love']), 0.12500000000000003, 0.0874421786945046)
QUERY: Point 3 says that the Standard error is inversely related to the sample size. Shouldn't it be: inversely related to the square root of sample size?
*************************
((10, '/Users/jag/Downloads/Stanford medstats/Unit 8 Module 1 part 2.srt', "It's a, it's a bit of an approximation, but it worked pretty well for all most all n. I can just plug in then for example, to be statistically significant when you have a sample size of 10, 2 divided by the square root of 10, gives you that you're going to need to have a correlation of 0.63. It's going to need to be pretty big in order to come out statistically significant with only 10, to be confident that that's really different than 0. With a 100 however, 2 divided by the square root of a 100 is, gives you a value of 0.2. So you can have any correlation coefficient for any variables in your study, that comes out to be .02 or greater, when you have a sample size of a 100 is going to be statistically significant. As you get to very big sample sizes, as I showed you before, say a 1000, 10,000, 100,000. The minimum correlation that's going to come out to be statistically significant, notice all of these, when your sample size is a 1000 or greater, 0.06, 0.02, these are all less than 0.1 correlation. That would be considered essentially no correlation, it's so small, that it's not meaningful. ", ['bit', 'approximation', 'n.', 'example', 'sample', 'size', 'root', 'correlation', 'order', 'however', 'root', 'value', 'correlation', 'coefficient', 'variables', 'study', 'sample', 'size', 'sample', 'sizes', 'correlation', 'notice', 'sample', 'size', 'correlation', 'correlation']), 0.5462309935991351, 0.5214132306238656)
((9, '/Users/jag/Downloads/Stanford medstats/Unit 8 Module 1 part 2.srt', "What's kind of interesting about the correlation coefficient. If you look at those formulas that I just showed you, it turns out that whether or not a correlation coefficient comes out to be statistically significant, depends on only two things. The magnitude of r that you observed in your sample, and in n the sample size. There is nothing else involved. So you can take the formulas that I have just showed you, and you can rearrange them a little bit. And you can show that there's a direct relationship between r and n, there is you can calculate a, the minimum size for the correlation coefficient that you would need to be statistically significant for every different sample size. And I, it's approximation here. But you take the formulas I had in the past, the last slide. And basically to be statistically significant, the, the minimum sta r that's going to be statically significant for a given sample size, is roughly equal to 2 divided by the square root of n. ", ['correlation', 'coefficient', 'formulas', 'correlation', 'coefficient', 'things', 'magnitude', 'r', 'sample', 'n', 'sample', 'size', 'nothing', 'formulas', 'bit', 'relationship', 'r', 'n', 'size', 'correlation', 'coefficient', 'sample', 'size', 'approximation', 'formulas', 'slide', 'sta', 'r', 'sample', 'size', 'root', 'n']), 0.5084263138713924, 0.4629197098418786)
((10, '/Users/jag/Downloads/Stanford medstats/Unit 5 Module 4 part 1.srt', "looking at the formula. The formula gives us some insight here. I don't, we're not worry about memorizing the formula but look at the formula in terms of what is it telling us. So standard error decreases as sample size goes up because sample sizes in the denominator. So bigger sample size means less standard error. Well that makes sense. As you sample more people you would expect the uncertainty to go down, the imprecision to go down. The variability of the static from sample to sample will go down if you're sampling more people. going to get closer to the true mean. The standard error increases with greater trait variability, that, that s, that standard deviation of the sample is in the numerator. So if vitamin D were more variable, that's going to cause more variation from in the statistic. Right? It's going to cause more uncertainly. So those are the two elements of standard error. And just to illustrate those, I did another simulation where I increased the sample sizes to 400. So I did many, many, many samples of 400. Notice to what happened to my normal curve, it shrunk. ", ['formula', 'formula', 'insight', 'formula', 'look', 'formula', 'terms', 'error', 'decreases', 'size', 'sample', 'sizes', 'denominator', 'sample', 'size', 'error', 'Well', 'sense', 'people', 'uncertainty', 'imprecision', 'variability', 'people', 'mean', 'error', 'increases', 'trait', 'variability', 's', 'deviation', 'sample', 'numerator', 'vitamin', 'D', 'variation', 'elements', 'error', 'simulation', 'sample', 'samples', 'Notice', 'curve']), 0.48418202613504197, 0.4448363052016043)
((4, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 7 mod1.srt', "So I want to somehow take repeated samples of the same size, of size n, from my dataset of size n and that, of course, if you, says, that sounds impossible, you can only take one sample. because they're the same size. So how can you do this? Well the, the trick is the key to all of this is sampling with replacement. So what is sampling with replacement? That means that when you're creating your new datasets, your repeated samples, some observations can appear more than once, and some observations will not appear at all. Sampling with the replacement is something like rolling a die, the one you can roll a one twice in a row. That's okay. So you can get the same observation to appear in your news sample twice and some of the observations won't appear at all. And that's the key to this. This is what will allow you to take multiple samples from an an initial sample, all of size n. And this is just bell, best illustrated with a picture. So let me just illustrate that now. So what we're doing, is we are taking some original samples. ", ['take', 'samples', 'size', 'size', 'n', 'dataset', 'size', 'n', 'course', 'sounds', 'sample', 'size', 'Well', 'trick', 'replacement', 'replacement', 'datasets', 'samples', 'observations', 'observations', 'replacement', 'something', 'rolling', 'die', 'twice', 'row', 'okay', 'observation', 'news', 'sample', 'twice', 'observations', 'multiple', 'samples', 'sample', 'size', 'n.', 'picture', 'let', 'samples']), 0.4577036541524985, 0.40988567108949087)
((11, '/Users/jag/Downloads/Stanford medstats/Unit 6 Module 1 part 1.srt', "When we do statistical tests. And those were directly related to the effect size divided by the standard error. And the standard error is some com-, composite of variability divided by sample size. It's not exactly variability divided by sample size. It's more complicated than that but in essence the standard error has a variability in the numerator and some version of sample size in the denominator. So, roughly our z or t square was always proportional to the effect size divided by the variability divided by the sample size. That actually puts the sample size essentially in the numerator. Bigger Z scores and bigger T scores mean smaller P values in other words if you have a bigger effect size that's going to decrease the size of your P value because its going to make you more likely to find a stastically significant effect. If your sample size is bigger, you're also going to be more likely to find a statistically significant effect. However, if you have greater variability in the, in the outcome or trait you are looking at, the greater the variability the smaller the z or ", ['tests', 'effect', 'size', 'error', 'error', 'variability', 'sample', 'size', 'variability', 'sample', 'size', 'essence', 'error', 'variability', 'numerator', 'version', 'sample', 'size', 'denominator', 'z', 't', 'square', 'effect', 'size', 'variability', 'sample', 'size', 'sample', 'size', 'numerator', 'Bigger', 'Z', 'scores', 'T', 'scores', 'P', 'values', 'words', 'effect', 'size', 'size', 'P', 'value', 'effect', 'sample', 'size', 'effect', 'variability', 'outcome', 'trait', 'variability', 'z']), 0.6499423302067071, 0.39055361673558914)
((0, '/Users/jag/Downloads/Stanford medstats/Unit 5 Homework with answers.srt', "The correct answer for this question is d, only, the standard error of the mean. This is because when we have 246 patients, we, that is quite a lot of patients. The central limit theorem would have started kicking in already. So we would have the mean, the median, and the standard deviation for the sample to be closer to the mean, the median, and the standard deviation for the total population. We wouldn't expect those parameters to change very much if we had it, added a hundred people. But the the standard error of the mean, which decreases directly with the number of Samples that we have in a sample size would definitely go down if we added a hundred people to the number of patients that we have. In this case, because we have a very small sample size of only 13, we need to opt for a T-distribution as opposed to the normal distribution. A d-distribution has one less degrees of freedom that a number of the population of the sample. In this case because we have 13 people-, children in the sample the degrees of freedom turn out to be 12. The formula for ", ['correct', 'answer', 'question', 'error', 'patients', 'lot', 'patients', 'limit', 'theorem', 'deviation', 'sample', 'deviation', 'population', 'parameters', 'people', 'error', 'decreases', 'number', 'Samples', 'sample', 'size', 'people', 'number', 'patients', 'case', 'sample', 'T-distribution', 'distribution', 'd-distribution', 'degrees', 'freedom', 'number', 'population', 'sample', 'case', 'children', 'sample', 'degrees', 'freedom', 'turn', 'formula']), 0.4162907234150368, 0.37164911640829873)
((2, '/Users/jag/Downloads/Stanford medstats/Unit 6 Module 1 part 2.srt', "Those sample size formulas are directly related to putting those elements together. So, power and sample size are directly related to one another. And those are affected by the effect size, the standard deviation, and the significance level. We can put all those five elements together into formulas that tell us how big of an n, how big of a sample, do I need for a given level of power for my study. I'm just going to put one formula up, one sample size formula up. On the screen here so we can talk about I'm not expecting you to, to want to memorize or even use this formula at this point. If you want to know more about sample size formulas, like the sample size calculation, there's an entire optional module you can take to learn about all the details of this formula. But what I really want Everybody to get out of here, is just the relationships between those five elements that I just talked about. Those five elements relate, and you can put them together to come up with a sample size format. So for example, this formula says that the sample size that you need in ", ['sample', 'size', 'formulas', 'putting', 'elements', 'power', 'sample', 'size', 'effect', 'size', 'deviation', 'significance', 'level', 'elements', 'formulas', 'tell', 'n', 'sample', 'level', 'power', 'study', 'formula', 'sample', 'size', 'formula', 'screen', 'formula', 'point', 'size', 'formulas', 'sample', 'size', 'calculation', 'module', 'details', 'formula', 'Everybody', 'relationships', 'elements', 'elements', 'sample', 'size', 'format', 'example', 'formula', 'sample', 'size']), 0.6084554357361275, 0.37097216980243114)
((11, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 7 mod1.srt', "a complicated where we don't have a formula. Or you might want to do that where its a well-known statistic but we've violated some assumptions, so maybe you have a really small sample or you violated normality. You don't have a normal distribution and so you're not sure if those formulas are really good so you could actually bootstrap it and there would be nothing wrong with just basing it on the bootstrap distribution. We're also going to use it in certain cases for prediction modeling, which we'll talk about later this week. But just to show you how the bootstrap would be used specifically to calculate standard errors for say a new statistic, just a statistic where you may have violated some of the assumptions. So first thing you're going to do is you're number your observations 1 through n, and then you're going to sample with replacement. So you're going to draw a random sample of size n. So observation 1 might appear twice, and observation 2 might not appear at all. You're going to make a bootstrap sample. Then you're going to calculate your statistic, say the mean or the beta coefficient or whatever statistic you're interested in on that sample. ", ['formula', 'assumptions', 'sample', 'normality', 'distribution', 'formulas', 'nothing', 'bootstrap', 'distribution', 'cases', 'prediction', 'week', 'bootstrap', 'errors', 'say', 'assumptions', 'thing', 'number', 'observations', 'n', 'replacement', 'random', 'sample', 'size', 'n.', 'So', 'observation', 'twice', 'observation', 'bootstrap', 'sample', 'beta', 'coefficient', 'sample']), 0.36417852036461484, 0.3695902552907175)
QUERY: If I am running a two or three-way ANOVA and the assumption of homogeneity of variances  is not met for my data, which is the alternative test?
*************************
((1, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 6 Module 1 Part 1.srt', "When I say linear models, I mean linear regression and then all the things that are related to linear regression so. Linear correlation, like the Pierson's correlation coefficient. And also ANOVA and ttest. In fact the ANOVA test and the ttest are just special cases of linear regression, so these are all fall, these all fall under a linear model. So they all have the same assumptions. So one of the assumptions is that our outcome variable is normally distributed. It turns out that this assumption is the most important for small samples. If you have large samples, the Central Limit Theorem kicks in, and if you violate normality the, these tests are fairly robust against this assumption. But especially if you have small samples, you're going to want to test to make sure that your outcome variable is normally distributed. We also have an assumption of homogeneity of variances. Basically, that says that the variances of that outcome variable are equal at all levels of the predictor. ", ['models', 'regression', 'things', 'regression', 'Linear', 'correlation', 'Pierson', 'correlation', 'coefficient', 'ANOVA', 'fact', 'ANOVA', 'test', 'ttest', 'cases', 'regression', 'fall', 'fall', 'linear', 'model', 'assumptions', 'assumptions', 'outcome', 'assumption', 'samples', 'samples', 'Central', 'Limit', 'Theorem', 'normality', 'tests', 'assumption', 'samples', 'test', 'outcome', 'assumption', 'homogeneity', 'variances', 'variances', 'outcome', 'levels', 'predictor']), 0.4003203845127178, 0.32738416539305937)
((1, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 7 Module 4.srt', "It's a simple dataset, and of course we're going to want to plot it. And since I only have six subjects, I might as well plot an individual line for each subject, and that's what I've done here. So this is the depression score over time for the six subjects. And you can see, well, they seem to be going down a little bit towards time point three. And then, rebounding a little bit at time point four. But there is some individual variability there. Before we jump into actually how to calculate repeated measures ANOVA, I just want to point out that there are some assumptions. So, as with any linear model, we are assuming that we have a normally distributed outcome. So I'm going to assume that this depression score is reasonably normally distributed here. The other assumption of repeated measures ANOVA is analogous to the homogeneity of variance assumption. That we had when we did regular ANOVA. Remember, that assumption was that the variances for each group were equal. It gets a little more complicated, when, ", ['dataset', 'course', 'subjects', 'line', 'subject', 'depression', 'score', 'time', 'subjects', 'bit', 'towards', 'time', 'point', 'bit', 'time', 'point', 'variability', 'measures', 'ANOVA', 'point', 'assumptions', 'model', 'outcome', 'depression', 'score', 'assumption', 'measures', 'ANOVA', 'homogeneity', 'variance', 'assumption', 'ANOVA', 'Remember', 'assumption', 'variances', 'group']), 0.3380617018914066, 0.26188053595219196)
((1, '/Users/jag/Downloads/Stanford medstats/Unit 8 Module 1 part 1.srt', "enough even when the underlying trait that your looking at is highly skewed. Eventually the central limit theorem kicks You can't guarantee exactly where that's going to kick in. So if you're dealing with small samples, under a 100, you have to worry about the normality assumption. Usually as long as our sample size is above 100. Most times unless you have an extremely skewed distribution, that central limit there will, will apply. But for small symbols we're going to worry about this and I'll show you some ways to evaluate the, this normal assumption for linear regression. We also have an assumption called the homogeneity of variances. For the ttest and ANOVA, that assumption said that in different groups, in the different groups we were comparing the variances across the different groups had to be the same. This week we're going to talk about linear correlation and linear regression. The assumption is the same except we might talking about now a continuous predictor. Instead of comparing groups our predictor variable might also be continuous. ", ['trait', 'looking', 'limit', 'theorem', 'kicks', 'samples', 'normality', 'assumption', 'sample', 'size', 'times', 'distribution', 'limit', 'symbols', 'ways', 'assumption', 'linear', 'regression', 'assumption', 'homogeneity', 'variances', 'ANOVA', 'assumption', 'groups', 'groups', 'variances', 'groups', 'week', 'correlation', 'regression', 'assumption', 'except', 'predictor', 'groups', 'predictor']), 0.36742346141747667, 0.2573019589717846)
((7, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 6 Module 1 Part 1.srt', "And they're all taking a test at the end of the study to say how much they learned and there's six per group for a total of 24 kids here so that's what the made up data looked like. So first thing I did with this data, you're always going to want to plot your data. So, before we do any fancy. Testing. We want to plot the data and get a sense of what's going on. Obviously you can figure out a lot from this plot. These are very simple data, we're not adjusting for anything. So I just plotted the box and whisker plots for those scores on this test for the four groups, 1, 2, 3, and 4. And you can see quite quickly that it looks like Groups 1 and 2 seem to have a higher mean score than Groups 3 and 4. So you can get a lot out of the plot. In fact that, this plot automatically gave me an ANOVA p-value. So what you're seeing up here is an F from a one-way ANOVA, and a p-value which is 0.0003. So these, this is a highly statistically significant difference. Somewhere in there, there lies a difference. ", ['test', 'end', 'study', 'group', 'kids', 'data', 'thing', 'data', 'data', 'fancy', 'data', 'sense', 'lot', 'plot', 'data', 'anything', 'box', 'whisker', 'plots', 'scores', 'test', 'groups', 'Groups', 'seem', 'score', 'Groups', 'lot', 'plot', 'fact', 'plot', 'ANOVA', 'F', 'ANOVA', 'difference', 'lies', 'difference']), 0.375, 0.24982217806474963)
((3, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 6 Module 1 Part 1.srt', "And we don't have to do any transformations of the outcome. So that's a really easy assumption to test. But don't forget that it's there. That we are making the assumption of linearity. When our predictor variable becomes time, as it will for repeated measures examples. We are assuming therefore for a lot of the models we're going to use. That the changes over time are relatively constant. So, just keep that in the back of your mind. That's something that you're going to need to test. Alright, so just starting with one-way ANOVA, basic ANOVA. When do we use this test? So, we use one-way ANOVA when we want to test, we want to compare, some outcomes, some continuous outcomes, and we're comparing the means of that outcome. Between more than two groups or greater than or equal to two groups. If you have just two groups, this essentially becomes a ttest. Of course you can run an ANOVA with only two groups then, it, it's equivalent though to the ttest. You can also have more than two groups within the ANOVA. ", ['transformations', 'outcome', 'assumption', 'test', 'assumption', 'linearity', 'predictor', 'becomes', 'time', 'measures', 'examples', 'lot', 'models', 'changes', 'time', 'back', 'mind', 'something', 'test', 'Alright', 'ANOVA', 'ANOVA', 'test', 'ANOVA', 'test', 'outcomes', 'outcomes', 'means', 'outcome', 'groups', 'groups', 'groups', 'ttest', 'course', 'ANOVA', 'groups', 'equivalent', 'groups', 'ANOVA']), 0.3908679799852858, 0.23767898187223593)
((8, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 6 Module 1 Part 1.srt', "Again, we can't say, specifically, which groups differ. But those groups are definitely not all equal. We also want to just plot the outcome variable of the scores here in a histogram, to look at whether or not these are normally distributed. This is a small sample. So we would want to make sure that we met the normality assumption here. So I plotted these scores in a histogram. The average scores around high 20s and low 30s and you can see that it's reasonably normally distributed. That's good enough for us to be able to run linear models, ANOVA linear regression on these data. Alright. So now, I am actually going to walk. Through the mechanics of calculating that one-way ANOVA. And what I've done here is just rearranged the data a little bit to make this easier. So here's my four groups, here are the six kids in each group. Here are their scores, their, their scores on that post test. And I've already calculated for each group. The mean score on that test for each group. And we saw on the plot, on the box plot that groups 1 and ", ['groups', 'groups', 'plot', 'outcome', 'scores', 'histogram', 'sample', 'normality', 'assumption', 'scores', 'histogram', 'scores', 'models', 'ANOVA', 'linear', 'regression', 'data', 'Alright', 'mechanics', 'ANOVA', 'data', 'bit', 'groups', 'kids', 'group', 'scores', 'scores', 'post', 'test', 'group', 'score', 'test', 'group', 'plot', 'box', 'plot', 'groups']), 0.2623360911485515, 0.22783670434434455)
((1, '/Users/jag/Downloads/Stanford medstats/Unit 7 Module 3 part 2.srt', "Our outcome variable here is nevi counts, it's clearly not normally distributed right. We have very much right skewed data. We would not expect nevi counts to be normally distributed. So how would we test whether or not those 3 groups are different? Well clearly, since we have very skewed data here, we're not going to want apply a nova, our sample size is. Is reasonably small here. It's 150 marathon runners, so there's, roughly something like 50 in each group. That's small enough with such skewed data that we're going to not want to rely on ANOVA. We're going to want to use a non-parametric test. So how did I get to that, we want the Kruskal-Wallis test. I want to know whether the nevi counts differ according to training velocity group, slow, medium, or fast. My outcome variable is nevi count. It is a continuous variable. It's actually a discrete variable but you know, it's it's a numerical variable. Is it normally distributed? No, it's definitely not normally distributed and it's also, we're also dealing with a relatively small sample size. The observations are not correlated; I have three independent groups. ", ['outcome', 'nevi', 'counts', 'data', 'nevi', 'counts', 'test', 'groups', 'Well', 'data', 'nova', 'sample', 'size', 'marathon', 'runners', 'something', 'group', 'data', 'ANOVA', 'test', 'Kruskal-Wallis', 'test', 'nevi', 'differ', 'training', 'velocity', 'group', 'medium', 'outcome', 'nevi', 'count', 'sample', 'size', 'observations', 'groups']), 0.29371347540384113, 0.21807069900909723)
((14, '/Users/jag/Downloads/Stanford medstats/HRP262 Unit 1 Module 4.srt', "indeed, had a chi square of 4.66. That corresponds to a p-value of .03. Now, as I mentioned, the log rank test is most commonly used to compare different groups in Kaplan-Meier. There are other statistics that you could use, however. So I'll just point those out to you now. So first of all there's something called a likelihood ratio test. The negative two Log likelihood ratio. You should recognize that, that comes from maximum likelihood estimation. So this value must come from some kind of regression. And in fact, it comes from assuming an exponential distribution. So if your data, actually if your survival curve actually follow a particular distribution called the exponential, you could fit a regression here. That regression will give you a p-value using negative two likelihood comparison. Now you'll see that when we do that here, the p-value comes out to be very similar. But there is an assumption here So just keep in mind that there is an assumption in, in that test. ", ['chi', 'square', 'log', 'rank', 'test', 'groups', 'statistics', 'something', 'likelihood', 'ratio', 'test', 'Log', 'likelihood', 'ratio', 'maximum', 'likelihood', 'estimation', 'value', 'kind', 'regression', 'fact', 'distribution', 'data', 'survival', 'curve', 'distribution', 'regression', 'regression', 'using', 'likelihood', 'comparison', 'comes', 'assumption', 'mind', 'assumption', 'test']), 0.2611164839335467, 0.20853456407668464)
QUERY: I'm surprised that the researchers did not found any correlation between coffee drinking and lung cancers: I heard there is a strong correlation between coffee drinking and cigarette smoking (e.g. illustrating that correlation is not causation). Perhaps, they did balance the subgroups for known cancer risks (such as smoking) !?
*************************
((1, '/Users/jag/Downloads/Stanford medstats/Unit 6 Homework with Answers.srt', "sides of the same person's face and these obviously will be correlated. This question has the authors running a lot of tests, so they have 15 different levels of black tea, coffee, decaf coffee. They have two diseases [UNKNOWN]. They also have two types of participants, pre menopausal versus postmenopausal. In such a large number of test that are run, we would expect to find a few p values that are less than .05 just by chance. The fact that we have a couple of values that are less than .05 does not reflect actual correlation in this case. Because this study examined individual women, the unit of observation here is a person. The observations in this study are not correlated. This is because this study examined independent women. This question gives us the increased risk various diseases comparing two groups, exposed versus unexposed. ", ['sides', 'person', 'face', 'question', 'authors', 'lot', 'tests', 'levels', 'tea', 'coffee', 'decaf', 'coffee', 'diseases', 'UNKNOWN', 'types', 'participants', 'menopausal', 'versus', 'postmenopausal', 'number', 'test', 'p', 'values', 'chance', 'fact', 'couple', 'values', 'correlation', 'case', 'study', 'women', 'unit', 'observation', 'person', 'observations', 'study', 'study', 'women', 'question', 'risk', 'diseases', 'groups', 'versus']), 0.16376789481350584, 0.28771535741243787)
((10, '/Users/jag/Downloads/Stanford medstats/RExercise1 full.srt', "And we can see that it gives us summary information for all the variables in our Dataset. Now, to specifically answer the question about how many missing values we have in the variable Carter. I go to, affine carter, I go to that, and then I look at NA's, or number of missings, in other words. And I see that we have 12 missing values in the variable carter. The next question asks us to create a histogram. In dark blue for the variable, coffee. And so what I'm going to do is go to Plots and select Plot B uilder. I'm going to click on Histogram and actually drag it down to this space right here. I'm going to let go and then again I'm going to click on coffee and drag it to the Variable window. And once you drag the variable coffee over variable will turn green. I'll hit OK. And this is going to plot a histogram of coffee drinking. ", ['information', 'variables', 'Dataset', 'question', 'values', 'Carter', 'affine', 'carter', 'NA', 'number', 'missings', 'words', 'values', 'carter', 'question', 'histogram', 'dark', 'blue', 'coffee', 'Plots', 'Plot', 'B', 'uilder', 'Histogram', 'space', 'coffee', 'drag', 'Variable', 'window', 'coffee', 'will', 'turn', 'OK.', 'histogram', 'coffee']), 0.20031323433445378, 0.28618151767317546)
((11, '/Users/jag/Downloads/Stanford medstats/Unit 6 Module 3 part 1.srt', "the type of breast cancer, they broke that down. They also looked at the type of women, is it a post menopausal woman or pre-menopausal women. So they made multiple subgroups. So essentially, they ended up running about 50 statistical tests to look at all of these different possible associations. So notice, a large number of tests here. Overall, there was no association between caffeine, coffee, caffeinated coffee, caffeinated tea, and breast cancer. However, they came out with 4 significant, or near significant P-values with significant statistical significance at the level of 0.05, in some subgroups. So, they found that coffee intake was linked to an increased risk of breast cancer in women who had benign breast disease. That's a particular subgroup. The P-value there was 0.08, so not quite statistically significant at the conventional 0.05. They also found that caffeine intake was linked to an increased link of a particular type of tumor, estrogen/progesterone negative tumors, as well as larger tumors, tumors larger than 2 cm. ", ['type', 'breast', 'cancer', 'type', 'women', 'post', 'menopausal', 'woman', 'women', 'multiple', 'subgroups', 'tests', 'associations', 'notice', 'number', 'tests', 'association', 'caffeine', 'coffee', 'coffee', 'tea', 'cancer', 'P-values', 'significance', 'level', 'subgroups', 'coffee', 'intake', 'risk', 'breast', 'cancer', 'women', 'disease', 'subgroup', 'caffeine', 'intake', 'link', 'type', 'tumor', 'tumors', 'tumors', 'tumors', 'cm']), 0.22420982516393334, 0.28216280650818776)
((10, '/Users/jag/Downloads/Stanford medstats/Unit 6 Module 3 part 1.srt', "That is, values between 0 and 0.05 or between 0 and 0.1, are not surprising. They're expected. In fact, it would be odd is you didn't get some p values that were statistically significant at the 0.05 level when you do so many tests. Let me give you another multiple testing example from the, from the [INAUDIBLE]. Just show you [INAUDIBLE] a real example where I think probably what's going on are chance findings. So, this was a study researchers were looking at whether or not caffeine and coffee drinking and tea drinking, whether or not any of that influences your risk of breast cancer. There's been some thought in the past that drinking caffeinated coffee, and a lot of caffeine might increase your risk of breast cancer. So they did a very thorough study. They looked at, they asked women about their caffeine, coffee and tea consumptions, and they looked at who got breast cancer and who didn't get breast cancer. Now, they looked at this in a lot of different ways. They looked at Does caffeine, coffee drinking or tea drinking, do any of those things relate to getting breast cancer at all. They also looked at breast cancer in multiple subgroups, so ", ['values', 'fact', 'p', 'values', 'level', 'tests', 'Let', 'multiple', 'testing', 'example', '[', 'INAUDIBLE', ']', 'INAUDIBLE', 'example', 'chance', 'findings', 'study', 'researchers', 'coffee', 'tea', 'influences', 'risk', 'breast', 'cancer', 'thought', 'coffee', 'lot', 'caffeine', 'risk', 'breast', 'cancer', 'study', 'women', 'caffeine', 'coffee', 'tea', 'consumptions', 'cancer', 'cancer', 'lot', 'ways', 'Does', 'caffeine', 'coffee', 'tea', 'things', 'breast', 'cancer', 'cancer', 'multiple', 'subgroups']), 0.26468888905891474, 0.2683468867879649)
((3, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Optional.srt', "What's going on here is that heavy drinkers also tend to be heavy smokers. And of course, we know that smoking is a major, major cause of lung cancer. So in this case, the apparent association between alcohol and lung cancer is really just due to confounding by smoking. I'm just going to show you this table here because it's a really great illustration, of how risk factors cluster in observational studies. So this is a study that we're going to return to in another couple of weeks, and we'll talk about in some, in some more detail, but I'm just showing you table one for the men from this study. And this study was looking at whether or not, how much red meat you eat is correlated to mortality? And what the researchers did, was they just asked people to report how much red meat they're eating. And they divided people up into quintiles of red meat intake. So Q one is the lowest quintile. ", ['drinkers', 'smokers', 'course', 'smoking', 'cause', 'lung', 'cancer', 'case', 'association', 'alcohol', 'lung', 'cancer', 'smoking', 'illustration', 'risk', 'factors', 'cluster', 'studies', 'study', 'couple', 'weeks', 'men', 'study', 'study', 'meat', 'mortality', 'researchers', 'people', 'meat', 'people', 'quintiles', 'meat', 'intake', 'Q']), 0.227429413073671, 0.2580577531126846)
((11, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 9 mod3 Part1.srt', "So, there are ways to kind of cheat that not have to estimate all of those separately. You don't want to use up all your degrees of freedom on this. So here's another possibility you could It's called an exchangeable correlation matrix. This is the simplest. Well, you could just say, well, I know there's some correlation. It's not zero. What, this is what it would look like if our time points were independent, if our outcomes were independent. We'd get, the correlations would all be zero. Okay. That's what we call the independence correlation structure. But if we know we have correlation, we might just say well, I'm going to assume that all the correlation coefficients are the same. So, eh, for the bone density one, we are saying you know, they were all pretty close. And maybe we'll just call it 0.94 for all of them, close enough. We're going to estimate a single correlation coefficient for all of, of the correlations. This is what we call an exchangeable structure. And that's the one that's mostly commonly used in the situation where you have data at one time but not repeated measures. And you just have some kind of clustering in the data. ", ['ways', 'cheat', 'degrees', 'freedom', 'possibility', 'correlation', 'matrix', 'Well', 'correlation', 'time', 'points', 'outcomes', 'correlations', 'Okay', 'independence', 'correlation', 'structure', 'correlation', 'correlation', 'coefficients', 'bone', 'density', 'correlation', 'coefficient', 'correlations', 'structure', 'situation', 'data', 'time', 'measures', 'kind', 'clustering', 'data']), 0.396683678449134, 0.25769428493997404)
((13, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 9 mod3 Part1.srt', "the correlation structure. So that even though you have to specify it up front, if you get it wrong, uyh, you probably, it probably is not going to be too big of a deal. All right, so, different correlation structures. So this is the independence correlation structure. That means that all of the outcomes are independent. So that one's easy. That they're all set to 0. Clearly, we're talking about, this week, about situations where we don't have independence. So this is what we would need in order to be able to do regular linear regression or regular logistic regression. Presumably, we don't have this if we're turning to GEE models. I mentioned the exchangeable correlation matrix. Whatever, however many time points or however many correlated things you have you're going to be estimating just one correlation coefficient for all of them. Well again, that does pretty well because you're not assuming they're zero, which is what, which is definitely problematic. But as long as they're all kind of similar there's no real pattern to them, you can do pretty well by just accounting for the fact that there is some correlation as opposed to saying there's none. ", ['correlation', 'structure', 'front', 'uyh', 'deal', 'right', 'correlation', 'structures', 'independence', 'correlation', 'structure', 'outcomes', 'set', 'week', 'situations', 'independence', 'order', 'linear', 'regression', 'regression', 'GEE', 'models', 'correlation', 'matrix', 'time', 'points', 'things', 'correlation', 'coefficient', 'Well', 'kind', 'pattern', 'fact', 'correlation', 'none']), 0.396683678449134, 0.25769428493997404)
((4, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit2 Mod1.srt', "alcohol increases the risks of certain cancers, such as breast cancer. That's pretty well established. However, it turns out that in this particular case for lung cancer, alcohol actually is not biologically related to lung cancer, as far as we know. It appears that those are not related causally. Of course, there is a very important confounder, a third variable lurking in the background here. So, heavy drinkers also tend to be smokers. And, we know for sure that smoking is strongly related to lung cancer. So smoking in this picture is the confounding variable. So you see that smoking is related both to alcohol and to lung cancer, but it's not on the causal pathway between those two, because in fact, alcohol is not causally related to lung cancer. The association here was completely driven by confounding with smoking. And I think it's helpful sometimes, to just put some hypothetical numbers, to illustrate this with some numbers. So what would it look like in terms of numerically, what does a confounder look like? ", ['alcohol', 'increases', 'risks', 'cancers', 'cancer', 'case', 'lung', 'cancer', 'alcohol', 'cancer', 'course', 'confounder', 'lurking', 'background', 'drinkers', 'smokers', 'cancer', 'smoking', 'picture', 'confounding', 'smoking', 'alcohol', 'cancer', 'causal', 'pathway', 'fact', 'alcohol', 'cancer', 'association', 'confounding', 'smoking', 'numbers', 'numbers', 'terms', 'confounder', 'look']), 0.29361010975735174, 0.25470930077081433)
QUERY: > I know they are not confidence intervals for sure.  How do you know this \for sure? They *are* confidence intervals in the brackets."
*************************
((4, '/Users/jag/Downloads/Stanford medstats/Unit 5 Homework with answers.srt', "in terms of the odd ratio we get the confidence interval to be .87 to 8.66. The answer to 'F' is false. The confidence intervals of the laws of an odd ratio as symmetric as we just found but when we exponentiate the symmetry is lost. And the confidence interval of the actual odds ratio is not symmetric. The answer to G is true. The 95% confidence interval for the oz ratio crosses one, so we cannot reject the null hypothesis. And thus we know that b is greater 0.05. Part A of this question wants us to roughly estimate the two sided p value associated with this distribution. If we down the number of observations we have to the left of negative 25 and to the right of 25, we'll see they're about 25 observation out of a thousand that have ", ['terms', 'ratio', 'confidence', 'interval', 'answer', 'confidence', 'intervals', 'laws', 'ratio', 'symmetry', 'confidence', 'interval', 'odds', 'answer', 'G', '%', 'confidence', 'interval', 'oz', 'ratio', 'crosses', 'null', 'hypothesis', 'b', 'Part', 'question', 'p', 'value', 'distribution', 'number', 'observations', 'left', 'right', 'observation', 'thousand']), 0.36961063547728645, 0.49057214254583187)
((12, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 7 mod1.srt', "They you're going to do this a large number of times. So you're going to do this say a 1,000 times a 100 times 200 times a large number of times, so that you can figure out, what is actually the distribution of this statistic. Then you take say those 1,000 beta coefficients and you can say plot them in a histogram and just look that what's the shape of the distribution. You can calculate the standard deviation of those betas, as you would calculate the standard deviations of any number. You've got a thousand, beta coefficients, you calculate their standard deviation. That's the standard error. And then you can use that for calculating p values. You can also calculate confidence intervals, so you can just literally look and say okay, where do I have, where do 95% of my beta coefficients lie, that's my 95% confidence interval, whatever's in the tail is left out. So, you can get empirical standard errors and empirical confidence intervals, and this one of the main uses of the bootstrap, but again it has very a lot of ", ['number', 'times', 'times', 'times', 'times', 'number', 'times', 'distribution', 'beta', 'coefficients', 'plot', 'histogram', 'shape', 'distribution', 'deviation', 'betas', 'deviations', 'number', 'thousand', 'beta', 'coefficients', 'deviation', 'error', 'p', 'values', 'confidence', 'intervals', 'okay', '%', 'beta', 'coefficients', '%', 'confidence', 'interval', 'tail', 'errors', 'confidence', 'intervals', 'uses', 'bootstrap', 'lot']), 0.2993421700446249, 0.45276017141338154)
((5, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit6 Mod6.srt', "They were just, they simply didn't have a close enough match in the data set. And in fact, even with a fairly liberal criteria of a 25 percent difference, they can only find matches for 253 of the original 406 mechanical valve patients. So they ended up in the match cohort with a data set of about 500 total people. Whereas in the original cohort, there was about 1,300. So, a lot of people that couldn't be matched here. Then they took the match cohort and they ran a survival analysis. So this is just a Kaplan-Meyer curve so it's unadjusted. In blue we have the Ross patients in red. We have the mechanical valve patients, the smaller dotted lines is to represent confidence intervals. You can see the confidence intervals are highly overlapping. There weren't that many deaths. So there was a 97 percent survival in the Mechanical Valve versus 95 percent in the Ross procedure, the p value is completely non significant. So remember that Ross patients, in previous studies, had been shown to do better in terms of survival but once we account for ", ['data', 'set', 'fact', 'criteria', 'percent', 'difference', 'matches', 'valve', 'patients', 'match', 'cohort', 'data', 'set', 'people', 'cohort', 'lot', 'people', 'match', 'cohort', 'survival', 'analysis', 'curve', 'Ross', 'patients', 'valve', 'patients', 'lines', 'confidence', 'intervals', 'confidence', 'intervals', 'deaths', 'percent', 'survival', 'Mechanical', 'Valve', 'percent', 'Ross', 'procedure', 'p', 'value', 'remember', 'Ross', 'patients', 'studies', 'terms', 'survival']), 0.23448415270421968, 0.39835674981725394)
((5, '/Users/jag/Downloads/Stanford medstats/Unit 2 Module 4 part 1.srt', "I want to spend just a minute here to walk you through what that confidence interval means. We're going to spend a lot of time on confidence intervals in an upcoming week of the course. But I want to just give you sort of a conceptual understanding of what that confidence interval means here. So the relative risk that we calculated in the study, what we call the point estimate. What we calculated in the study was exactly 0.46 rounded up to .5. So that indicates a halving of risk. But that's just what we saw in the study. And of course, whenever we do a study, we know that there's going to be some uncertainty. In other words the true effect of Vioxx on GI events might not be exactly 0.5. We understand that when we do a study, there's uncertainty. We have to put a margin of error around our estimate. That margin of error is what we call the 95% confidence interval. So the 95% confidence interval here is 0.3 to 0.6. That gives us a plausible range of values for the true effect. ", ['minute', 'confidence', 'interval', 'lot', 'time', 'confidence', 'intervals', 'upcoming', 'week', 'course', 'understanding', 'confidence', 'interval', 'risk', 'study', 'point', 'estimate', 'study', 'indicates', 'halving', 'risk', 'study', 'course', 'study', 'uncertainty', 'words', 'effect', 'Vioxx', 'GI', 'events', 'study', 'uncertainty', 'margin', 'error', 'estimate', 'margin', 'error', '%', 'confidence', 'interval', '%', 'confidence', 'interval', 'range', 'values', 'effect']), 0.3244428422615251, 0.3816809593723694)
((6, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 8 mod5.srt', "You have a 6.7% increase in your prevalence of or risk of menstrual irregularity. We can also do the 95% confidence limits, so if I exponentiate those confidence limits from the previous page, I get a lower bound of about 0.04, 1.04. And an upper bound of about 1.096. So that can be interpreted as the adjusted risk ratio. You don't need to worry about this business about odds ratios being misleading, you can avoid them altogether. And just to compare, remember when we did the odds ratio from logistic regression, what we got there was point estimate of 1.12 and a 95% confidence interval of 1.06 to 1.1 about 1.19. So the width of the confidence interval is similar here but these are all inflated. ", ['%', 'increase', 'prevalence', 'irregularity', '%', 'confidence', 'limits', 'confidence', 'limits', 'page', 'bound', 'bound', 'risk', 'ratio', 'business', 'odds', 'ratios', 'remember', 'odds', 'regression', 'point', 'estimate', '%', 'confidence', 'interval', 'width', 'confidence', 'interval']), 0.31426968052735443, 0.3547164077221215)
((3, '/Users/jag/Downloads/Stanford medstats/Unit 5 Homework with answers.srt', "the odd ratio is given by by 10 this one by 40, this one by 5, this one by 55. This turns out to be .586. The 95% confidence in material is now given in new by plus or minus one point. 9 6, times the standard error. In this case it's 1.01, plus, minus 1.96 times .586. Which is an old issue of 1.44 To 2.16. This question wants us to exponentiate the values we found in d. So we have the confidence interval in terms of odds ratio as opposed to log of odds ratio. So that would be an exponential of minus 1.4, which would be the lower bound. Exponential of 2.16 which is an exponential of the upper bound and ", ['ratio', '%', 'confidence', 'material', 'point', 'times', 'error', 'case', 'times', 'issue', 'question', 'values', 'd.', 'So', 'confidence', 'interval', 'terms', 'odds', 'odds', 'minus', 'bound', 'bound']), 0.21081851067789195, 0.3376666948801147)
((1, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 7 mod1.srt', "And if you think about the way we do studies. If I say, well I went out and I collected my sample of 100 people, and I calculated a mean, then you would say, well what do you mean by the distribution of that statistic? That statistic only has one number. But remember that in statistics, what we do is we think about theoretically if I could repeat that same experiment over and over and over and over again, what would the distribution of those means be? In other words, if I could go out to my original population and take a new sample of 100 and calculate the mean in that sample, and then I could do that again and again and again and again, I could actually literally see what the distribution of the means was. And, of course, that information is important because I need it in order to be able to calculate P values and confidence intervals. So imagine that I could do somehow a repeated sampling like this and I could, since we've been talking about beta coefficients from logistic regression, I could calculate a beta coefficient from logistic regression again, again and again in multiple samples of the same size. ", ['way', 'studies', 'sample', 'people', 'distribution', 'number', 'remember', 'statistics', 'experiment', 'distribution', 'means', 'words', 'population', 'sample', 'sample', 'distribution', 'means', 'course', 'information', 'order', 'P', 'values', 'confidence', 'intervals', 'imagine', 'beta', 'coefficients', 'regression', 'beta', 'coefficient', 'regression', 'multiple', 'samples', 'size']), 0.16012815380508716, 0.32133768742990465)
((14, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod2.srt', "I compared the natural log of the odds ratio that I observed with its standard, with its standard error. Simple to calculate. The 95% confidence interval is similarly going to be based on the standard error. Notice, though, that we're going to be calculating the 95% confidence interval for the natural log of the odds ratio by adding and subtracting 1.96. Times the standard error. That will give me a confidence interval for the natural log of the odds ratio. You don't want to report that in your paper, though, because odds ratios are hard enough to understand. Don't bother with natural logs of odds ratios, nobody can understand that. So what we're going to do, of course, at the end of the day, is take the upper and lower bounds and exponentiate them to get back to an actual odds ratio measure. And that's going to have an interesting effect, that, if you've ever noticed odds ratios and 95% confidence intervals in the literature, a lot of people would remark, well, they're not symmetric. If you look at the confidence interval, this was the confidence interval from a previous module for an odds ratio. ", ['log', 'odds', 'error', 'Simple', '%', 'confidence', 'interval', 'error', 'Notice', '%', 'confidence', 'interval', 'log', 'odds', 'Times', 'error', 'confidence', 'interval', 'log', 'odds', 'paper', 'odds', 'ratios', 'Do', 'logs', 'odds', 'ratios', 'nobody', 'course', 'end', 'day', 'bounds', 'odds', 'measure', 'effect', 'odds', 'ratios', '%', 'confidence', 'intervals', 'literature', 'lot', 'people', 'confidence', 'interval', 'confidence', 'interval', 'module', 'odds']), 0.30207229640147215, 0.3194831257847559)
QUERY: Does confession mean many post-hoc analysis or also when you specify them a priori?
*************************
((11, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 5 Module 2 Part 1.srt', "the treated group are no longer getting the benefits of the drug, we're going to expect to see a few higher deaths than the treated group. So maybe something around 31, if you want the math. So the observed relative risk is going to be something closer to one. So if you kind of estimate it out, you'd expect to see something around 0.7 even though the true value is 0.5. You're getting closer to the null, it's a diluted effect, and that's the general effect that doing an intention-to-treat analysis is going to have on your results. So, that often, for many randomized, for most randomized trials, they do some kind of intention-to-treat analysis as their primary analysis. Let me just review what the alternatives to intention-to-treat analysis would be. So you see in the literature a lot of times what people actually do is not a pure, pure intention-to-treat analysis but is actually what's called a modified intention-to-treat analysis. This is actually probably more common in the literature. A pure intention-to-treat would say, once randomized always analysed, and, ", ['group', 'benefits', 'drug', 'deaths', 'group', 'something', 'math', 'risk', 'something', 'closer', 'something', 'value', 'null', 'effect', 'effect', 'analysis', 'results', 'trials', 'kind', 'analysis', 'analysis', 'Let', 'alternatives', 'analysis', 'literature', 'lot', 'times', 'people', 'pure', 'pure', 'analysis', 'analysis', 'literature', 'pure', 'intention-to-treat']), 0.29452841620428954, 0.13704208248251118)
((6, '/Users/jag/Downloads/Stanford medstats/Genomic Expression  How to Analyze It.srt', "And you can see that not all of them do, so that what this heirchical clustering has done is discovered that diffuse large B-cell lymphomas are heterogeneous, that some of them are germinal center type, and some of them are non-germinal center type. And then you can begin to do things that are medically relevant. So let's move on. A second approach is to use supervised analysis. Supervised analysis looks at the data with supervision. You have information about your samples, and supervised analysis uses that information. Suppose you have two classes of samples. Thing one and thing two. Supervised analysis will find the genes with expression profiles that differ between thing one and thing two. An example of how this is done is the method known as Significant Analysis of Microarray or SAM. ", ['clustering', 'diffuse', 'B-cell', 'lymphomas', 'center', 'type', 'center', 'type', 'things', 'let', 'move', 'approach', 'analysis', 'analysis', 'looks', 'data', 'supervision', 'information', 'samples', 'analysis', 'uses', 'information', 'Suppose', 'classes', 'samples', 'thing', 'analysis', 'genes', 'expression', 'profiles', 'thing', 'thing', 'example', 'method', 'Significant', 'Analysis', 'Microarray', 'SAM']), 0.22360679774997896, 0.13339791014471267)
((14, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 5 Module 2 Part 1.srt', "where you're truly not doing intention-to-treat in any way anymore, would include the as-treated analysis and the per-protocol analysis. So an as-treated analysis, would analyze participants according to what actually happened, what treatments they actually took. So if you think of the women's health initiative, study for example. In Cox regression you can have time changing predictors, so that's great because you can see over time women started, started drugs or stopped drugs. You could change their status. You could change which group they were in over time. And that would be an as-treated analysis. It would actually incorporate the fact that they dropped in or off, or out, you know. Onto or off of drugs. That would be an as treated analysis. A per-protocol analysis is even stricter than that. And it's going to drop, exclude everybody from the analysis, who, essentially wasn't a perfect participant. Who violated the study protocol in any way, so they, they switched groups, or they didn't come in for all of their followup measurements or, you know, ", ['way', 'anymore', 'analysis', 'per-protocol', 'analysis', 'analysis', 'participants', 'treatments', 'women', 'health', 'initiative', 'study', 'example', 'Cox', 'regression', 'time', 'predictors', 'time', 'women', 'drugs', 'drugs', 'status', 'group', 'time', 'analysis', 'fact', 'drugs', 'analysis', 'analysis', 'everybody', 'analysis', 'perfect', 'participant', 'study', 'protocol', 'way', 'groups', 'followup', 'measurements']), 0.3146266024828463, 0.1324088144881441)
((8, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 5 Module 2 Part 1.srt', "If you prescribe it, how likely is it to have a benefit. So it's getting more at effectiveness. Rather than efficacy, which is probably what we care most about, in most cases, we want to look at real life opposed to perfect use. So those sort of the main two rationales for using intention to treat analysis. Now, what effect does intention to treat have on a statistical analysis? So basically, we are preserving the benefits of randomization, so we're not going to introduce any bias here. We're going to avoid that you know, confounding. However, we there's going to be all of this noise that's introduced by the fact that people didn't actually do what we think they did, or what they should have done. And so that noise is going to have the effect of diluting our effect sizes. So it's going to reduce our statistical power. If you use an intention-to-treat analysis, you tend to underestimate treatment effects. So you don't get a bias in there, but you tend to, bring things closer to the null. We had this additional variability which sort of waters down the results, ", ['benefit', 'effectiveness', 'efficacy', 'cases', 'life', 'use', 'sort', 'rationales', 'intention', 'analysis', 'effect', 'intention', 'analysis', 'benefits', 'randomization', 'bias', 'noise', 'fact', 'people', 'noise', 'effect', 'effect', 'sizes', 'power', 'analysis', 'treatment', 'effects', 'bias', 'things', 'null', 'variability', 'sort', 'waters', 'results']), 0.18257418583505533, 0.12771747666234076)
((8, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 4 Module 2 part 1.srt', "So we are able to now make sure that we're not counting somebody as, as the the effect of winning, we're not having it kind of act retroactively. So this is a great way to analyze these data. It did yield a smaller effect size. It yielded an effect size of a relative reduction of 20%. And they don't tell us in the paper is that adjusted for any covariance or not. I'm kind of guessing that it's probably just a basic analysis. An additional adjustment may attenuate this even a little bit further. Notice it's starting to boarder on being nonsignificant here. So one of the criticisms of that paper, you know, later on, is people said, well, that more proper analysis was buried in the paper and the, the authors chose to highlight kind of the, the less good analysis. And indeed, when a few years later, some other authors did a reanalysis of the original data, this is a kind of a busy table, but I'll just want to highlight one thing, in their new analysis, they didn't do time-dependent covariance, they did something slightly different. ", ['somebody', 'effect', 'act', 'way', 'data', 'yield', 'effect', 'size', 'effect', 'size', 'reduction', '%', 'paper', 'covariance', 'guessing', 'analysis', 'adjustment', 'bit', 'Notice', 'criticisms', 'paper', 'people', 'analysis', 'paper', 'authors', 'highlight', 'kind', 'analysis', 'years', 'authors', 'reanalysis', 'data', 'kind', 'table', 'thing', 'analysis', 'covariance', 'something']), 0.21081851067789195, 0.12576875581258676)
((11, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit6 Mod6.srt', "So in the, this analysis, even after adjusting for propensity score, the Ross procedure actually looks more harmful than the mechanical valve. Now, this was a secondary analysis, the matched analysis was the main one, and the authors thought, perhaps this was driven by the fact, this finding was driven by the fact that, if you look at the distribution of propensity scores, for the Ross patients, it was pretty skewed, it had a long right T And so, that's skewness might be driving this hazard ratio. There may not have been any matches, any, anybody similar to all of these people in the tail, may not have had any similar, people in the mechanical valve group, and you can't really adjust there for, for it. So we saw in the match analysis that there was probably just Some people who don't have a good comparator, therefore you can't really completely adjust for confounding here. And so something's going on that's a little funny. But if anything, the Ross procedure is worse then the Now I've told ", ['analysis', 'propensity', 'score', 'Ross', 'procedure', 'valve', 'analysis', 'analysis', 'authors', 'fact', 'finding', 'fact', 'distribution', 'propensity', 'scores', 'Ross', 'patients', 'right', 'T', 'skewness', 'hazard', 'ratio', 'matches', 'anybody', 'people', 'tail', 'people', 'valve', 'group', 'match', 'analysis', 'people', 'comparator', 'something', 'funny', 'anything', 'Ross', 'procedure', 'Now', 'I']), 0.21081851067789195, 0.12576875581258676)
((2, '/Users/jag/Downloads/Stanford medstats/Unit 8 Module 4 part 1.srt', "also to improve your predictions. So now instead of saying it's our political bent that predicts whether or not we like Obama, maybe there are other variables I can come up with that also help improve that prediction beyond political bent. Are there other things that predict who's going to like Obama? Another thing that you can do in a multivariate analysis is tests for interaction. I'm actually not going to talk about interactions in any detail this week. I'm going to give this a lot of time next week. I sort of alluded to interactions in Week So you may be familiar with the concept. But it's something that you can easily test for in the multivariate regression analysis context. Again i'm going to get to that next week. In this module I'm going to work on the first two of these. Controlling for confounders and improving predictions. So starting with the first one. How does multivariate analysis how can it be used to control for confounding? I'm just going to illustrate this with an example. ", ['predictions', 'bent', 'Obama', 'variables', 'prediction', 'bent', 'Are', 'things', 'predict', 'Obama', 'thing', 'multivariate', 'analysis', 'tests', 'interaction', 'interactions', 'detail', 'week', 'lot', 'time', 'week', 'interactions', 'Week', 'So', 'concept', 'something', 'multivariate', 'regression', 'analysis', 'context', 'week', 'module', 'confounders', 'predictions', 'analysis', 'control', 'confounding', 'example']), 0.1732050807568877, 0.12116343697871898)
((2, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 5 Module 2 Part 2.srt', "they want to do something proactive, so they decide to go and get the surgery anyway, so a lot of switching groups in this trail. Now, let me show you the results. So there's Three ways that the data were presented. So first of all, the primary analysis for the randomized trial was an intention-to-treat analysis. Generally, most people with randomized trial data will choose an intention-to-treat analysis as primary for the reasons we've talked about. They also then as a secondary analysis broke intention-to-treat and said well let's look at who actually got surgery versus who didn't. And then finally they have the results of the observational cohort study where people self-selected. So any intention to treat analysis, they literally said okay, if you were randomized to surgery, we're going to take your change in bodily pain, physical function and disability, your change scores over the time period of the study. And we're going to average together everybody who was randomized to surgery, and then we're going to average together everybody who was randomized to the non-operative group. And when they do that, they see that there is a slight, slight benefit for surgery. ", ['something', 'surgery', 'anyway', 'lot', 'switching', 'groups', 'trail', 'results', 'Three', 'ways', 'data', 'analysis', 'trial', 'analysis', 'people', 'trial', 'data', 'analysis', 'reasons', 'analysis', 'broke', 'look', 'surgery', 'versus', 'results', 'cohort', 'study', 'people', 'intention', 'analysis', 'okay', 'change', 'bodily', 'pain', 'function', 'disability', 'change', 'scores', 'time', 'period', 'study', 'together', 'everybody', 'together', 'everybody', 'group', 'slight', 'slight', 'benefit', 'surgery']), 0.23063280200722125, 0.12036439548171013)
QUERY: If the rsults of ANOVA (P Value) is not significant >0.05 but the Bartlett's corrected P value is <0.05. What conclusions can one draw?
*************************
((24, '/Users/jag/Downloads/Stanford medstats/Unit 6 Module 3 part 1.srt', "Then you allow the next p-value, you're a little less stringent. You then would say that the next cutoff statistical significance would make it a little less conservative, so [INAUDIBLE], now, we're going to compare p2 the next p-value up in significance to 0.05 divided by 9 rather than 0.05 divided 10. In other words you P value cut off is a little less stringent. If that next highest P value makes it, it's significant at that level then you continue to the next step. If not, you stop and so on and so forth. So this is going to be less conservative than the [INAUDIBLE], than the Bonferroni because some of the p-values are not going to be compared against that most stringent cut off level. Now the Hochberg is the exact same in the reverse. So the Hochberg says start with the largest. The least significant p-value and compare that to just plain old alpha just the 0.05 your uncorrected p-value. If that's significant every thing is significant. If your worst P value is 0.05 everything will be significant. ", ['p-value', 'cutoff', 'significance', 'INAUDIBLE', 'p2', 'p-value', 'significance', 'words', 'P', 'value', 'highest', 'P', 'value', 'level', 'step', '[', 'INAUDIBLE', ']', 'Bonferroni', 'p-values', 'cut', 'level', 'Hochberg', 'same', 'reverse', 'Hochberg', 'start', 'compare', 'alpha', 'thing', 'worst', 'P', 'value', 'everything']), 0.32142857142857145, 0.32948576151502934)
((12, '/Users/jag/Downloads/Stanford medstats/Unit 7 Module 2 part 1.srt', "If I took that F statistic to an F chart, I have to plug in, I have to tell the computer, the chart, that I have a numerator degrees of freedom of 2, a denominator degrees of freedom of 30. I can get a corresponding P value. The P value does come out to be highly Statistically significant here, it's something like .00, less than .001. Because under the null hypothesis the f should come out to be one. 6.6 definitely exceeds that. Again don't worry too much about the details of this test. All of this will be calculated for you by the computer. Just get the variable, the basic idea here. Here. Now, we get this statistically significant ANOVA here, that tells us that at least one of the groups differs, it doesn't tell us which one differ. If we then want to answer that question about where those specific differences lie, we would then have to go in and do some post hoc tests. Some actual comparing specific groups but because we ", ['F', 'F', 'chart', 'computer', 'chart', 'numerator', 'degrees', 'freedom', 'denominator', 'degrees', 'freedom', 'corresponding', 'P', 'value', 'P', 'value', 'something', 'null', 'hypothesis', 'f', 'details', 'test', 'computer', 'idea', 'ANOVA', 'tells', 'groups', 'differs', 'question', 'differences', 'post', 'hoc', 'tests', 'comparing', 'groups']), 0.2619684159977919, 0.31597721149393854)
((6, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod5.srt', "any that she doesn't that she guesses to be milk poured first can't be tea poured first and vice versa. So that's the outcome now we look at that we say well she did you know if we were. If it was chance you'd probably only get about two right, but you know, probably three right might also be in the realm of what we might expect of a chance. We might, we might have been amazed has she gotten all four right. So, Fisher wanted to calculate a P value here. How we are going to calculate a P value here? Notice, very sparse numbers, we got a lot of one cells. Clearly we don't want to do anything, we don't want to do a chi-squared. Which is approximating a normal [UNKNOWN] binomial. We want to do an exact probability. So how do we do this? So step one of the Fisher's test is actually to identify the observe table and all the tables that represent a more extreme outcome. Okay, so what does that mean? So this is the observe table, we just saw that on the previous slide. That's what we observe but remember that the P value is what we observe Everything more extreme. Well what is, what would be more extreme in this case? ", ['milk', 'ca', 'vice', 'versa', 'outcome', 'chance', 'realm', 'chance', 'Fisher', 'P', 'value', 'P', 'value', 'Notice', 'numbers', 'lot', 'cells', 'anything', '[', 'UNKNOWN', ']', 'probability', 'step', 'Fisher', 'test', 'observe', 'table', 'tables', 'outcome', 'Okay', 'observe', 'table', 'slide', 'remember', 'P', 'value', 'Everything', 'Well', 'case']), 0.3079736598731197, 0.3156935781995815)
((13, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod5.srt', "not equal to double the one tailed p-value because there is, sometimes, the Fishers exact test does not have a symmetric distribution. And the area under one tail might be different than the area under the other tail. So the way to calculate the two-tailed is just to add up the p-value of what you observed in all other observed tables that had equal or smaller P values. That happens to work out to be double the one-sided P value here, but, as you'll see in your homework that isn't always the case. That's a hint on your homework, by the way. So those are the P values you get out of SAS again, you're going to want to report the two-sided P value at the end of the day. And I just show you the output from SAS when I ran this fisher's exact test in SAS indeed I get a number of P a P values here so here's that last cited P value I was telling you about we're generally going to ignore that. Here's that right-sided P value he was talking about. That's the one-tailed probability we're also going to ignore that. The table probability by the way is the probability of the observed table, of that one observed table. I get it we usually are going to ignore that too but ", ['Fishers', 'exact', 'test', 'distribution', 'area', 'tail', 'area', 'tail', 'way', 'tables', 'P', 'values', 'P', 'value', 'homework', 'case', 'hint', 'homework', 'way', 'P', 'SAS', 'P', 'value', 'end', 'day', 'output', 'SAS', 'fisher', 'test', 'SAS', 'number', 'P', 'P', 'P', 'value', 'P', 'value', 'probability', 'table', 'probability', 'way', 'probability']), 0.4583492485141056, 0.294357091271757)
((4, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 7 mod2.srt', "only 10% of the population or less had that, had the positive value on that one. Since so few people had it they thought that it's not going to be a good predictor because it's just so rare. So they screened those out as well. Then whatever predictors were left, after this initial screen, they're going to move forward to a multivariate logistic regression. So they just put them all into a multivariate logistic regression. The outcome is delirium, and whatever predictors made it through that screen are now in the model. And then they're going to prune the model, using backward elimination. Any variables that have a P value over 0.10, are going to be taken out of the model, any P variables that have a P less that 0.10 are going to be retained in the model. Now backward elimination could be done manually, meaning that somebody goes through and looks at the p values, and takes out the variables that are higher than that, or it can also be done au, automatically, with an automatic algorithm in the computer. And then they're just saying they did this to Come, come up with independent risk factors for delirium. ", ['%', 'population', 'value', 'people', 'predictor', 'predictors', 'screen', 'multivariate', 'regression', 'regression', 'outcome', 'delirium', 'predictors', 'screen', 'model', 'model', 'backward', 'elimination', 'variables', 'P', 'value', 'model', 'P', 'P', 'model', 'elimination', 'somebody', 'looks', 'p', 'values', 'variables', 'au', 'algorithm', 'computer', 'Come', 'risk', 'factors', 'delirium']), 0.25555062599997597, 0.2817489163715822)
((14, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod5.srt', "what we care about is the two-sided P value which happens to be at the very end of our output in SAS at least, and that's the P value that we're going to report. I want to just show you that I also asked SAS to run the chi square test for that two by two table. So my two by two table here again looked like this. I have 3, 1, 1, 3. I can ask SAS to run a chi square test on And, and it will run it. When it did that chi square test were clearly violating the assumption of not having sparse data here. It's probably a bad idea to approximate here with a, a, discreet distribution with a continuous one we get a chi-Square value of 0.15. You can see that two sided P value of 0.15 is very different tan the two sided P value we got from the Fisher's exact. Which tells you that, you know, again When you have sparse data it can really make a difference if you correctly chose the Fisher's exact or try to apply the Chi-Squared. Luckily SAS pretty nice, it gives you a little warning down here. Says 100% of the cells have expected counts less than 5, ", ['P', 'value', 'happens', 'end', 'output', 'SAS', 'P', 'value', 'SAS', 'chi', 'square', 'test', 'SAS', 'chi', 'square', 'test', 'chi', 'square', 'test', 'assumption', 'sparse', 'data', 'idea', 'discreet', 'distribution', 'value', 'P', 'value', 'tan', 'P', 'value', 'Fisher', 'tells', 'data', 'difference', 'Fisher', 'try', 'SAS', '%', 'cells', 'counts']), 0.3358825530637211, 0.27680363353986487)
((9, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 7 Module 4.srt', "My sum of squares due to different time points is 224. I can then calculate an F statistic and a P value. The P value comes out to be out 0.117. All right so that's the incorrect way. There's something we haven't accounted for here, which is the fact that we have different subjects in the dataset. So there is. I'm now going to do this the correct way, running a repeated measures ANOVA. And basically what's going to happen here is that we're going to explain away some of this unaccounted for, this error variability. This sum of squares error or sum of squares within. Which was 700 and, 676. That's going to be reduced by accounting the fact that we have the same subjects. We're going to account for the differences between different subjects. Almost as, as if subject was itself a grouping variable. So we're going to end up reducing some of this variance. If you reduce the unexplained variability, your P values are going to get smaller, right? So we, we can explain some of the variability by just the fact that we have different subjects in the data set. ", ['sum', 'squares', 'time', 'points', 'F', 'P', 'value', 'P', 'value', 'right', 'incorrect', 'way', 'something', 'fact', 'subjects', 'dataset', 'correct', 'way', 'measures', 'ANOVA', 'error', 'variability', 'sum', 'squares', 'error', 'sum', 'squares', 'accounting', 'fact', 'subjects', 'account', 'differences', 'subjects', 'variance', 'variability', 'P', 'values', 'variability', 'fact', 'subjects', 'data', 'set']), 0.253546276418555, 0.2766509967846643)
((2, '/Users/jag/Downloads/Stanford medstats/RExercise3.srt', "Okay? So that's another option for you to use if you want to. So I want to scatter plot. So I'm going to go to Plots. Here. I'm going to select Scatterplots. And I'm going to unselect common axis. I think it's actually a little bit easier to interpret when you don't have a lot of variables correlating with each other. To not have a common axis. So, then I'm going to hit Okay. I'm curious about what Options has, so I'm going to click on that now. I see that it has options for different sets of output that we want. So this has options for whether we want a two sided P value or a one sided P value, and what confidence level we want our output to be set at, and then of course the N, the confidence interval. The test statistic and the P-value. We want all of these in our output, so I'm going to leave them checked. Pearson's correlation is selected by default, so that's good. And we are ready to run our test. So you'll notice a few things happen here. ", ['Okay', 'option', 'plot', 'Plots', 'Scatterplots', 'axis', 'bit', 'lot', 'variables', 'axis', 'Okay', 'Options', 'options', 'sets', 'output', 'options', 'P', 'value', 'P', 'value', 'confidence', 'level', 'output', 'course', 'N', 'confidence', 'interval', 'test', 'output', 'Pearson', 'correlation', 'default', 'test', 'things']), 0.21821789023599236, 0.2707050049754204)
QUERY: I also have the same question! When do we use one-way or two-way ANOVA?! And what is the differences between them?!
*************************
((15, '/Users/jag/Downloads/Stanford medstats/RExercise2.srt', "So K-Sample Test actually just means multiple independent samples, so K just being a random number. So a random number of samples And to back this up we also see in the bottom left-hand corner, the default test here is a one-way ANOVA. So to run an ANOVA in Deducer even though we won't actually run one now, you'd go to Analysis. Select key sample test and then select your variables in this menu. Moving on to question six. So we're now interested in seeing whether there's a difference in whether one considers themselves book smart or street smart. Based upon whether you played a varsity sport in high school. So, 6A asks us what statistical test we'd use in this situation. What's different about this question. ", ['Test', 'independent', 'samples', 'K', 'random', 'number', 'random', 'number', 'samples', 'up', 'bottom', 'left-hand', 'corner', 'default', 'test', 'ANOVA', 'ANOVA', 'Deducer', 'Analysis', 'Select', 'sample', 'test', 'variables', 'menu', 'question', 'seeing', 'difference', 'considers', 'smart', 'street', 'smart', 'varsity', 'sport', 'school', 'asks', 'test', 'situation', 'question']), 0.2182178902359924, 0.18473750280040507)
((3, '/Users/jag/Downloads/Stanford medstats/Unit 7 Module 2 part 2.srt', "repeated-measures ANOVA. And it answers the following questions. You get a couple of things out of repeated-measures ANOVA. So if you've got say two groups or more than two groups that you're comparing over multiple- Time points. It can answer the following question. First of all, it can answer just whether not there are significant differences across time periods. Does your outcome change at all across time periods? So, that's the time factor in repeated-measures ANOVA. You can also ask whether or not there were any significant differences between your groups, if you've got multiple groups you're comparing over time. [UNKNOWN] you can ask whether or not there are significant differences in the changes over time between those groups. So you have what is called a time effect and group effect in a group by time effect and usually what we care about like in that depression example I've just showed you, we care about the group by time effect, we cared whether or not there were differences in the changes in depression over time between Between the groups. And repeated measures ANOVA can answer that question for you. It's very good for longitudinal data. ", ['repeated-measures', 'ANOVA', 'questions', 'couple', 'things', 'repeated-measures', 'ANOVA', 'say', 'groups', 'groups', 'Time', 'points', 'question', 'First', 'differences', 'time', 'periods', 'outcome', 'change', 'across', 'time', 'periods', 'time', 'factor', 'repeated-measures', 'ANOVA', 'differences', 'groups', 'multiple', 'groups', 'time', '[', 'UNKNOWN', ']', 'differences', 'changes', 'time', 'groups', 'time', 'effect', 'group', 'effect', 'group', 'time', 'effect', 'depression', 'example', 'group', 'time', 'effect', 'differences', 'changes', 'depression', 'time', 'Between', 'groups', 'measures', 'ANOVA', 'question', 'data']), 0.2765006318046655, 0.1787780062206234)
((4, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 6 Module 1 Part 1.srt', "The null hypothesis for an ANOVA is that the means in all the groups that you're looking at, however many there are, let's say we have four here, that the means for your outcome variable are equal. This is a global test because notice that if we reject this null hypothesis, all we can say is that at least one of those means is not equal to the others. It doesn't tell us specifically where those differences lie. So after you do an ANOVA, if you have a significant finding. Then you may want to go in and find exactly, to specify exactly where those differences lie. But that's going to require some kind of correction for multiple comparisons if you want to look at pairwise comparisons between diff, specific different groups. What are we basically doing when we do an ANOVA? We're going to be doing some kind of F test, and remember the idea. The F test, is that we're going to be comparing the variance between groups, to the variance within groups. And again, I'm assuming that you've had a course that's covered ANOVA before, so that's why I'm going over this quickly, this is just to serve as a review. ", ['null', 'hypothesis', 'ANOVA', 'means', 'groups', 'say', 'means', 'outcome', 'test', 'notice', 'null', 'means', 'others', 'differences', 'ANOVA', 'finding', 'find', 'differences', 'kind', 'correction', 'multiple', 'comparisons', 'pairwise', 'comparisons', 'diff', 'groups', 'ANOVA', 'kind', 'F', 'test', 'remember', 'idea', 'F', 'test', 'variance', 'groups', 'variance', 'groups', 'course', 'ANOVA', 'review']), 0.2596453934447493, 0.17653470068738594)
((12, '/Users/jag/Downloads/Stanford medstats/Unit 7 Module 2 part 1.srt', "If I took that F statistic to an F chart, I have to plug in, I have to tell the computer, the chart, that I have a numerator degrees of freedom of 2, a denominator degrees of freedom of 30. I can get a corresponding P value. The P value does come out to be highly Statistically significant here, it's something like .00, less than .001. Because under the null hypothesis the f should come out to be one. 6.6 definitely exceeds that. Again don't worry too much about the details of this test. All of this will be calculated for you by the computer. Just get the variable, the basic idea here. Here. Now, we get this statistically significant ANOVA here, that tells us that at least one of the groups differs, it doesn't tell us which one differ. If we then want to answer that question about where those specific differences lie, we would then have to go in and do some post hoc tests. Some actual comparing specific groups but because we ", ['F', 'F', 'chart', 'computer', 'chart', 'numerator', 'degrees', 'freedom', 'denominator', 'degrees', 'freedom', 'corresponding', 'P', 'value', 'P', 'value', 'something', 'null', 'hypothesis', 'f', 'details', 'test', 'computer', 'idea', 'ANOVA', 'tells', 'groups', 'differs', 'question', 'differences', 'post', 'hoc', 'tests', 'comparing', 'groups']), 0.17149858514250885, 0.17149858514250885)
((1, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 7 Module 1.srt', "Running say an ANCOVA, adjusting for baseline differences, that's called an end-point analysis. So there's a number of ways you could simplify the picture. These methods of course, lose some information because you were kind of again simplifying or ignoring the repeated outcome. A second strategy which is also been around for a long time, and you'll see in the literature, is to use some kind of repeated measures ANOVA. And I'll review how to do a repeated measures ANOVA this week. Repeated measures ANOVA is somewhat limited. It's an ANOVA, so you have to have a categorical predictor in there. There's also some fairly restrictive assumptions. And you have to fill in missing data, and things like that, but again, sometimes a simple method is easy to get your head around, so, and you'll see these a lot in the literature, so I'll show you that, those this week. The more sophisticated methods, we're going to go over in the next two weeks, and those are the generalized estimating equations in the mixed models. These are more sophisticated ways of handling repeated measures outcome. ", ['ANCOVA', 'baseline', 'differences', 'analysis', 'number', 'ways', 'picture', 'methods', 'course', 'lose', 'information', 'again', 'outcome', 'strategy', 'time', 'literature', 'kind', 'measures', 'ANOVA', 'measures', 'ANOVA', 'week', 'Repeated', 'ANOVA', 'ANOVA', 'predictor', 'assumptions', 'missing', 'data', 'things', 'method', 'head', 'lot', 'literature', 'week', 'methods', 'weeks', 'estimating', 'equations', 'models', 'ways', 'handling', 'measures']), 0.24573659359149524, 0.16642728823993813)
((12, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 7 Module 5.srt', "So so I tend to end up ignoring these, but you have to understand where they come from. So the between-group sum of squares we already calculated was 126. The between-subject variability, the unexplained variability between subjects. So the differences between subjects that aren't due to group but what's left over is this 150. This you can, you can compare what's explained and what's unexplained here in the normal way we would for ANOVA. And you can get a p-value, and this is what I've been calling the group effect. So this is completely ignoring changes over time and just seeing, out of all the differences between subject, the subject, subject one versus two, two versus three, et cetera. How much of the differences are explained by whether or not you're in group A versus B? And how much, many of the differences between subjects are left unexplained? And so, it doesn't quite come out to be statistically significant here. And the picture is, what you're really comparing is, is line, the blue line, is it significantly higher than the red line? ", ['between-group', 'sum', 'squares', 'variability', 'variability', 'subjects', 'differences', 'subjects', 'group', 'way', 'ANOVA', 'group', 'effect', 'changes', 'time', 'differences', 'subject', 'versus', 'versus', 'et', 'cetera', 'differences', 'group', 'versus', 'B', 'differences', 'subjects', 'picture', 'line', 'line', 'red', 'line']), 0.2439750182371333, 0.1652342457021054)
((14, '/Users/jag/Downloads/Stanford medstats/RExercise2.srt', "Heading, and we see that the p-value is equal to 0.19. So this does not change our conclusion. If you remember previously, we had a p-value of 0.21. So it lowers it a little bit actually but, it's still greater than 0.05, so we can not reject the null. Question 5 is asking us What menu option we would select to perform an ANOVA? So, it's basically asking us how we would perform an ANOVA and Deducer and R. Well, if you think about it, an ANOVA is kind of like an extension of a t-test over multiple groups. So it makes sense to have it near the T-test option. So if we're hunting for this, I would go under Analysis and try to look somewhere near the Two Sample Test. And I notice that it says the K-Sample Test, well what is that? ", ['conclusion', 'bit', 'null', 'Question', 'ANOVA', 'ANOVA', 'Deducer', 'R.', 'Well', 'ANOVA', 'extension', 'groups', 'sense', 'option', 'Analysis', 'try', 'Sample', 'Test', 'Test']), 0.23570226039551584, 0.16488255337762678)
((6, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 7 Module 4.srt', "But here's what it would look like. I would have to use the long form of the data because I'm going to be pretending for the moment that I have 24 independent observations. I am not accounting for the fact that person one was measured at multiple time points. So I'm just saying, well imagine I've got 24 independent people. And they're each measured at a different time point. So I'm going to run this. And the long form of the data to achieve that, I'll just show you the code here so you can see what the model is. I am putting in time as my categorical predictor, my outcome variable is score. And I have no you know again, I am assuming 24 independent observations here. This is going to compare the means from each time point as if they were independent samples, so this is analogous to using a two sample t-test. When a paired t-test would have been appropriate. It's going to result in a loss of power. So here's the, the incorrect, the one-way, incorrect one-way ANOVA. And let me show you what we would do. So again, this just, is just doing an ANOVA like we reviewed last week. So I've got my four time points. ", ['form', 'data', 'moment', 'observations', 'fact', 'person', 'multiple', 'time', 'points', 'people', 'time', 'point', 'form', 'data', 'code', 'model', 'time', 'predictor', 'outcome', 'observations', 'means', 'time', 'point', 'samples', 'loss', 'power', 'incorrect', 'one-way', 'incorrect', 'one-way', 'ANOVA', 'ANOVA', 'week', 'time', 'points']), 0.19380063324460367, 0.16406649788441638)
QUERY: Doesn't the example with Fisher's exact test illustrate well the difference between the type I error and the p-value?  The probability of the woman getting 3 out of 4 cups right is 0.23. It seems to me that this is the type I error, namely P(observing the effect | H0). However, it is *not* the same as p-value, which is the probability of observing this or more extreme effect (p = 0.23 + 0.01 = 0.24).  I was under the impression following the discussion of type I and II errors that the type I error is the same as the p-value, but it seems to me that they're not. The wikipedia page also mentions that there seems to be a vast amount of discussion out there about this issue.  Any clarifications/remarks are welcome! R
*************************
((24, '/Users/jag/Downloads/Stanford medstats/Unit 6 Module 3 part 1.srt', "Then you allow the next p-value, you're a little less stringent. You then would say that the next cutoff statistical significance would make it a little less conservative, so [INAUDIBLE], now, we're going to compare p2 the next p-value up in significance to 0.05 divided by 9 rather than 0.05 divided 10. In other words you P value cut off is a little less stringent. If that next highest P value makes it, it's significant at that level then you continue to the next step. If not, you stop and so on and so forth. So this is going to be less conservative than the [INAUDIBLE], than the Bonferroni because some of the p-values are not going to be compared against that most stringent cut off level. Now the Hochberg is the exact same in the reverse. So the Hochberg says start with the largest. The least significant p-value and compare that to just plain old alpha just the 0.05 your uncorrected p-value. If that's significant every thing is significant. If your worst P value is 0.05 everything will be significant. ", ['p-value', 'cutoff', 'significance', 'INAUDIBLE', 'p2', 'p-value', 'significance', 'words', 'P', 'value', 'highest', 'P', 'value', 'level', 'step', '[', 'INAUDIBLE', ']', 'Bonferroni', 'p-values', 'cut', 'level', 'Hochberg', 'same', 'reverse', 'Hochberg', 'start', 'compare', 'alpha', 'thing', 'worst', 'P', 'value', 'everything']), 0.18232322463624387, 0.3657452846829935)
((4, '/Users/jag/Downloads/Stanford medstats/Unit 7 Module 2 part 1.srt', "Of course, there not independent comparisons. But it's very hard to calc, to, to figure out exactly how what this probability I'm about to calculate is, if you don't assume independence. So I'll, I'm going to calculate this probabilty assuming independence. It might be slightly different beause really we dont have indepedence here. But if the, these were three independent tests Each one of those tasks has a 5% chance of a type 1 error. So what's the chance of making at least one type 1 error? That's going to be 1 minus 95% rate to the third. Using our little trick in probability, 1 minus the probability of none is equal to the probability of at least one. So, we're roughly somewhere around a 14% chance of making a type 1 error. That increases our type 1 error rate if we if we run all those three pairwise tests. The Inova, on the other hand, runs just one test, just characterizes one p value. It's called a global test. Because it's looking at all those comparisons essentially at once. So the null hypothesis for the ANOVA, for the one-way ANOVA is that the means of all the groups are equal. So like the means by mode density. ", ['course', 'comparisons', 'probability', 'independence', 'probabilty', 'independence', 'beause', 'indepedence', 'tests', 'tasks', '%', 'chance', 'type', 'error', 'chance', 'type', 'error', 'minus', '%', 'rate', 'trick', 'probability', 'minus', 'probability', 'none', 'probability', '%', 'chance', 'type', 'error', 'type', 'error', 'rate', 'pairwise', 'tests', 'Inova', 'hand', 'test', 'p', 'value', 'test', 'comparisons', 'null', 'hypothesis', 'ANOVA', 'ANOVA', 'means', 'groups', 'means', 'mode', 'density']), 0.3122900632617698, 0.2421886107957282)
((3, '/Users/jag/Downloads/Stanford medstats/Unit 6 Module 1 part 1.srt', "You can only make a type 1 error is the effect isn't real. If the null hypothesis isn't true. You can only make a type 2 error is the effect is real. If the null hypothesis is false. Just kind of keep that in mind. I think the easiest way to understand type 1 and type 2 error. Used to put them in a little box, and I'm going to do that in a moment, but whenever I talk about type one and type two error, it always reminds me of something called Pascal's wager. So, this is a little bit of an aside, but, I was a philosophy major in college, and, if you've ever taken a philosophy 101 class, you've probably had to read. A bunch of proofs from philosophers throughout history about either the existence or non-existence of God, plus there's a lot to think about and talk about in the philosophy literature. One of my favorite proofs of all the ones I've had to read, was something called Pascal's Wager. And I really like it, because it's simple, it's a simple logical, rational argument, rather than these kind of long winded, drawn out proofs that were hard to read. ", ['type', 'error', 'effect', 'null', 'hypothesis', 'type', 'error', 'effect', 'null', 'hypothesis', 'false', 'keep', 'mind', 'way', 'type', 'type', 'error', 'box', 'moment', 'type', 'type', 'error', 'something', 'Pascal', 'wager', 'bit', 'aside', 'philosophy', 'college', 'philosophy', 'class', 'bunch', 'proofs', 'philosophers', 'history', 'existence', 'non-existence', 'God', 'lot', 'philosophy', 'literature', 'proofs', 'ones', 'something', 'Pascal', 'Wager', 'argument', 'kind', 'proofs']), 0.3267097770509697, 0.22206527559921052)
((2, '/Users/jag/Downloads/Stanford medstats/Unit 6 Module 1 part 1.srt', "So, at the end of the day we have to make a binary decision. We either reject the null or we don't. That decision can have some errors associated with it. Let's say that there, that the null hypothesis is true. There really isn't an effect. But we erroneously reject the null hypothesis when we shouldn't, that's a false positive. We've made a false positive. We call that a type 1 error sometimes also called an alpha. On the other hand let's say that the null hypothesis is false and there is actually an effect. But we failed to reject the null hypothesis. That's a false negative error. We call that a type 2 error it's also known as beta. Directly related to the type 2 error is something called statistical power. Statistical power is just the complement of the type 2 error. If the probabilty that is there really is a effect. If the null the hypothesis is false. It's the probability that we're going to find that effect in our study. Notice that all of these have a sneakly conditional. ", ['end', 'day', 'decision', 'null', 'decision', 'errors', 'Let', 'say', 'null', 'hypothesis', 'effect', 'null', 'hypothesis', 'false', 'false', 'type', 'error', 'alpha', 'hand', 'let', 'say', 'null', 'hypothesis', 'false', 'effect', 'null', 'hypothesis', 'false', 'error', 'type', 'error', 'beta', 'type', 'error', 'something', 'power', 'power', 'complement', 'type', 'error', 'probabilty', 'effect', 'null', 'hypothesis', 'false', 'probability', 'effect', 'study', 'Notice']), 0.25761060260835605, 0.21145137435917333)
((10, '/Users/jag/Downloads/Stanford medstats/Unit 9 Module 4.srt', "So, we have no evidence that the effect of the drug differs by counseling type. Our main conclusion, then. Our correct take home message would be that the drug improves quitting rates over counseling alone, period. There's a main effect for a drug, there's no main effect for counseling type and there's no interaction between drug, and counseling type. It's not statistically significant, at least, that we don't have evidence that the drug behaves differently in those two groups, at least not from this study. The second example I'm going to show you is a cross-sectional study of about 1700 men and women. In this study, they actually did formally test for interaction and they did find a significant interaction. This study was looking at the relationship between sleep duration and sleep problems, and hypertension. It's a cross-sectional study but they treated hypertension as the outcome variable. Hypertension is binary, yes, no. So, we're going to be using, they did use, logistic regression. ", ['evidence', 'effect', 'drug', 'differs', 'counseling', 'type', 'conclusion', 'correct', 'home', 'message', 'drug', 'rates', 'counseling', 'period', 'effect', 'drug', 'effect', 'counseling', 'type', 'interaction', 'drug', 'counseling', 'type', 'evidence', 'drug', 'groups', 'study', 'example', 'study', 'men', 'women', 'study', 'test', 'interaction', 'interaction', 'study', 'relationship', 'sleep', 'duration', 'problems', 'hypertension', 'study', 'hypertension', 'outcome', 'Hypertension', 'regression']), 0.22645540682891915, 0.20923058960074983)
((7, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 7 mod3 take2.srt', "doing these automated selection procedures. I'll show you an example in a minute where we can guess that the, the type 1 error rate was about 16% with automated selection procedures in a particular simulation, I'll show you. So there, the p-values, your p-values are going to be underestimated, so your type 1 error is going to be much higher. So if you use a p-value cut off of 0.05, the true type 1 error might be 16%, the true significance level might be 16%. So your p-values are going to be much too small. You're also going to over estimate your effect sizes. So all your betas are going to be biased a little bit too high. You're going to be a little bit overly optimistic. And also you're going to be overfitting your model, which just means that you are fitting to all the little quirks in your data. Which means if you calculate something about how well does your model fit your data points, it's going to look like it fits them better than it does. It's going to look like the model is more accurate than it actually is. ", ['selection', 'procedures', 'example', 'minute', 'type', 'error', 'rate', '%', 'selection', 'procedures', 'simulation', 'p-values', 'p-values', 'type', 'error', 'cut', 'type', 'error', '%', 'significance', 'level', '%', 'p-values', 'estimate', 'effect', 'sizes', 'betas', 'bit', 'bit', 'model', 'quirks', 'data', 'something', 'model', 'fit', 'data', 'points', 'model']), 0.21341639745917707, 0.20756761196984394)
((8, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 7 mod3 take2.srt', "So you're going to get model overfitting. The underestimation of p-values and the overestimation of effect sizes, those are important. We most read when we talk about prediction models, about the overfitting part. But we're going to try to, in some cases, to correct also for the overestimation of the effect sizes. Because all of these things are going on, it's absolutely important that we validate. If you're going to use automatic selection procedures, you absolutely, absolutely must validate, either an internal validation or an external validation. And you should also be aware that there are some modern techniques that are being applied to automatic selection procedures. That are slight modifications that can actually improve all of these things. So, reduce the chances of type 1 errors. Shrink the effect sizes down to more, reasonable effect sizes. And, also correct for over-fitting. So one of those modifications that you can use is something called shrinkage. ", ['model', 'underestimation', 'p-values', 'overestimation', 'effect', 'sizes', 'prediction', 'models', 'overfitting', 'part', 'try', 'cases', 'overestimation', 'effect', 'things', 'selection', 'procedures', 'validation', 'validation', 'techniques', 'selection', 'procedures', 'modifications', 'things', 'reduce', 'chances', 'type', 'errors', 'Shrink', 'effect', 'effect', 'modifications', 'something', 'shrinkage']), 0.21172526114494827, 0.20356190678492567)
((9, '/Users/jag/Downloads/Stanford medstats/Unit 6 Module 1 part 1.srt', "That's the Type I error rate directly related to the significance level. The Type II error rate is the probability of missing an effect to get a false negative. Now, we don't usually talk directly about the Type II error in medical studies. What we do instead is talk about something that's the compliment of the Type II error. We talk about the statistical power. So, statistical power is just the probability of finding an effect if it's there. Is the probability of not making a type II error. That's usually what we talk about in, in medical studies. When we design studies a typical study might be designed to have a power 80%, so that means we're allowing a false negative rate of type II error rate of about 20%. Now that's sort of your typical study. That means that we, when we go out to do studies we're accepting that in about 1 in 5 studies we're going to miss a real effect, when there are actually effects we're going to miss about 1 in 5 of them. Statistical power is such an important concept in medicine/g, ", ['Type', 'I', 'error', 'rate', 'significance', 'level', 'Type', 'II', 'error', 'rate', 'probability', 'effect', 'false', 'Type', 'II', 'error', 'studies', 'talk', 'something', 'compliment', 'Type', 'II', 'error', 'power', 'power', 'probability', 'finding', 'effect', 'probability', 'type', 'II', 'error', 'studies', 'studies', 'study', 'power', '%', 'means', 'false', 'rate', 'type', 'II', 'error', 'rate', '%', 'sort', 'study', 'studies', 'studies', 'miss', 'effect', 'effects', 'miss', 'power', 'concept']), 0.22922278536386947, 0.2022552026889697)
QUERY: May the HIV transmission calculation (Unit 3 - module2) be thought as a permutation with replacement repeated 100 times? In other words, if the transmission occurs in the first act, it does not matter if occurs again in the second, third or 100th act; if it occurs in the second act, it does not matter if occurs again in the third, fourth or 100th act and so on... Or is it a combination?  Thanks!
*************************
((6, '/Users/jag/Downloads/Stanford medstats/RExercise2.srt', "Get right now versus non varsity athletes. To reinforce that conclusion what we're going to do is look at our plot here. It's always a good idea to visualize your data just to make sure. You understand what's going on. It can sort of act as a very good sanity check. On the x-axis here we have exercise. And on the y-axis we have our two groups, varsity and non-varsity athletes. And we can tell that although the averages are different. There's a significant amount of overlap in how the data is distributed. And so it's not surprising that we came to the result that we can not reject the null. Moving on to problem two, now we're interested in comparing whether students love math more, or whether students love writing more. And the way we're going to do that in this data set is to compare the math love variable and the writing love. ", ['Get', 'non', 'varsity', 'conclusion', 'plot', 'idea', 'data', 'act', 'sanity', 'check', 'x-axis', 'y-axis', 'groups', 'varsity', 'non-varsity', 'athletes', 'averages', 'amount', 'overlap', 'data', 'result', 'null', 'problem', 'comparing', 'students', 'math', 'students', 'way', 'data', 'set', 'math', 'love', 'writing', 'love']), 0.08006407690254358, 0.19714616511160418)
((4, '/Users/jag/Downloads/Stanford medstats/Unit 4 1b.srt', "Another question, and we'll get into this much further in terms of the drill down is, why did it actually happen? You know, you've heard about the five why's, we're going to talk a little bit more about that later when we get into more of the details, but you have to ask the question, why did it happen? Another question, what are we going to do to prevent it from happening again? What are the actions or system process changes, and what kind of outcomes can we tolerate in our organization so that we make sure that we don't have to have these kind of events happening again? How will we know that our actions improve patient safety and that's the need, as part of the plan do check act process is like checking to make sure that we're heading in the right direction, that our outcomes, the metrics that we're setting as expectation are being met. And, if they're not being met, we may have to go back and ", ['question', 'further', 'terms', 'drill', 'bit', 'details', 'question', 'question', 'actions', 'system', 'process', 'changes', 'kind', 'outcomes', 'organization', 'kind', 'events', 'actions', 'patient', 'safety', 'need', 'part', 'plan', 'check', 'act', 'process', 'checking', 'right', 'direction', 'outcomes', 'metrics', 'expectation', 'met', 'met']), 0.07844645405527362, 0.1931630037079653)
((4, '/Users/jag/Downloads/Stanford medstats/BWH Module 6   Lines and Monitors   HD 1080p.srt', "The C wave is due to isovolemic right ventricular contraction. When the, tricuspid valve closes and bulges back into the right atrium. This occurs in early systole. You'll notice that the C wave occurs after the QRS oh the EKG and the A wave occurs after the P wave. The V wave is caused by filling of the atrium with blood from the vena cava. This happens while the tricuspid valve is closed during late systole. There are two descents. Firstly the X descent is caused by atrial relaxation, which causes a decline in atrial pressure during ventricular contraction. This occurs in mid systole, so you'll notice it occurs just before the T wave of the EKG. The Y descent is caused by a decrease in atrial pressure as the tricuspid valve opens and blood begins flowing from the right atrium into the right ventricle. ", ['C', 'right', 'ventricular', 'contraction', 'closes', 'bulges', 'right', 'atrium', 'occurs', 'systole', 'C', 'occurs', 'QRS', 'oh', 'EKG', 'wave', 'P', 'V', 'filling', 'atrium', 'blood', 'vena', 'cava', 'happens', 'valve', 'systole', 'descents', 'X', 'descent', 'relaxation', 'causes', 'decline', 'pressure', 'contraction', 'occurs', 'systole', 'T', 'EKG', 'Y', 'descent', 'decrease', 'pressure', 'valve', 'opens', 'blood', 'right', 'atrium', 'right', 'ventricle']), 0.12941939442218642, 0.1765210334817457)
((1, '/Users/jag/Downloads/Stanford medstats/Geriatric Ethics Discussion.srt', "We'll also discuss the terms capacity, competence, autonomy and surrogate decision maker, and discuss how those relate to making decisions for patients who may not be able to make decisions on their own, for their own medical care. The third objective, will be to discuss how medical professionals should act when surrogate decision makers and advanced directives conflict. In those situations, what should the medical, what should the medical practitioner do? Especially in emergency situations, this can be perplexing, and a decision needs to be made quickly. Thank you for joining me today, Dr. Milner.  Very happy to be here.  So in discussing Mr. Williams, and in this particular situation, where there's ambiguous information, how would you suggest the anesthesiologist act? So in this particular situation, it's pretty clear that the anesthesiologist should intubate. And the reason for that, is because although there was a suggestion of documented evidence of wishes, we don't actually have any documented evidence of wishes. ", ['terms', 'capacity', 'competence', 'autonomy', 'decision', 'maker', 'discuss', 'decisions', 'patients', 'decisions', 'care', 'objective', 'professionals', 'decision', 'makers', 'directives', 'situations', 'practitioner', 'emergency', 'situations', 'decision', 'Thank', 'today', 'Dr.', 'Milner', 'Very', 'happy', 'discussing', 'Mr.', 'Williams', 'situation', 'information', 'anesthesiologist', 'act', 'situation', 'anesthesiologist', 'reason', 'suggestion', 'evidence', 'wishes', 'evidence', 'wishes']), 0.0716114874039433, 0.17633289067717772)
((0, '/Users/jag/Downloads/Stanford medstats/Geriatric Ethics Discussion.srt', "[BLANK_AUDIO] [MUSIC] Hi, I'm Dr. Kevin Jenner. I'm a resident in Anesthesiology at the Medical College of Wisconsin. In this module of the Essentials of Geriatric Anesthesia course, we're going to be talking to Dr. Lauren Milner, she's joining me from the Center for Biomedical Ethics, at Stanford University, where she's a fellow. She's also a committee member on the Stanford Hospital Medical Ethics Committee. In this section, we have three objectives. Those are to first discuss what should be done with Mr. Williams in this particular medical situation. Mr. Williams has a do not intubate order that was seen in the computer, but the computer system in unavailable. So, how should a medical practitioner act in the face of ambiguous information regarding the patient's wishes? ", ['[', 'BLANK_AUDIO', ']', '[', 'MUSIC', ']', 'Hi', 'Dr.', 'Kevin', 'Jenner', 'resident', 'Anesthesiology', 'Medical', 'College', 'Wisconsin', 'module', 'Geriatric', 'Anesthesia', 'course', 'Dr.', 'Lauren', 'Milner', 'Center', 'Biomedical', 'Ethics', 'Stanford', 'University', 'committee', 'member', 'Stanford', 'Hospital', 'Medical', 'Ethics', 'Committee', 'section', 'objectives', 'discuss', 'Mr.', 'Williams', 'situation', 'Mr.', 'Williams', 'do', 'order', 'computer', 'computer', 'system', 'practitioner', 'act', 'face', 'information', 'patient', 'wishes']), 0.06583080186766332, 0.1620987918243187)
((8, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 4 Module 2 part 1.srt', "So we are able to now make sure that we're not counting somebody as, as the the effect of winning, we're not having it kind of act retroactively. So this is a great way to analyze these data. It did yield a smaller effect size. It yielded an effect size of a relative reduction of 20%. And they don't tell us in the paper is that adjusted for any covariance or not. I'm kind of guessing that it's probably just a basic analysis. An additional adjustment may attenuate this even a little bit further. Notice it's starting to boarder on being nonsignificant here. So one of the criticisms of that paper, you know, later on, is people said, well, that more proper analysis was buried in the paper and the, the authors chose to highlight kind of the, the less good analysis. And indeed, when a few years later, some other authors did a reanalysis of the original data, this is a kind of a busy table, but I'll just want to highlight one thing, in their new analysis, they didn't do time-dependent covariance, they did something slightly different. ", ['somebody', 'effect', 'act', 'way', 'data', 'yield', 'effect', 'size', 'effect', 'size', 'reduction', '%', 'paper', 'covariance', 'guessing', 'analysis', 'adjustment', 'bit', 'Notice', 'criticisms', 'paper', 'people', 'analysis', 'paper', 'authors', 'highlight', 'kind', 'analysis', 'years', 'authors', 'reanalysis', 'data', 'kind', 'table', 'thing', 'analysis', 'covariance', 'something']), 0.06537204504606135, 0.16096916975663778)
((6, '/Users/jag/Downloads/Stanford medstats/Unit 9 Homework with Answers.srt', "the overweight and the obese, compared with normal weight. However all of the confidence intervals cross one. Well how do we interpret this? Well, what's probably going on here is that, that under weight category is small, there aren't that many people running around who are under weight. So it does look like there's some elevation in the risk of dementia here, but perhaps because of the small sample size we have low statistical power. And that's why the findings don't reach a statistical significance. So the best choice here would be a. Being underweight may increase the risk of dementia, but the findings are not statistically significant, likely due to small sample size. For this question you have to be careful about how you determine interaction. In fact, the correct answer here is c, we don't have enough information in this table alone to tell whether or not we have a significant act. Interaction between gender with the weight category and our outcome here, dementia in midlife. It does look like there's less of a relationship, right? For men, it doesn't look like there's a It's a huge relationship between ", ['overweight', 'obese', 'weight', 'confidence', 'intervals', 'Well', 'Well', 'weight', 'category', 'people', 'weight', 'elevation', 'risk', 'dementia', 'sample', 'size', 'power', 'findings', 'significance', 'choice', 'risk', 'dementia', 'findings', 'sample', 'size', 'question', 'interaction', 'fact', 'correct', 'answer', 'c', 'information', 'table', 'alone', 'act', 'Interaction', 'gender', 'weight', 'category', 'outcome', 'midlife', 'relationship', 'men', 'relationship']), 0.06537204504606135, 0.16096916975663778)
((12, '/Users/jag/Downloads/Stanford medstats/Unit 4 module 4 part 2.srt', "And indeed if we see this as a binomial with a n of 100 and a p of 0.5, if I multiply 100 times 0.5 I get 50. So expected value's easy, what about the variance and standard deviation? So the expected value is n times p equals fif, 50. The variance is n times p times 1 minus p, so that would be 100 times 0.5 times 0.5. That comes out to be 25. Remember I told you, that the cap on variance for a binomial is a quarter of n, and that occurs the the biggest, when the maximum occurs when you p is 0.5, so the variance here turns out to be exactly a quarter of n. If the variance is 25, the standard deviation is the square root of that, or 5. So what does this tell us? How is this helpful or useful. Well this tells you that if you flip a coin a 100 times, your expecting to get about 50 heads. But of course we know that we're not going to get exactly 50 heads every time. We could easily get 49, 48, 51, it's not going to be exactly heads because there's some random chance going on here. ", ['n', 'p', 'times', 'value', 'variance', 'deviation', 'value', 'times', 'equals', 'variance', 'times', 'times', 'minus', 'p', 'times', 'times', 'Remember', 'cap', 'variance', 'quarter', 'n', 'occurs', 'maximum', 'occurs', 'variance', 'quarter', 'n.', 'If', 'variance', 'deviation', 'square', 'root', 'tell', 'Well', 'coin', 'times', 'expecting', 'heads', 'course', 'heads', 'time', 'heads', 'random', 'chance']), 0.16189403145068335, 0.16002248369025415)
QUERY: The sample size of the standard counseling group is smaller than that of the weight-focused counseling group. Is it reasonable to consider the sample size could be influencing the lack of statistical significance in the within-group comparison of bupropion versus placebo in the standard counseling group?
*************************
((7, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit4 Mod5_2.srt', "Now as we talked about in the earlier module on this example, the P-value's highly significant for the weight-focused counseling group, the P-value comparing placebo and drug within that group. It didn't quite make statistical significance. It, within the standard counseling group. But it's very close to that. What we would conclude is there just appears to be a main effect for drug. Overall, drug seems to be doing something. Even though it didn't quite reach statistical significance in the standard counseling group. We talked about not misinterpreting those P-values in an earlier module. If you look at the, what the authors concluded in the media coverage of this study, here's the basically conclusion that was drawn. Among weight-concerned women smokers, drug therapy increases cessation rates when added to a specialized weight concerns intervention, the weight counseling, but not when added to standard counseling. The implication there is that there is an interaction. They're saying that the drug works in one group but not the other. ", ['module', 'example', 'counseling', 'group', 'comparing', 'placebo', 'drug', 'group', 'significance', 'counseling', 'group', 'effect', 'drug', 'drug', 'something', 'significance', 'counseling', 'group', 'P-values', 'module', 'authors', 'media', 'coverage', 'study', 'conclusion', 'women', 'smokers', 'drug', 'therapy', 'cessation', 'rates', 'weight', 'concerns', 'intervention', 'weight', 'counseling', 'counseling', 'implication', 'interaction', 'drug', 'group']), 0.5044197184254879, 0.4196270442767621)
((7, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit4 Mod5.srt', "even within a standard counseling group. Now, as we talked in the earlier module on this example, the P-value is highly significant for the weight-focused counseling group, the P-value comparing placebo and drug within that group. It didn't quite make statistical significance it, within the Standard counseling group, but it's very close to that. What we would conclude is there's just appears to be a main effect for drug. Overall, drug seems to be doing something, even though it didn't quite reach statistical significance in the Standard counseling group. We talked about not misinterpreting those P-values in an earlier module. If you look at the, what the author's concluded in the media coverage of this study, here's the basic conclusion that was drawn. Among weight concerned women smokers, drug therapy increased the cessation rates when added to a specialized weight concerns intervention, the weight counseling, but not when added to standard counseling. The implication there is that there is an interaction. They're saying that the drug works in one group, but not the other. ", ['counseling', 'group', 'module', 'example', 'counseling', 'group', 'comparing', 'placebo', 'drug', 'group', 'significance', 'Standard', 'counseling', 'group', 'effect', 'drug', 'drug', 'something', 'significance', 'Standard', 'counseling', 'group', 'P-values', 'module', 'author', 'media', 'coverage', 'study', 'conclusion', 'women', 'smokers', 'drug', 'therapy', 'cessation', 'rates', 'weight', 'concerns', 'intervention', 'weight', 'counseling', 'counseling', 'implication', 'interaction', 'drug', 'group']), 0.5346982846794047, 0.40010061617458414)
((7, '/Users/jag/Downloads/Stanford medstats/Unit 9 Module 4.srt', "At 6 months, it went from 10% to 21%, so there's a pretty big increase. Basically, a doubling of quitting rates in the drug group, even within the standard counseling group. Now, as we talked about in the earlier module on this example, the P-value is highly significant for the weight focus counseling group, the P value of comparing placebo and drug within that group. It didn't quite make statistical significance within the standard counseling group, but it's very close to that. What we would conclude is there just appears to be a main effect for drug. Overall, drug seems to be doing something. Even though it didn't quite reach statistical significance in the standard counseling group. We talked about not misinterpreting those p values in an earlier module. If you look at the, what the authors' concluded in the media coverage of this study. Here's the basic conclusion that was drawn. Among weight concerned women smokers, drug therapy increased cessation rates when added to a specialized weight concerns intervention, the weight counseling, but not when added to standard counseling. ", ['months', '%', '%', 'increase', 'doubling', 'rates', 'drug', 'group', 'counseling', 'group', 'module', 'example', 'weight', 'focus', 'counseling', 'group', 'P', 'value', 'placebo', 'drug', 'group', 'significance', 'counseling', 'group', 'effect', 'drug', 'drug', 'something', 'significance', 'counseling', 'group', 'p', 'values', 'module', 'authors', 'media', 'coverage', 'study', 'conclusion', 'women', 'smokers', 'drug', 'therapy', 'cessation', 'rates', 'weight', 'concerns', 'intervention', 'weight', 'counseling', 'counseling']), 0.5120950881529179, 0.3831872406937092)
((5, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit4 Mod5.srt', "The outcome here was smoking abstinence. It was biochemically confirmed. It's a binary outcome. You either were abstinent or you weren't. So that means we're going to be using logistic regression to analyze the data. Here's the results of that study. It's a rather busy table so I'm going to zoom in on some things now. First of all, we're looking at actually three different outcomes here. They measured the abstinence rates at three months, six months, and 12 months into the study. So each of those rows represent a different time point in this study. If you just look at the placebo groups within each of those counseling groups. You'll see that the rates of abstinence are pretty similar. So at three months it was 18%, where where in the placebo group in the weight-focused counseling group were abstinent versus 19% in the placebo group in the other counseling group. Those rates look very similar to what we would say here is that there doesn't appear to be a main effect for counseling. ", ['outcome', 'abstinence', 'outcome', 'means', 'regression', 'data', 'results', 'study', 'table', 'things', 'First', 'outcomes', 'abstinence', 'rates', 'months', 'months', 'months', 'study', 'rows', 'time', 'point', 'study', 'placebo', 'groups', 'groups', 'rates', 'abstinence', 'months', '%', 'placebo', 'group', 'counseling', 'group', 'abstinent', 'versus', '%', 'placebo', 'group', 'counseling', 'group', 'rates', 'effect', 'counseling']), 0.38948559553109735, 0.3721416193694153)
((5, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit4 Mod5_2.srt', "The outcome here was smoking abstinence. It was biochemically confirmed. It's a binary outcome. You either were abstinent or you weren't. So that means we're going to be using logistic regression to analyze the data. Here's the results of that study. It's a rather busy table, so I'm going to zoom in on some things now. First of all, we're looking at actually three different outcomes here. They measured the abstinence rates at three months, six months, and 12 months into the study. So each of those rows represent a different time point in the study. If you just look at the placebo groups within each of those counseling groups. You'll see that the rates of abstinence are pretty similar. So at three months it was 18%, were at, were in the placebo group. In the weight-focused counseling group, were abstinent versus 19% in the placebo group in the other counseling group. Those rates look very similar to what we would say here is that there doesn't appear to be a main effect for counseling. ", ['outcome', 'abstinence', 'outcome', 'means', 'regression', 'data', 'results', 'study', 'table', 'things', 'First', 'outcomes', 'abstinence', 'rates', 'months', 'months', 'months', 'study', 'rows', 'time', 'point', 'study', 'placebo', 'groups', 'groups', 'rates', 'abstinence', 'months', '%', 'placebo', 'group', 'counseling', 'group', 'abstinent', 'versus', '%', 'placebo', 'group', 'counseling', 'group', 'rates', 'effect', 'counseling']), 0.38948559553109735, 0.3721416193694153)
((8, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit4 Mod5_2.srt', "They're implying that there is an interaction between drug and counseling type. However, they haven't done any formal analyses to test for an interaction here. So there is, they haven't specifically talked about interaction, but that's really the implication here. And they haven't formally tested that. So I actually took the data in their paper and ran it in a logistic regression to test for interaction, to see whether or not there really is an interaction. And what would it mean if there was an interaction? So just go back to this slide for a minute. An interaction would mean that the drug effect, the difference between, you know, going from here to here, the difference between placebo and drug was much bigger in the weight-focused counseling group than in the standard counseling group. It does appear to be somewhat bigger. That the change from placebo to, to drug, appears to be somewhat bigger in the weight-focused counseling group, but not a lot bigger than in the standard counseling group. We can formally test whether or not that effect size is bigger. We can run a logistic regression with an interaction term. ", ['interaction', 'drug', 'counseling', 'type', 'analyses', 'test', 'interaction', 'interaction', 'implication', 'data', 'paper', 'regression', 'test', 'interaction', 'interaction', 'interaction', 'slide', 'minute', 'interaction', 'drug', 'effect', 'difference', 'difference', 'placebo', 'drug', 'counseling', 'group', 'counseling', 'group', 'change', 'placebo', 'drug', 'counseling', 'group', 'lot', 'bigger', 'counseling', 'group', 'test', 'effect', 'size', 'regression', 'interaction', 'term']), 0.38994481920563634, 0.36349051381230374)
((8, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit4 Mod5.srt', "They're implying that there is an interaction between drug and counseling type. However, they haven't done any formal analyses, to test for an interaction here. So there is, they haven't specifically talked about interaction, but that's really the implication here. And they haven't formally tested that. So I actually took the data in their paper and ran it in a logistic regression to test for interaction to see whether or not there really is an interaction. And what would it mean if there was an interaction. So just go back to this slide for a minute. An interaction would mean that the drug effect, the difference between, you know, going from here to here, the difference between placebo and drug was much bigger in the weight focus counseling group then in the standard counseling group. It does appear to be somewhat bigger. That the, change from placebo to drug appears to be somewhat bigger in the weight focus counseling group, but not a lot bigger than in the standard counseling group. We can formally test whether or not that effect size is bigger. We can run a logistic regression with an interaction term. ", ['interaction', 'drug', 'counseling', 'type', 'analyses', 'test', 'interaction', 'interaction', 'implication', 'data', 'paper', 'regression', 'test', 'interaction', 'interaction', 'interaction', 'slide', 'minute', 'interaction', 'drug', 'effect', 'difference', 'difference', 'placebo', 'drug', 'weight', 'focus', 'counseling', 'group', 'counseling', 'group', 'change', 'placebo', 'drug', 'weight', 'focus', 'counseling', 'group', 'lot', 'bigger', 'counseling', 'group', 'test', 'effect', 'size', 'regression', 'interaction', 'term']), 0.38043253759720735, 0.3546235563376058)
((8, '/Users/jag/Downloads/Stanford medstats/Unit 9 Module 4.srt', "The implication there is that there is an interaction, they're saying that the drugs works in one group, but not the other. They're implying that there is an interaction between drug and counseling type. However, they haven't done any formal analyses to test for an interaction here. So, there is, they haven't specifically talked about interaction but that's really the implication here, and they haven't formally tested that. So, I actually took the data in their paper and re-entered in the logistic regression to test for interaction to see whether or not there really is an interaction. And what would it mean if there was an interaction? So, just go back to this slide for a minute. An interaction would mean that the drug affect, the difference between, you know, going from here to here. The difference between placebo and drug was much bigger in the weight-focused counseling group than in the standard counseling group. It does appear to be somewhat bigger. The, the, change from placebo to drug appears to be somewhat bigger in the weight-focused counseling group, but not a lot bigger than in the standard counseling group. ", ['implication', 'interaction', 'drugs', 'group', 'interaction', 'drug', 'counseling', 'type', 'analyses', 'test', 'interaction', 'interaction', 'implication', 'data', 'paper', 'regression', 'test', 'interaction', 'interaction', 'interaction', 'slide', 'minute', 'interaction', 'drug', 'affect', 'difference', 'difference', 'placebo', 'drug', 'counseling', 'group', 'counseling', 'group', 'change', 'placebo', 'drug', 'counseling', 'group', 'lot', 'bigger', 'counseling', 'group']), 0.40252368434130203, 0.33274517662163366)
QUERY: In Unit 2 Module 2 : the incidence rate for the example of vioxx and naproxen are correct??  For Vioxx I have 2.4 for 100 persons x year and for naproxen 5.2 and in the course the results are 2.1 and 4.5 respectively..I expect yours comments.. Thanks in advance
*************************
((9, '/Users/jag/Downloads/Stanford medstats/Unit 2 Module 4 part 1.srt', "Not one person wrote a letter to the editor of the New England Journal, after this article was published, questioning the author's logic here. People bought into the story that the authors were trying to sell, that Naproxen is this miracle heart protective drug. Instead of that Vioxx is a dangerous drug. Here's the, what they wrote in the paper's conclusion. I showed you this in the teaser earlier. Thus, our results are consistent with the theory that Naproxen has a coronary protective effect. And highlight the fact that rofecoxib Vioxx does not provide this type of protection. The finding that naproxen therapy was associated with a lower rate of myocardial infarction needs further confirmation in larger studies. Of course, naproxen is the reference group, the control group in this study. So every comparison really outta be Vioxx versus naproxen, not naproxen versus Vioxx. But they totally put the focus on naproxen to take the focus away from Vioxx and it worked. In fact it turns out that naproxen has a very small, if any cardio protective affect. ", ['person', 'letter', 'editor', 'New', 'England', 'Journal', 'article', 'questioning', 'author', 'logic', 'People', 'story', 'authors', 'Naproxen', 'miracle', 'heart', 'drug', 'Vioxx', 'drug', 'paper', 'conclusion', 'teaser', 'results', 'theory', 'Naproxen', 'effect', 'fact', 'rofecoxib', 'Vioxx', 'type', 'protection', 'finding', 'therapy', 'rate', 'infarction', 'confirmation', 'studies', 'course', 'reference', 'group', 'control', 'group', 'study', 'comparison', 'outta', 'Vioxx', 'Vioxx', 'focus', 'naproxen', 'focus', 'Vioxx', 'fact', 'naproxen', 'cardio', 'affect']), 0.25730700836107073, 0.32181049467883116)
((7, '/Users/jag/Downloads/Stanford medstats/Unit 2 Module 4 part 1.srt', "We would interpret this that Vioxx increases the rate or risk of heart attacks by fourfold, or you would say by 320%. But let's see what the authors actually put in the abstract here. So, here's the abstract from the paper. The incidence of myocardial infarction, heart attack, was lower among patients in the naproxen group than among those in the Vioxx group. 0.1% versus 0.4%. We already saw that number in the last module. Relative risk 0.2, 95% confidence interval, 0.1 to 0.7. So notice that when the authors reported the data on GI events, they put the naproxen in the denominator. They put the naproxen as the reference group, but when they report the data on heart attacks, they're flipping things around. They're making Vioxx the reference group. They're putting Vioxx in the denominator. And you can probably think of the reason that they might have wanted to do that. So there are actually two funny things going on here, ", ['Vioxx', 'rate', 'heart', 'attacks', 'fourfold', '%', 'authors', 'abstract', 'abstract', 'paper', 'incidence', 'infarction', 'heart', 'attack', 'patients', 'naproxen', 'group', 'Vioxx', 'group', '%', 'versus', '%', 'number', 'module', 'risk', '%', 'confidence', 'interval', 'notice', 'authors', 'data', 'GI', 'events', 'naproxen', 'denominator', 'naproxen', 'reference', 'group', 'data', 'heart', 'attacks', 'things', 'Vioxx', 'reference', 'group', 'Vioxx', 'denominator', 'reason', 'things']), 0.22779791898059978, 0.3044114881038004)
((22, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Optional.srt', "very generalizable. That means the that the results might apply to a very specific subset of patients, but not to all the patients that you would care about. An example of a randomized trial, is this example that I mentioned in the teaser and it's an example that I'm going to keep using to illustrate the concepts this week. So this was a double-blind, randomized, controlled trial. It was published in the New England Journal of Medicine in 2000. And the researchers were comparing what's called a COX-2 inhibitor, a COX-2 inhibitor named Rofecoxib. I'm now going to tell you what the actual brand name of that drug is. The drug is actually Vioxx. And if I say that. The drug is Vioxx. Some of you will remember, that will trigger a memory, that that was in the media, Vioxx was in the media very heavily in the mid-2000s. But they were studying Vioxx as a new, pain treatment for rheumatoid arthritis, and they were comparing it with an older pain treatment, a non-steroidal anti-inflammatory called naproxen. And what they were interested in is that naproxen and ", ['results', 'subset', 'patients', 'patients', 'example', 'trial', 'example', 'teaser', 'example', 'concepts', 'week', 'trial', 'New', 'England', 'Journal', 'Medicine', 'researchers', 'inhibitor', 'inhibitor', 'Rofecoxib', 'brand', 'name', 'drug', 'drug', 'Vioxx', 'drug', 'Vioxx', 'memory', 'media', 'Vioxx', 'media', 'mid-2000s', 'Vioxx', 'pain', 'treatment', 'arthritis', 'pain', 'treatment', 'naproxen']), 0.23094010767585027, 0.2548653168986608)
((8, '/Users/jag/Downloads/Stanford medstats/Unit 2 Module 4 part 1.srt', "as I alluded to in the last module. So firstly, they reported risks rather than rates. For the primary outcome GI events, remember they actually reported the incidence rates, they reported the risks here, likely because the risk difference here looks smaller than the rate difference. Secondly, they flipped the relative risk. They divided the risk in the Naproxen group by the risk in the Vioxx group rather than the other way around. So remember the risk in the Vioxx group was 0.42. The risk in the Naproxen group is 0.1. If you divide 0.1 by 0.42 you get a relative risk of 0.24 and they rounded down in the abstract to 0.2. They rounded to one decimal place. So they twisted the message here, right. They twisted the message to make it look like naproxen is protective rather than that Vioxx is harmful. And technically, they haven't done anything wrong mathematically. The numbers are correct. But flipping the relative risk of course completely shifts the take home message, and people missed it. ", ['module', 'risks', 'rates', 'outcome', 'GI', 'events', 'remember', 'incidence', 'rates', 'risks', 'risk', 'difference', 'rate', 'difference', 'risk', 'risk', 'Naproxen', 'group', 'risk', 'Vioxx', 'group', 'way', 'remember', 'risk', 'Vioxx', 'group', 'risk', 'Naproxen', 'group', 'risk', 'abstract', 'place', 'message', 'message', 'naproxen', 'Vioxx', 'anything', 'numbers', 'risk', 'course', 'take', 'home', 'message', 'people']), 0.1382189480930176, 0.1839987497613015)
((8, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit 8 mod4.srt', "normally distributed, but often with counts, they're not normally distributed. So with counts, think of negative binomial or plasan. And then finally, I just want to mention that as with the plasan regression. You can take the data co-efficients from a negative binomial regression and they have a direct interpretation as relative risks. So, if you just exponentiate the data co-efficients from negative binomial regression. That's going to give you the incidence rate ratio, same as with Poisson. So, everything is identical. I want to give you an example here or a model is a little complicated because I have got all Interaction in a higher order terms, but let me walk you through one calculation. So, for example, our beta for the number of new technologies, that was negative .27. Our beta for the interaction between technologies is And year turned out to be positive 0.015, and remember that I've modeled years here as the number of years since 1969. So how would I get the incidence rate ratio for new technology Well, of course, ", ['counts', 'counts', 'mention', 'plasan', 'regression', 'data', 'co-efficients', 'regression', 'interpretation', 'risks', 'data', 'co-efficients', 'regression', 'incidence', 'rate', 'ratio', 'Poisson', 'everything', 'example', 'model', 'Interaction', 'order', 'terms', 'calculation', 'example', 'beta', 'number', 'technologies', 'beta', 'interaction', 'technologies', 'year', 'remember', 'years', 'number', 'years', 'incidence', 'rate', 'ratio', 'technology', 'Well', 'course']), 0.19123657749350298, 0.1692310213745225)
((6, '/Users/jag/Downloads/Stanford medstats/Unit 2 Module 4 part 1.srt', "The real effect of Vioxx on on GI events we believe is somewhere between 0.3 and 0.6. Another way to interpret it is we say we can be 95% confident that the true effect of Vioxx verus naproxen is between a 40% and 70% reduction in the rate of GI events. Where did that 40% and 70% come from? The upper limit of the confidence interval is a relative risk of 0.6. That's a 40% reduction in risk. The lower limit in the confidence interval is a relative risk of 0.3. That's a 70% reduction in risk. Now let's move to the heart attack data from the study. What's the rate ratio going to be here. The rate ratio will be the rate in the Vioxx group divided by the rate in the Naproxen group. When you do that division it comes out to be 4.2, if the risk ratio is actually identical, as I said before, because the groups should follow for equal amounts of time. The risk ration and the rate ratio are the same. How do we interpret these results? ", ['effect', 'Vioxx', 'GI', 'events', 'way', '%', 'confident', 'effect', 'Vioxx', 'verus', 'naproxen', '%', '%', 'reduction', 'rate', 'GI', 'events', '%', '%', 'come', 'limit', 'confidence', 'interval', 'risk', '%', 'reduction', 'risk', 'limit', 'confidence', 'interval', 'risk', '%', 'reduction', 'risk', 'move', 'heart', 'attack', 'data', 'study', 'rate', 'ratio', 'rate', 'ratio', 'rate', 'Vioxx', 'group', 'rate', 'Naproxen', 'group', 'division', 'risk', 'ratio', 'groups', 'amounts', 'time', 'risk', 'ration', 'rate', 'ratio', 'results']), 0.1697056274847714, 0.16180663743316986)
((7, '/Users/jag/Downloads/Stanford medstats/HRP262 Unit 1 Module 2.srt', "So I did a rough estimation from the curve kind of taking the, the mid point, of the ranges that are shown here. I got that there was about maybe a total of 1781, Person years represented here you know everybody died eventually so there's a 100 events I started with a 100 people. There's 100 deaths and they were alive in cumulative total for about 70 net 181 person years. So we can calculate that the rate of death was .05 Six per year so about your chances of dying in any given year were about 5 to 6% which is really, really high. The last thing I want to point out is this module has talked about incidence rates which I'm assuming that many of you are already actually familiar with. Again, the incidence rate just gives the average rate of the events over a given time period. What we're going to be focusing on more in this class, is something called the hazard rate. So I want to just put in your mind right now what the concept of the hazard rate is. It's very close to the incidence rate, But it's an instantaneous incidence rate. ", ['estimation', 'curve', 'kind', 'mid', 'point', 'ranges', 'total', 'Person', 'years', 'everybody', 'events', 'people', 'deaths', 'total', 'person', 'years', 'rate', 'death', 'year', 'chances', 'year', '%', 'thing', 'point', 'module', 'incidence', 'rates', 'incidence', 'rate', 'rate', 'events', 'time', 'period', 'class', 'something', 'hazard', 'rate', 'mind', 'concept', 'hazard', 'rate', 'incidence', 'rate', 'incidence', 'rate']), 0.24678107889564976, 0.1333633746046921)
((2, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 3 Module 1 Part 2.srt', "was chosen as the reference groups so it's set to 1.00. The non-significant trend group was had a decreased rate of publication so the hez, the hazard ratio's below 1. It does cross one here, it's not a statistically significant difference from the null group but you can see there's some suggestion of that group having a worse publication rate. And finally, the significant group, the ones who actually found significant results, did have a statistically significant increase in the publication rate compared with the null group. And we can interpret this hazard ratio as basically having about a twofold higher incidence of publication compared to those will null results. So it gives us an effect size we can measure. Second example a, of a use of Cox regression in a literature has to do with a series of studies that were published that were looking at Academy Award winners versus nominees, and trying to get at this question, people always say, well, it was just an honor to be nominated. Is it really as good to just be nominated as it is to actually win, ", ['reference', 'groups', 'trend', 'group', 'rate', 'publication', 'hez', 'hazard', 'ratio', 'difference', 'null', 'group', 'suggestion', 'group', 'worse', 'publication', 'rate', 'group', 'ones', 'results', 'increase', 'publication', 'rate', 'group', 'hazard', 'ratio', 'twofold', 'incidence', 'publication', 'results', 'effect', 'size', 'example', 'use', 'Cox', 'regression', 'literature', 'series', 'studies', 'Academy', 'Award', 'winners', 'nominees', 'question', 'people', 'honor']), 0.14757295747452437, 0.12210101055072366)
QUERY:  But suppose in a hypothetical situation in the placebo group, had only 100 people responded to the hypertension question and if all of them were hypertensives, the answers would have been 100% hypertensives in the placebo group? This would be quite misleading as would (100/3281*100=) 3.05% hypertensives in the placebo group. Wouldn't it be better if the question was set in a more accurate language? Or is this the standard phrase in statistics which although refers to the group as a whole but implies only the subgroup which responded?
*************************
((11, '/Users/jag/Downloads/Stanford medstats/Unit 2 Homework with questions.srt', "So let me write than in, 31.5, 31.5, okay? And then we have this probability in the reference group, variable. And that's actually just going to be 29%. Okay, so it's the same thing. It's, it's we're still looking at this category of data. The reference group is the placebo group. And the probability it represented by this percentage. So we're going to write that in. So this is going to be 29%, 29% or 0.29 right and then, when we do add this calculation we see that the adjusted, risk ratio is, is 3.2 and of course this is the adjusted risk ratio because we're using the adjusted RR. To calculate this and to make this conversion. The last question in this homework set is asking us what the best way would be to represent this very large odds ratio here, 57.7. ", ['let', 'okay', 'probability', 'reference', 'group', '%', 'Okay', 'thing', 'category', 'data', 'reference', 'group', 'placebo', 'group', 'probability', 'percentage', '%', '%', 'right', 'calculation', 'ratio', 'course', 'risk', 'ratio', 'RR', 'conversion', 'question', 'homework', 'way', 'odds']), 0.4476368928928758, 0.404465031149535)
((17, '/Users/jag/Downloads/Stanford medstats/Unit 1 Homework Solutions with questions.srt', "So, the first one is asking us what the median weight of the placebo group is. So, we're immediately our attention is drawn to this column, okay. And then we want to find the weight variable. We notice that it's given, the median is reported, okay. So, the median in this case is going to be 81, specifically, 81 kilograms. What is the percent of participants in the ranolazine, I'm not sure if I'm pronouncing that right, in the, in the drug group basically, okay. So, you notice it says female sex here, that's the percentage of the participants who are female, which means that about 66% of those participants are male. The next question asks us about the number the percentage of participants rather in the placebo group who have hypertension. ", ['weight', 'placebo', 'group', 'attention', 'column', 'okay', 'weight', 'okay', 'case', 'kilograms', 'percent', 'participants', 'ranolazine', 'right', 'drug', 'group', 'okay', 'sex', 'percentage', 'participants', '%', 'participants', 'question', 'number', 'percentage', 'participants', 'placebo', 'group', 'hypertension']), 0.40375739103654934, 0.4006239309899091)
((10, '/Users/jag/Downloads/Stanford medstats/HRP 262 Unit 5 Module 2 Part 1.srt', "chance of dying in the treated group, and about 50 deaths in the placebo group. And, of course, you know, give or take a few due to random choice. We're not going to get exactly 25, we may get 26, or 24, something around that. And so the observed relative risk that you're going to see it's going to be somewhere around 0.5, maybe not exactly 0.5 but somewhere around 0.5. Now imagine that things happen in the trial, as in most trials. Start people do in fact switch groups. So if, fairly early in the study about 25 treated subjects switch to the placebo stop taking their treatment group, their treatment drug, and 25 placebo subjects stop taking the placebo and actually go out and get the treatment, which actually did happen. In the women's health initiative, for example. Then, what effect is that going to have on our relative risk at the end of the day? Well, now because some of the placebo participants are actually taking the drug and getting the benefit of it, we're actually going to see fewer deaths in the placebo group, maybe 43 or 44 rather than about 50. And since some of the people in ", ['chance', 'group', 'deaths', 'placebo', 'group', 'course', 'choice', 'something', 'risk', 'things', 'trial', 'trials', 'Start', 'people', 'fact', 'switch', 'groups', 'study', 'subjects', 'placebo', 'stop', 'treatment', 'group', 'treatment', 'drug', 'placebo', 'subjects', 'placebo', 'treatment', 'women', 'health', 'initiative', 'example', 'effect', 'risk', 'end', 'day', 'Well', 'placebo', 'participants', 'drug', 'benefit', 'deaths', 'placebo', 'group', 'people']), 0.47599729681315056, 0.32974600292821765)
((5, '/Users/jag/Downloads/Stanford medstats/Unit 9 Module 4.srt', "And they were randomly assigned to get a drug, an anti-smoking drug, bupropion, or to get a placebo. So that makes up four total groups. The outcome here was smoking abstinence. It was biochemically confirmed. It's a binary outcome: you either were abstinent or you weren't. So that means we're going to be using logistic regression to analyze the the data. Here's the result to that study. It's a rather busy table, so I'm going to zoom in on some things now. First of all, we're looking at actually three different outcomes here. They measured the abstinence rates at 3 months, 6 months and 12 months into the study. So, each of those rows represent a different time point in this study. If you just look at the placebo groups within each of those counseling groups, you'll see that the rates of abstinence are pretty similar. So, at 3 months, it was 18%, where where in the placebo group. In the weight focused counseling group, we're abstinent versus 19%, in the placebo group in the other counseling group. ", ['drug', 'drug', 'bupropion', 'placebo', 'makes', 'groups', 'outcome', 'abstinence', 'outcome', 'means', 'regression', 'data', 'result', 'study', 'table', 'things', 'First', 'outcomes', 'abstinence', 'rates', 'months', 'months', 'months', 'study', 'rows', 'time', 'point', 'study', 'placebo', 'groups', 'groups', 'rates', 'abstinence', 'months', '%', 'placebo', 'group', 'weight', 'group', 'versus', '%', 'placebo', 'group', 'counseling', 'group']), 0.41713510908978174, 0.3276454548601241)
((19, '/Users/jag/Downloads/Stanford medstats/Unit 1 Homework Solutions with questions.srt', "hypertension, hyperlipidemia, whether you're a current smoker or not. History of prior MI history of prior coronary revascularization and prior heart failure. The last part of this first homework question asks you to identify the box plot that represents the distribution of each in the placebo group. So, we look at the placebo group. Whoops, I just covered what I needed to show you. So, we look at the placebo group, and we see that the median is 64 with an interquartile range of 56 to 72. Okay. So, for a box plot, the, the interquartile range, this is really going to be really informative, so, you're going to want to pick the box plot that looks like a boxy part. So, this, this represents the interquartile range here, I just overshot that. So, the box here represents the interquartile range. ", ['hypertension', 'hyperlipidemia', 'smoker', 'History', 'MI', 'history', 'revascularization', 'heart', 'failure', 'part', 'homework', 'question', 'box', 'plot', 'distribution', 'placebo', 'group', 'placebo', 'group', 'Whoops', 'placebo', 'group', 'range', 'Okay', 'box', 'plot', 'interquartile', 'range', 'box', 'plot', 'boxy', 'part', 'interquartile', 'range', 'box', 'interquartile', 'range']), 0.34695269929572436, 0.32642706571488467)
((5, '/Users/jag/Downloads/Stanford medstats/HRP262 Unit 1 Module 1.srt', "you can see 1 or 2, 3%, a relatively rare outcome it might be better to describe it, or to draw it as a cumulative hazard because if you start at 100%, you're not going to go down very much. But you'll see the curves drawn in both ways. I'll just note here, that we're going to be talking about something called the hazard ratio that's also printed on this little graphic in this particular study. The hazard ratio, which is very similar to a relative risk, for coronary heart disease if you compared women on, in the hormone group to women in the placebo group, the hazard ratio was 1.29. The way that you would interpret that is that there was a 29% higher rate of the, of coronary heart disease in the hormone group versus the placebo group. So what is survival analysis? It's a set of statistical tools that are used for analyzing time-to-event data. And I'll talk about exactly what time-to-event data is in just a second ", ['%', 'outcome', 'hazard', '%', 'curves', 'ways', 'something', 'hazard', 'ratio', 'graphic', 'study', 'hazard', 'ratio', 'risk', 'heart', 'disease', 'women', 'hormone', 'group', 'women', 'placebo', 'group', 'hazard', 'ratio', 'way', '%', 'rate', 'heart', 'disease', 'hormone', 'group', 'placebo', 'group', 'analysis', 'set', 'tools', 'analyzing', 'data', 'data']), 0.4047781491783451, 0.32450762461416627)
((8, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod1.srt', "Times the standard error. The standard error here. Again, I'm going to plug in just the observed proportions in each group. So, 1 minus 0.364 and the treatment group divided by 33. Plus 0.629 times 1 minus 0.629, divided by 35 for the placebo group. If you crank out the math on that, this. Standard error comes out to be about 0.117. And so you add and subtract that from the 26.5%. You end up with a confidence interval of 3.6% to 49.4%. So indeed that confidence interval does not cross zero. So we're 95% sure that the true value is not zero. So in other words, we can clearly reject the null hypothesis that the two groups are the same. ", ['Times', 'error', 'error', 'proportions', 'group', 'minus', 'treatment', 'group', 'Plus', 'times', 'minus', 'placebo', 'group', 'math', 'Standard', 'error', 'subtract', '%', 'confidence', 'interval', '%', '%', 'confidence', 'interval', '%', 'sure', 'value', 'words', 'null', 'hypothesis', 'groups']), 0.3970832554685131, 0.31853725575813335)
((5, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit4 Mod5.srt', "The outcome here was smoking abstinence. It was biochemically confirmed. It's a binary outcome. You either were abstinent or you weren't. So that means we're going to be using logistic regression to analyze the data. Here's the results of that study. It's a rather busy table so I'm going to zoom in on some things now. First of all, we're looking at actually three different outcomes here. They measured the abstinence rates at three months, six months, and 12 months into the study. So each of those rows represent a different time point in this study. If you just look at the placebo groups within each of those counseling groups. You'll see that the rates of abstinence are pretty similar. So at three months it was 18%, where where in the placebo group in the weight-focused counseling group were abstinent versus 19% in the placebo group in the other counseling group. Those rates look very similar to what we would say here is that there doesn't appear to be a main effect for counseling. ", ['outcome', 'abstinence', 'outcome', 'means', 'regression', 'data', 'results', 'study', 'table', 'things', 'First', 'outcomes', 'abstinence', 'rates', 'months', 'months', 'months', 'study', 'rows', 'time', 'point', 'study', 'placebo', 'groups', 'groups', 'rates', 'abstinence', 'months', '%', 'placebo', 'group', 'counseling', 'group', 'abstinent', 'versus', '%', 'placebo', 'group', 'counseling', 'group', 'rates', 'effect', 'counseling']), 0.38529914101523577, 0.3138119789799388)
QUERY:  I understand the derivation that a binomial with probability p and sample size n has mean n*p and standard deviation sqrt(n*p*(1-p)). I'm trying to keep distinct the concepts of standard deviation and standard error - the former describes variation within a sample, whereas the latter describes variation of a statistic coming from multiple samples. My trouble comes with the formula: std err = std dev/sqrt(n), which I thought was true for a mean of any distribution. If that were the case, a binomial would have standard error = sqrt(n*p*(1-p))/sqrt(n) = sqrt(p*(1-p)). But the video around 3:15 has std err = std dev = sqrt(n*p*(1-p)), which seems to mush together these concepts that I'm trying to keep separate. Can you clarify?  Thanks! 
*************************
((1, '/Users/jag/Downloads/Stanford medstats/Unit 5 Homework with answers.srt', 'the standard error is given by standard deviation divided by the square root. Of the number of observations in the sample. And if we substitute the values that we have, this comes out to 0.23. This tells us that a is the right answer. For this question, we know that the mean is .66. How do we notice this, is because we have 20 patients out of 30 who improve based on the exercise test, test. We also know that a standard error is given by the formula P times 1 minus p by a n in this case P is 0.66. 1 minus p is 0.33 and n is 30. So we have a standard error of 0.086. ', ['error', 'deviation', 'root', 'number', 'observations', 'sample', 'values', 'right', 'answer', 'question', 'patients', 'exercise', 'test', 'test', 'error', 'formula', 'P', 'minus', 'p', 'n', 'case', 'P', 'minus', 'p', 'n', 'error']), 0.2969569354582493, 0.38184734266704917)
((7, '/Users/jag/Downloads/Stanford medstats/Unit 5 Module 4 part 1.srt', "The mean of the means is the true mean in the population. Well that makes sense because if that's the true mean, then most of our samples should hover around 62, the true mean. The standard deviation of the means turns out to be 3.3 nanomoles per liter. Now that's interesting, you remember that the, the standard deviation of the trait was 33 nanomoles per liter [SOUND] but the standard deviation of the means which we also called the standard error of the mean turns out to be 3.3. It's exactly 33 divided by 10. Well, the sample size here was 100, so what we're actually doing is, we're dividing the standard deviation by the square root of n, and that's what gives us the 3.3. This 3.3, I didn't get by a mathematical theory though. I just had the computer calculate the standard deviation from these 10,000 observations. And it turns out to be 3.3. So, the distribution of mean vitamin D that I got from this computer simulation. ", ['means', 'mean', 'population', 'Well', 'sense', 'mean', 'samples', 'mean', 'deviation', 'means', 'nanomoles', 'liter', 'deviation', 'trait', 'nanomoles', 'liter', 'SOUND', 'deviation', 'means', 'error', 'turns', 'Well', 'sample', 'size', 'deviation', 'root', 'n', 'theory', 'computer', 'calculate', 'deviation', 'observations', 'distribution', 'vitamin', 'D', 'computer', 'simulation']), 0.2851124011492332, 0.3394508709755408)
((10, '/Users/jag/Downloads/Stanford medstats/Unit 5 Module 4 part 1.srt', "looking at the formula. The formula gives us some insight here. I don't, we're not worry about memorizing the formula but look at the formula in terms of what is it telling us. So standard error decreases as sample size goes up because sample sizes in the denominator. So bigger sample size means less standard error. Well that makes sense. As you sample more people you would expect the uncertainty to go down, the imprecision to go down. The variability of the static from sample to sample will go down if you're sampling more people. going to get closer to the true mean. The standard error increases with greater trait variability, that, that s, that standard deviation of the sample is in the numerator. So if vitamin D were more variable, that's going to cause more variation from in the statistic. Right? It's going to cause more uncertainly. So those are the two elements of standard error. And just to illustrate those, I did another simulation where I increased the sample sizes to 400. So I did many, many, many samples of 400. Notice to what happened to my normal curve, it shrunk. ", ['formula', 'formula', 'insight', 'formula', 'look', 'formula', 'terms', 'error', 'decreases', 'size', 'sample', 'sizes', 'denominator', 'sample', 'size', 'error', 'Well', 'sense', 'people', 'uncertainty', 'imprecision', 'variability', 'people', 'mean', 'error', 'increases', 'trait', 'variability', 's', 'deviation', 'sample', 'numerator', 'vitamin', 'D', 'variation', 'elements', 'error', 'simulation', 'sample', 'samples', 'Notice', 'curve']), 0.3044713810352108, 0.3190684635887383)
((8, '/Users/jag/Downloads/Stanford medstats/Unit 5 Module 4 part 1.srt', "I see that it's normally distributed. The mean is equal to the true mean. And the standard deviation, or what we also call the standard error, is 3.3 nanomoles per liter. I got this all out of the computer simulation. But the distribution of a sample mean, in general, people have long ago worked out with mathematical theory. And it turns out that if you're talking about sample means, means that you calculate from data, they follow, actually they don't exactly follow a normal distribution. If you want to know more about why that watch the optional module on the Central Limit Theorem. It turns out they follow a T-distribution, remember that a T-distribution is essentially the same as a normal distribution as long as you're talking about n's sample sizes at least of 100. So since we have a sample size of over 100, essentially it's, it's on a normal distribution. This means that if you're talking about sample means and you're talking about small samples, actually small samples you have to worry about it being on a T-distribution. You'll have slightly fatter tails than the normal. So it's not exactly, means don't exactly follow a normal distribution for ", ['mean', 'deviation', 'error', 'nanomoles', 'liter', 'all', 'computer', 'simulation', 'distribution', 'sample', 'mean', 'people', 'theory', 'sample', 'means', 'data', 'distribution', 'watch', 'module', 'Central', 'Limit', 'Theorem', 'T-distribution', 'remember', 'T-distribution', 'distribution', 'n', 'sample', 'sample', 'size', 'distribution', 'sample', 'means', 'samples', 'samples', 'T-distribution', 'tails', 'distribution']), 0.29414729940092255, 0.3165531958101896)
((9, '/Users/jag/Downloads/Stanford medstats/Unit 5 Module 4 part 1.srt', "real data. They usually follow a T but by the time you're talking about a sample size of greater than 100 where, where it's interchangeable with the normal. So I'm just going to call it a normal distribution. In general, sample means the mean of the means is the true mean in the population. Well, that make sense. The standard error, somebody has worked out the formula for standard error, the standard error of a mean is the standard deviation of the trait, the variability of the trait like vitamin divided by the square root of n, the number of people you're averaging over. And let's remember, we saw that that in fact works out with exactly what we saw in the simulation because the standard deviation here was 33 if I divide by the square root of 100. I get exactly 3.3, exactly what came out in the simulation as well. So both the simulation and mathematical theory are matching up. So the standard error of a mean, again, is the standard deviation of the trait divided by the square root of n. So you can see some things about how standard error works just by ", ['data', 'T', 'time', 'sample', 'size', 'distribution', 'means', 'means', 'mean', 'population', 'Well', 'make', 'sense', 'error', 'somebody', 'formula', 'error', 'error', 'deviation', 'trait', 'variability', 'trait', 'vitamin', 'root', 'n', 'number', 'people', 'remember', 'fact', 'works', 'simulation', 'deviation', 'root', 'simulation', 'simulation', 'theory', 'error', 'deviation', 'trait', 'root', 'n.', 'So', 'things', 'error', 'works']), 0.2594298807053415, 0.30050892199746293)
((12, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod3.srt', "0.5 raised to 39, 0.5 raised to 14 and you can see that we're going to have to keep going for a long time here. That's going to take us a while, because we have to get all the way down to 0, and then that will give us the one tail, we would have to multiply that by 2 to get the two tail. It's rather tedious to calculate out the exact binomial probability. Your, of course the computer can do that for you easily. But, what we can do instead is that we know that when we have a large enough n, when n times p is greater than 5, we can actually just do a normal approximation to the binomial. So, just to remind you how that works. What we say is, okay, think about what a binomial looks like. Our expected value here is 26.5, and expect to be around here, and then my binomial is going to look something like this. It's very likely that I'm going to get things in the range of you know, 26, 25, 24, and then it's going to kind of drop off from there. So if did out all of the binomial probabilities, this would be all the way from zero to 53, so you know zero here 53, I could get anywhere from zero to 53 pairs having diabetic cases. ", ['time', 'while', 'way', 'tail', 'tail', 'probability', 'course', 'computer', 'n', 'p', 'approximation', 'okay', 'looks', 'value', 'binomial', 'something', 'things', 'range', 'drop', 'probabilities', 'way', 'pairs', 'cases']), 0.12962962962962962, 0.2627269975310945)
((0, '/Users/jag/Downloads/Stanford medstats/Unit 4 Module 6 part 2.srt', "When we do statistics on binomials, what we actually usually do are statistics on proportions. So I just want to point that out now. We don't usually talk about, oh, I got 60 I found 60 smokers in my 200 cases. We don't usually talk about the number 60, we talk that about the proportion that that's 30% of my sample. So our statistics are all going to be on the proportions rather than the counts. So you'll just notice when we do statistics on proportions, it'll look a lot like the binomial. But everything will differ by a factor of So, a statistic on a proportion, when I, I represent that as a p-hat, that means a sample proportion. If I want to know, what's the mean and standard deviation of a sample proportion, those fall right out of the binomial. The mean and variance for a binomial. So the mean is just p. That's, np for binomial, divide by n. Because a proportion divides by n, that's the mean for a, proportion is p. And similarly, the standard deviation for the binomial is the square root of n times p times 1 minus p. If you divide that quantity by n, it turns out to be this lower quantity. ", ['statistics', 'binomials', 'statistics', 'proportions', 'point', 'oh', 'smokers', 'cases', 'number', 'proportion', '%', 'sample', 'statistics', 'proportions', 'counts', 'statistics', 'proportions', 'lot', 'everything', 'factor', 'So', 'proportion', 'means', 'sample', 'proportion', 'mean', 'deviation', 'sample', 'proportion', 'fall', 'variance', 'p.', 'That', 'np', 'n.', 'Because', 'proportion', 'n', 'mean', 'proportion', 'p.', 'deviation', 'square', 'root', 'n', 'times', 'times', 'minus', 'p.', 'If', 'quantity', 'n', 'quantity']), 0.19638761583844802, 0.2532247015739791)
((5, '/Users/jag/Downloads/Stanford medstats/Unit 4 module 2 part 1.srt', "So, remember in the first week of the course, when we calculated the sample mean, what we did was to add up the values for everybody in our sample and divide by n, that's something you've been doing all your life. Well that, if I'm just rearrange the summation a little bit here, I can show you that that's the same as taking every persons value and multiplying it by 1 over n, and then adding those up. So, every person is being weighted by 1 over n. That's their probability. That's their frequency in the sample. It just happens that everybody counts once. So, everybody has the same weight. So, we don't actually have to multiply everybody by the weight. We can just divide by n at the end. But really, that's everybody's weight. Everybody has a probability in this sample, frequency in the sample of 1 over n. So this is really, the sample mean is really just a special case of expected value. I was, just make sure we're keeping our, our symbol straight here. So when I talk about sample means I use this symbol X bar. ", ['remember', 'week', 'course', 'sample', 'mean', 'values', 'everybody', 'sample', 'n', 'something', 'life', 'Well', 'rearrange', 'summation', 'bit', 'persons', 'value', 'n', 'person', 'n.', 'That', 'probability', 'frequency', 'sample', 'everybody', 'counts', 'everybody', 'weight', 'everybody', 'weight', 'n', 'end', 'everybody', 'weight', 'Everybody', 'probability', 'sample', 'frequency', 'sample', 'n.', 'So', 'sample', 'mean', 'case', 'value', 'symbol', 'straight', 'sample', 'means', 'symbol', 'X', 'bar']), 0.24573659359149524, 0.24004015446010465)
QUERY: If I follow the logic of the sunscreen case, the null hypothesis would be there is no association between exposure to PERC and Parkinson's disease. Then  I should be considering the case where both case and control had been exposed. But there is zero observation in this category. Am I missing something?
*************************
((3, '/Users/jag/Downloads/Stanford medstats/Unit 7 Homework with Answers.srt', "If there was no relationship between the chemical and getting Parkinson's disease, this would follow a binomial distribution. X would follow a binomial distribution, and the n here is 6 which is just determined by how many discordant pairs we ended up with. The p would .5 under the null hypothesis. In other words just think intuitively, if there's not relationship here between the chemical and Parkinson's disease. You'd expect there, did, the discord appears to, kind of, be even about three where it was the case that it was exposed and not the control. And three where it was the control that was exposed and not the case. The question we're really asking here is, how far off is a 51 split from what we'd expect under the normal, which is like a 3 3 split. So we're on a binomial, we're going to have to because of small numbers, we're going to have to calculate that exact binomial probability. So we're literally going to go through and calculate the exact binomial probabilities here. So what did we actually see? We saw the case where we ended up with five discordant pairs where it was the case who had been exposed and not the control. The binomial probability, we'll just apply our binomial probability formula here, ", ['relationship', 'Parkinson', 'disease', 'distribution', 'X', 'distribution', 'n', 'pairs', 'p', 'null', 'hypothesis', 'words', 'relationship', 'Parkinson', 'disease', 'discord', 'kind', 'case', 'control', 'control', 'case', 'question', 'split', 'split', 'numbers', 'probability', 'probabilities', 'case', 'pairs', 'case', 'control', 'probability', 'probability', 'formula']), 0.51234753829798, 0.3639757712918614)
((2, '/Users/jag/Downloads/Stanford medstats/Unit 7 Homework with Answers.srt', "Some of them weren't, both were not exposed others, there was a discordant. So it turns out that the majority were not exposed. So we had 93 pairs where neither the case nor the control was exposed. There weren't any new pairs where both case and control were exposed, however, we did have a couple of pairs where there was discordance. So there were five pairs where the case that is the person with Parkinson's di, disease was exposed to this chemical. But the control was not and we have one situation, one case where the opposite was true. The control was exposed to the chemical but the case was not. It turns our for McNemar's test the only thing that's of use the only information that's informative for us here is the discordant pair. So we have six discordant pairs. And the null hypothesis here is that the chemical is not related to whether or not you're a case or a control. So, under the null hypothesis we've got six discordant pairs. ", ['others', 'majority', 'pairs', 'case', 'control', 'pairs', 'case', 'control', 'couple', 'pairs', 'discordance', 'pairs', 'case', 'person', 'Parkinson', 'di', 'disease', 'control', 'situation', 'case', 'control', 'case', 'McNemar', 'test', 'thing', 'use', 'information', 'pair', 'pairs', 'null', 'hypothesis', 'case', 'control', 'null', 'hypothesis', 'pairs']), 0.5403830945624493, 0.31104593955354864)
((9, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod3.srt', "We have 37 pairs, discordant pairs, where it was the case that was diabetic versus only 16 where it was the control that was diabetic. You can see that that's pretty unbalanced, and we're going to think that that's probably going to be something statistically significant. But our, under the null, we would expect to see half here and half in cell B and half in cell C. So we start with that null hypothesis, and what we have to recognize now, is that we actually have a binomial distribution. Okay, so if you think about this carefully, I have 53 discordant pairs here. Okay? Under the null hypothesis of no relationship between diabetes and case control status, I would actually expect to see about half the pairs would be in this cell and about half would be in this cell. I see, expect to see an even split. Of course, we always know in statistics that nothing ever gets evenly split, especially since we can't have half a person, so ", ['pairs', 'pairs', 'case', 'versus', 'control', 'something', 'null', 'half', 'cell', 'B', 'cell', 'C.', 'So', 'hypothesis', 'distribution', 'Okay', 'pairs', 'Okay', 'null', 'hypothesis', 'relationship', 'diabetes', 'case', 'control', 'status', 'pairs', 'cell', 'cell', 'course', 'statistics', 'nothing', 'person']), 0.3266372899592439, 0.28527611279967274)
((3, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod3.srt', "and to actually increase the power of your statistical test down the line, and so we often do this with case control studies. You take a case, you get somebody with it a particular disease, and then you go out and find a control for that person, but you just don't pick any control. Maybe you pick somebody who's around the same age as the case and also is of the same gender and perhaps also is a smoker or non-smoker, maybe match them on smoking. When you do that, you're controlling for all these extraneous sources of variability, you're increasing your chances of being able to see a signal. Of course, you have to remember to account this pair matching at the end of the day when you analyze your data. So here's something, you know, you might match one control to one case based on things like age, gender, and smoking again. So let me just give you an example where pair matching was done. This was a study from the New England Journal of Medicine many years ago, and they wanted to know whether or not getting a tonsillectomy was related to the chances of getting Hodgkin's disease. ", ['power', 'test', 'line', 'case', 'control', 'studies', 'case', 'somebody', 'disease', 'control', 'person', 'control', 'somebody', 'age', 'case', 'gender', 'smoker', 'non-smoker', 'smoking', 'sources', 'variability', 'chances', 'signal', 'course', 'pair', 'end', 'day', 'data', 'something', 'control', 'case', 'things', 'age', 'gender', 'smoking', 'let', 'example', 'pair', 'study', 'New', 'England', 'Journal', 'Medicine', 'years', 'tonsillectomy', 'chances', 'Hodgkin', 'disease']), 0.4182141805046713, 0.27542930268598975)
((15, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit2 Mod5.srt', "We are going to be in the numerator we are going to be adding zero for all the concorded cells. And then we are going to be adding one-half, positive one-half for all the B cells. Remember the B situation is where it's the case that has the diabetes and the, control that doesn't or the case that has the exposure and the control that doesn't, if we're generalizing. For all the C cells, it's going to be negative one half, and then we're going to be squaring that. In the denominator, the variances are only occur where we have discordant cells and that's going to be the b plus c and we're going to, each one of those contributes one fourth to the variance. So we can write this down generally. We can show that is always the case that the Mantel-Haenszel statistic when applied to this particular situation will simplify, again, these halves. Halve squared will come out and it'll cancel the one fourth will simplify to B minus C square, divided by B plus C. ", ['numerator', 'zero', 'cells', 'one-half', 'B', 'Remember', 'B', 'situation', 'case', 'diabetes', 'control', 'case', 'exposure', 'control', 'C', 'half', 'denominator', 'variances', 'cells', 'b', 'c', 'contributes', 'variance', 'down', 'case', 'Mantel-Haenszel', 'situation', 'halves', 'Halve', 'B', 'minus', 'C', 'square', 'B', 'C']), 0.32914029430219166, 0.2590241823775216)
((8, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod3.srt', "We get an odds ratio here of 2.3. So there's a twofold increase in the odds of getting a heart attack if you are diabetic. That's how you would interpret that. So that's the McNemar's odds ratio. Very simple to calculate. We also want to get a p-value here, so we can cal-, we can do what's called a McNemar's chi-squared test, and now I'm going to lead you through the logic of that test. Again it focuses only on the discordant pairs. The null hypothesis here is that being diabetic is unrelated to having a heart attack. Another way to say that is that the chances among the discordant pairs that it's the case that's diabetic, should be equal to the chance that it's the control that's diabetic. So when we restrict our analysis down to the discordant pairs, what we'd expect to see if there was no relationship between diabetes and heart attacks, we'd expect to see kind of an even number, an even split. It would be equally likely that it would be the control that would be diabetic as it would be that the case would be diabetic. Now you can see clearly we don't have an even split here. ", ['odds', 'increase', 'odds', 'heart', 'attack', 'McNemar', 'odds', 'Very', 'McNemar', 'test', 'logic', 'test', 'pairs', 'null', 'hypothesis', 'having', 'heart', 'attack', 'way', 'chances', 'pairs', 'case', 'chance', 'control', 'analysis', 'pairs', 'relationship', 'diabetes', 'heart', 'attacks', 'kind', 'number', 'control', 'case']), 0.28516171605086693, 0.2583967373872739)
((4, '/Users/jag/Downloads/Stanford medstats/HRP261 Unit1 Mod3.srt', "So what they did was they found 85 Hodgkin's patients, and they looked for a control out of the siblings of the Hodgkin's patient. They looked for a sibling who had not had Hodgkin's disease but was of the same gender, and also was in a, you know, reasonably similar age bracket, within five years of the patient's age. So this was a way for them to try to say, to then look at, did the case have a tonsillectomy in the past, and did the sib control? We've got pair matching here because obviously the case and the control are more related to each other than to the, everybody else in the sample. So this was actually presented in the New England Journal of Medicine incorrectly. So here is how the authors re-, reported that data. They had 85 cases and 85 controls, and you can see in this two by two table, they reported each case and control individually, and there's no accounting for the fact that they had matching. When you calculate just the basic odds ratio from this two by two table, and the basic chi square statistic, it's not significant, and ", ['Hodgkin', 'patients', 'control', 'siblings', 'Hodgkin', 'patient', 'sibling', 'Hodgkin', 'disease', 'gender', 'age', 'bracket', 'years', 'patient', 'age', 'way', 'case', 'tonsillectomy', 'sib', 'control', 'pair', 'matching', 'case', 'control', 'everybody', 'sample', 'New', 'England', 'Journal', 'Medicine', 'authors', 'data', 'cases', 'controls', 'case', 'control', 'accounting', 'fact', 'odds', 'chi', 'square']), 0.34403123102809335, 0.24386365451463313)
((0, '/Users/jag/Downloads/Stanford medstats/Unit 7 Homework with Answers.srt', "These data came from a twin study, so each case is somebody with Parkinson's disease and they have a twin who does not have Parkinson's disease. So we have these case control pairs that are correlated by the fact that they're within a twin pair. So we have correlated data here. Our outcome here is binary. You have Parkinson's disease or you don't. Our exposure variable, our predictor is whether or not you've been various chemicals. So we have essentially categorical data for both the predictor and the outcome but we have this correlation. So that's going to tell us that we're going to have to do a McNemar's test. However, if you look carefully at a lot of these cells. Remember for a McNemar's test, we only care about the discordant pairs which would be these two middle two columns. If you look at all of these columns, a lot of the columns involve sparse data. So one or two people only in some of those cells. ", ['data', 'twin', 'study', 'case', 'somebody', 'Parkinson', 'disease', 'twin', 'Parkinson', 'disease', 'case', 'control', 'pairs', 'fact', 'twin', 'pair', 'data', 'outcome', 'Parkinson', 'disease', 'exposure', 'predictor', 'chemicals', 'data', 'predictor', 'outcome', 'correlation', 'McNemar', 'test', 'lot', 'cells', 'Remember', 'McNemar', 'test', 'pairs', 'columns', 'columns', 'lot', 'columns', 'sparse', 'data', 'people', 'cells']), 0.2931977358041869, 0.24024683141417805)
